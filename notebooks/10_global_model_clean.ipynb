{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c039d719",
      "metadata": {
        "id": "c039d719"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/10_global_model_clean.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d67760d",
      "metadata": {
        "id": "6d67760d"
      },
      "source": [
        "# PHASE 8: Global Model - Unified Training\n",
        "\n",
        "**Objective**: Train a **single** anomaly detection model on **all 3 classes** simultaneously.\n",
        "\n",
        "## Key Differences from Per-Class Models:\n",
        "1. **Single Model**: Train ONE PatchCore and ONE PaDiM on merged training data\n",
        "2. **Per-Class Thresholds**: Calibrate separate thresholds for each class on validation\n",
        "3. **Identical Shortcut Problem**: Can normals from one class be confused with anomalies from another?\n",
        "4. **Performance Gap Analysis**: Quantify degradation vs per-class models\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outcome:\n",
        "Global models should perform **worse** than per-class models due to:\n",
        "- **Distribution heterogeneity**: Mixing different textures/objects\n",
        "- **Identical shortcut**: Normal patterns of Class A may appear anomalous for Class B\n",
        "- **Feature space contamination**: Shared representation struggles with diverse nominal distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73a7b8b",
      "metadata": {
        "id": "a73a7b8b"
      },
      "source": [
        "## Terminological Note (Model-Unified vs Absolute-Unified)\n",
        "\n",
        "Following the taxonomy from recent literature [CADA, Guo et al. 2024; HierCore, Heo & Kang 2025]:\n",
        "\n",
        "| Setting | Training | Inference | Threshold | Our Experiment |\n",
        "|---------|----------|-----------|-----------|----------------|\n",
        "| **Per-Class** | Separate model per class | Class known | Per-class | ❌ |\n",
        "| **Model-Unified** | Single model for all classes | Class known | Per-class | ✅ **This notebook** |\n",
        "| **Absolute-Unified** | Single model for all classes | Class UNKNOWN | Single global | ❌ |\n",
        "\n",
        "**Our setting**: We train ONE global model but calibrate **per-class thresholds** at inference\n",
        "(class is known). This is the \"model-unified\" setting, NOT \"absolute-unified\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d69dc0",
      "metadata": {
        "id": "83d69dc0"
      },
      "source": [
        "## 0. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ffcfc2ec",
      "metadata": {
        "id": "ffcfc2ec",
        "outputId": "160ae961-00b1-4816-bf1a-bed5ac6f40be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Done!\n",
            "\n",
            "Cloning repository (branch: main)...\n",
            "Removing existing repository...\n",
            "Cloning into '/content/Detection-of-Anomalies-with-Localization'...\n",
            "remote: Enumerating objects: 939, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (120/120), done.\u001b[K\n",
            "remote: Total 939 (delta 59), reused 61 (delta 18), pack-reused 798 (from 2)\u001b[K\n",
            "Receiving objects: 100% (939/939), 330.45 MiB | 13.97 MiB/s, done.\n",
            "Resolving deltas: 100% (516/516), done.\n",
            "Updating files: 100% (120/120), done.\n",
            "Done!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "SETUP COMPLETE - PHASE 8: GLOBAL MODEL (MODEL-UNIFIED)\n",
            "======================================================================\n",
            "Project:    /content/Detection-of-Anomalies-with-Localization\n",
            "Dataset:    /content/drive/MyDrive/mvtec_ad\n",
            "Branch:     main\n",
            "Models:     /content/Detection-of-Anomalies-with-Localization/outputs/models\n",
            "Results:    /content/Detection-of-Anomalies-with-Localization/outputs/results\n",
            "Thresholds: /content/Detection-of-Anomalies-with-Localization/outputs/thresholds\n",
            "Viz:        /content/Detection-of-Anomalies-with-Localization/outputs/visualizations/global_model\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SETUP - Mount Google Drive & Clone Repository\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# Clone repository on main branch\n",
        "print(\"Cloning repository (branch: main)...\")\n",
        "repo_dir = '/content/Detection-of-Anomalies-with-Localization'\n",
        "\n",
        "# Remove if exists\n",
        "if os.path.exists(repo_dir):\n",
        "    print(\"Removing existing repository...\")\n",
        "    !rm -rf {repo_dir}\n",
        "\n",
        "# Clone from main branch\n",
        "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git {repo_dir}\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# Setup paths\n",
        "PROJECT_ROOT = Path(repo_dir)\n",
        "\n",
        "# Dataset location (only clean for this notebook)\n",
        "CLEAN_DATASET_PATH = Path('/content/drive/MyDrive/mvtec_ad')\n",
        "\n",
        "# Output directories\n",
        "MODELS_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
        "RESULTS_DIR = PROJECT_ROOT / 'outputs' / 'results'\n",
        "THRESHOLDS_DIR = PROJECT_ROOT / 'outputs' / 'thresholds'\n",
        "VIZ_DIR = PROJECT_ROOT / 'outputs' / 'visualizations' / 'global_model'\n",
        "\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "THRESHOLDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Verify dataset exists\n",
        "if not CLEAN_DATASET_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Clean dataset not found at {CLEAN_DATASET_PATH}\\n\"\n",
        "        f\"Please ensure mvtec_ad folder is in your Google Drive.\"\n",
        "    )\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SETUP COMPLETE - PHASE 8: GLOBAL MODEL (MODEL-UNIFIED)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Project:    {PROJECT_ROOT}\")\n",
        "print(f\"Dataset:    {CLEAN_DATASET_PATH}\")\n",
        "print(f\"Branch:     main\")\n",
        "print(f\"Models:     {MODELS_DIR}\")\n",
        "print(f\"Results:    {RESULTS_DIR}\")\n",
        "print(f\"Thresholds: {THRESHOLDS_DIR}\")\n",
        "print(f\"Viz:        {VIZ_DIR}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94538d9c",
      "metadata": {
        "id": "94538d9c",
        "outputId": "7ec85f8a-d6ee-43a6-d620-9eb820de1c2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.8/851.8 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for freia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu --quiet\n",
        "!pip install anomalib --quiet\n",
        "!pip install umap-learn --quiet\n",
        "import umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e2c9ea8a",
      "metadata": {
        "id": "e2c9ea8a",
        "outputId": "9b497b1b-9ecd-4296-d1aa-84c945e2d652",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# Scientific computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Deep Learning\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Dimensionality reduction\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Project imports\n",
        "from src.data.dataset import MVTecDataset\n",
        "from src.data.transforms import get_clean_transforms\n",
        "from src.models.patchcore import PatchCore\n",
        "from src.models.padim_wrapper import PadimWrapper\n",
        "from src.metrics.threshold_selection import calibrate_threshold\n",
        "from src.metrics.image_metrics import compute_auroc, compute_auprc, compute_f1_at_threshold, compute_classification_metrics\n",
        "from src.metrics.pixel_metrics import compute_pixel_auroc, compute_pro\n",
        "from src.utils.reproducibility import set_seed\n",
        "from src.utils.paths import ProjectPaths\n",
        "\n",
        "# Set matplotlib style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0c933060",
      "metadata": {
        "id": "0c933060",
        "outputId": "cf0be25e-c1f6-46aa-d636-01b02379dff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed set to 42 for reproducibility\n",
            "Device: cuda\n",
            "Classes: ['hazelnut', 'carpet', 'zipper']\n",
            "Seed: 42 (FIXED for reproducibility)\n"
          ]
        }
      ],
      "source": [
        "# CRITICAL: Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Configuration\n",
        "CLASSES = ['hazelnut', 'carpet', 'zipper']\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "paths = ProjectPaths(PROJECT_ROOT)\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Classes: {CLASSES}\")\n",
        "print(f\"Seed: 42 (FIXED for reproducibility)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18792528",
      "metadata": {
        "id": "18792528"
      },
      "source": [
        "## 1. Data Preparation: Merge Training Sets from All Classes\n",
        "\n",
        "**CRITICAL**: We create a **SINGLE** global training set by merging Train-clean from all 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fb7edda",
      "metadata": {
        "id": "0fb7edda",
        "outputId": "ac3c7b17-727e-4870-cd49-8861a74d69ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded splits for classes: ['splits', 'metadata']\n"
          ]
        }
      ],
      "source": [
        "# Load clean splits\n",
        "splits_path = paths.DATA_PROCESSED / 'clean_splits.json'\n",
        "with open(splits_path, 'r') as f:\n",
        "    splits = json.load(f)\n",
        "\n",
        "print(f\"Loaded splits for classes: {list(splits.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a4f62c7c",
      "metadata": {
        "id": "a4f62c7c",
        "outputId": "966887c7-512f-4f48-f5f5-e14fe9548c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  hazelnut  :  312 normal images\n",
            "  carpet    :  224 normal images\n",
            "  zipper    :  192 normal images\n",
            "\n",
            "Global Train Set: 728 total images\n",
            "   Distribution: Hazelnut=312, Carpet=224, Zipper=192\n"
          ]
        }
      ],
      "source": [
        "# Merge all train_clean images into a single global dataset\n",
        "global_train_images = []\n",
        "global_train_masks = []\n",
        "global_train_labels = []\n",
        "global_train_class_ids = []  # Track which class each image belongs to\n",
        "\n",
        "for class_idx, class_name in enumerate(CLASSES):\n",
        "    class_splits = splits['splits'][class_name]['train']\n",
        "\n",
        "    # Train only contains normal images\n",
        "    n_samples = len(class_splits['images'])\n",
        "\n",
        "    global_train_images.extend(class_splits['images'])\n",
        "    global_train_masks.extend([None] * n_samples)  # Only normals\n",
        "    global_train_labels.extend([0] * n_samples)  # Label 0 = normal\n",
        "    global_train_class_ids.extend([class_idx] * n_samples)\n",
        "\n",
        "    print(f\"  {class_name:10s}: {n_samples:4d} normal images\")\n",
        "\n",
        "print(f\"\\nGlobal Train Set: {len(global_train_images)} total images\")\n",
        "print(f\"   Distribution: Hazelnut={global_train_class_ids.count(0)}, \"\n",
        "      f\"Carpet={global_train_class_ids.count(1)}, Zipper={global_train_class_ids.count(2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1fa729e0",
      "metadata": {
        "id": "1fa729e0",
        "outputId": "497b7708-cbe2-4595-fa39-2f194b063b6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Train DataLoader: 23 batches\n"
          ]
        }
      ],
      "source": [
        "# Custom collate function to handle None masks (MUST BE DEFINED FIRST!)\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"Custom collate function that handles None masks.\"\"\"\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    masks = [item[1] for item in batch]  # Keep as list (may contain None)\n",
        "    labels = torch.tensor([item[2] for item in batch])\n",
        "    paths = [item[3] for item in batch]\n",
        "    return images, masks, labels, paths\n",
        "\n",
        "# Create global training dataset\n",
        "transform_clean = get_clean_transforms()\n",
        "\n",
        "global_train_dataset = MVTecDataset(\n",
        "    images=global_train_images,\n",
        "    masks=global_train_masks,\n",
        "    labels=global_train_labels,\n",
        "    transform=transform_clean,\n",
        "    phase='train'\n",
        ")\n",
        "\n",
        "global_train_loader = DataLoader(\n",
        "    global_train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=custom_collate_fn  # <-- QUESTA È LA RIGA CHIAVE!\n",
        ")\n",
        "\n",
        "print(f\"Global Train DataLoader: {len(global_train_loader)} batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66f1966",
      "metadata": {
        "id": "c66f1966"
      },
      "source": [
        "## 2. Train Single PatchCore Global Model\n",
        "\n",
        "**KEY POINT**: Training ONE model on the merged dataset (not 3 separate models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b4c0b7ba",
      "metadata": {
        "id": "b4c0b7ba",
        "outputId": "b37b5b71-f6ee-420a-f68e-55106d49b683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING GLOBAL PATCHCORE MODEL\n",
            "======================================================================\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s]\n",
            "Extracting features: 100%|██████████| 23/23 [10:51<00:00, 28.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Using FAISS for fast k-NN search (memory bank: 28537 samples, 1536 dims)\n",
            "\n",
            "PatchCore Global trained in 11248.54s\n",
            "   Memory bank size: 28,537 patches\n",
            "   Model saved to: /content/Detection-of-Anomalies-with-Localization/outputs/models/patchcore_global_clean.npy\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRAINING GLOBAL PATCHCORE MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize PatchCore with same hyperparameters as per-class models\n",
        "patchcore_global = PatchCore(\n",
        "    backbone_layers=['layer2', 'layer3'],\n",
        "    patch_size=3,\n",
        "    coreset_ratio=0.05,  # 5% as per PHASE 3.5\n",
        "    n_neighbors=9,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Train on global dataset\n",
        "start_time = time.time()\n",
        "patchcore_global.fit(global_train_loader, apply_coreset=True)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nPatchCore Global trained in {training_time:.2f}s\")\n",
        "print(f\"   Memory bank size: {patchcore_global.memory_bank.features.shape[0]:,} patches\")\n",
        "\n",
        "# Save model\n",
        "save_path = paths.MODELS / 'patchcore_global_clean.npy'\n",
        "patchcore_global.save(paths.MODELS, class_name='global', domain='clean')\n",
        "print(f\"   Model saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8ee2bf",
      "metadata": {
        "id": "0d8ee2bf"
      },
      "source": [
        "## 3. Train Single PaDiM Global Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a9ff8b67",
      "metadata": {
        "id": "a9ff8b67",
        "outputId": "88262e75-89a0-4655-da14-3ade9deef15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676,
          "referenced_widgets": [
            "5fe9198e1540406d8ee5cf1baa5c7065",
            "e0401038b0764389b01aa841b569253b",
            "64e0e256a7ef4259b632168cfba8549b",
            "dc8b73a776604cccaed8f29781d28a2c",
            "d38687ebe7b04132bcea2cbc4fce4f56",
            "ce06ba0588ed4138832e11cb65de8f90",
            "061d896c613241d195bb231043256e3f",
            "fb1441a8812d41b2943cd0b923ac1d0b",
            "46308d1fddec4d8a9686c3b981beb600",
            "dab0f4bfc25f4ac9a0c2f984a55d62e0",
            "cf0d3bdced1346d38dc33d1994a99964"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING GLOBAL PADIM MODEL\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/276M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fe9198e1540406d8ee5cf1baa5c7065"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training PaDiM on 728 normal samples\n",
            "Backbone: wide_resnet50_2 | Layers: ['layer1', 'layer2', 'layer3']\n",
            "N features: 100\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting features: 100%|██████████| 23/23 [00:34<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fitting Gaussian distributions...\n",
            "  Total samples: 728\n",
            "  Memory bank size: 23 batches\n",
            "\n",
            "============================================================\n",
            "[OK] Training completed in 35.40s\n",
            "  Gaussian mean shape: torch.Size([100, 3136])\n",
            "  Inv covariance shape: torch.Size([3136, 100, 100])\n",
            "  Memory usage: 120.83 MB\n",
            "============================================================\n",
            "\n",
            "\n",
            "PaDiM Global trained in 35.41s\n",
            "[OK] Model saved: padim_global_clean.pt\n",
            "  Stats saved: padim_global_clean.json\n",
            "   Model saved to: /content/Detection-of-Anomalies-with-Localization/outputs/models/padim_global_clean.pt\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TRAINING GLOBAL PADIM MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Initialize PaDiM\n",
        "padim_global = PadimWrapper(\n",
        "    backbone='wide_resnet50_2',\n",
        "    layers=['layer1', 'layer2', 'layer3'],\n",
        "    n_features=100,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# Train on global dataset\n",
        "start_time = time.time()\n",
        "padim_global.fit(global_train_loader, verbose=True)\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nPaDiM Global trained in {training_time:.2f}s\")\n",
        "\n",
        "# Save model\n",
        "save_path = paths.MODELS / 'padim_global_clean.pt'\n",
        "padim_global.save(save_path)\n",
        "print(f\"   Model saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2571c097",
      "metadata": {
        "id": "2571c097"
      },
      "source": [
        "## 4. Per-Class Threshold Calibration (Using Global Models)\n",
        "\n",
        "**CRITICAL**: Although we have ONE model, we calibrate **SEPARATE thresholds** for each class on their validation sets.\n",
        "\n",
        "This allows fair comparison: each class gets an optimal threshold despite using a shared model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "31091db2",
      "metadata": {
        "id": "31091db2",
        "outputId": "62a43772-00d3-4365-dbbc-b7b401f6e5bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PER-CLASS THRESHOLD CALIBRATION (GLOBAL MODELS)\n",
            "======================================================================\n",
            "\n",
            "Calibrating thresholds for hazelnut...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3534579942.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Get predictions from GLOBAL models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     patchcore_scores, _ = patchcore_global.predict(\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mreturn_heatmaps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    238\u001b[0m                 ]\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PER-CLASS THRESHOLD CALIBRATION (GLOBAL MODELS)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "thresholds_global = {\n",
        "    'patchcore': {},\n",
        "    'padim': {}\n",
        "}\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    print(f\"\\nCalibrating thresholds for {class_name}...\")\n",
        "\n",
        "    # Load validation split for this class\n",
        "    val_split = splits['splits'][class_name]['val']\n",
        "\n",
        "    val_dataset = MVTecDataset(\n",
        "        images=val_split['images'],\n",
        "        masks=val_split['masks'],\n",
        "        labels=val_split['labels'],\n",
        "        transform=transform_clean,\n",
        "        phase='val'\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # Get predictions from GLOBAL models\n",
        "    patchcore_scores, _ = patchcore_global.predict(\n",
        "        torch.cat([batch[0] for batch in val_loader]).to(device),\n",
        "        return_heatmaps=False\n",
        "    )\n",
        "\n",
        "    padim_scores, _ = padim_global.predict(\n",
        "        torch.cat([batch[0] for batch in val_loader]).to(device),\n",
        "        return_heatmaps=False\n",
        "    )\n",
        "\n",
        "    val_labels = np.array(val_split['labels'])\n",
        "\n",
        "    # Calibrate thresholds to maximize F1\n",
        "    threshold_pc = calibrate_threshold(patchcore_scores, val_labels)\n",
        "    threshold_pd = calibrate_threshold(padim_scores, val_labels)\n",
        "\n",
        "    thresholds_global['patchcore'][class_name] = float(threshold_pc)\n",
        "    thresholds_global['padim'][class_name] = float(threshold_pd)\n",
        "\n",
        "    # Compute F1 at calibrated thresholds\n",
        "    f1_pc = compute_f1_at_threshold(val_labels, patchcore_scores, threshold_pc)\n",
        "    f1_pd = compute_f1_at_threshold(val_labels, padim_scores, threshold_pd)\n",
        "\n",
        "    print(f\"   PatchCore: threshold={threshold_pc:.4f}, val_F1={f1_pc:.4f}\")\n",
        "    print(f\"   PaDiM:     threshold={threshold_pd:.4f}, val_F1={f1_pd:.4f}\")\n",
        "\n",
        "# Save thresholds\n",
        "thresholds_path = paths.THRESHOLDS / 'global_thresholds.json'\n",
        "with open(thresholds_path, 'w') as f:\n",
        "    json.dump(thresholds_global, f, indent=2)\n",
        "\n",
        "print(f\"\\nThresholds saved to: {thresholds_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b7358f2",
      "metadata": {
        "id": "0b7358f2"
      },
      "source": [
        "## 5. Evaluate Global Models on Test-Clean (Per-Class)\n",
        "\n",
        "Test the global models on each class separately using per-class thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54e7c323",
      "metadata": {
        "id": "54e7c323"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"EVALUATING GLOBAL MODELS ON TEST-CLEAN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_global = {\n",
        "    'patchcore': {},\n",
        "    'padim': {}\n",
        "}\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    print(f\"\\nEvaluating {class_name}...\")\n",
        "\n",
        "    # Load test split\n",
        "    test_split = splits['splits'][class_name]['test']\n",
        "\n",
        "    test_dataset = MVTecDataset(\n",
        "        images=test_split['images'],\n",
        "        masks=test_split['masks'],\n",
        "        labels=test_split['labels'],\n",
        "        transform=transform_clean,\n",
        "        phase='test'\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=32,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    # Get all test images and labels\n",
        "    test_images = torch.cat([batch[0] for batch in test_loader]).to(device)\n",
        "    test_masks_list = [batch[1] for batch in test_loader]\n",
        "    test_labels = np.array(test_split['labels'])\n",
        "\n",
        "    # PatchCore predictions\n",
        "    pc_scores, pc_heatmaps = patchcore_global.predict(test_images, return_heatmaps=True)\n",
        "    pc_predictions = (pc_scores >= thresholds_global['patchcore'][class_name]).astype(int)\n",
        "\n",
        "    # PaDiM predictions\n",
        "    pd_scores, pd_heatmaps = padim_global.predict(test_images, return_heatmaps=True)\n",
        "    pd_predictions = (pd_scores >= thresholds_global['padim'][class_name]).astype(int)\n",
        "\n",
        "    # Compute image-level metrics\n",
        "    pc_auroc = compute_auroc(test_labels, pc_scores)\n",
        "    pc_auprc = compute_auprc(test_labels, pc_scores)\n",
        "    pc_metrics = compute_classification_metrics(test_labels, pc_predictions)\n",
        "\n",
        "    pd_auroc = compute_auroc(test_labels, pd_scores)\n",
        "    pd_auprc = compute_auprc(test_labels, pd_scores)\n",
        "    pd_metrics = compute_classification_metrics(test_labels, pd_predictions)\n",
        "\n",
        "    # Compute pixel-level metrics (for anomalous images with masks)\n",
        "    anomaly_indices = np.where(test_labels == 1)[0]\n",
        "    if len(anomaly_indices) > 0:\n",
        "        test_masks_anomalies = [test_masks_list[i] for i in anomaly_indices if test_masks_list[i] is not None]\n",
        "        pc_heatmaps_anomalies = pc_heatmaps[anomaly_indices]\n",
        "        pd_heatmaps_anomalies = pd_heatmaps[anomaly_indices]\n",
        "\n",
        "        if len(test_masks_anomalies) > 0:\n",
        "            pc_pixel_auroc = compute_pixel_auroc(test_masks_anomalies, pc_heatmaps_anomalies)\n",
        "            pc_pro = compute_pro(test_masks_anomalies, pc_heatmaps_anomalies)\n",
        "\n",
        "            pd_pixel_auroc = compute_pixel_auroc(test_masks_anomalies, pd_heatmaps_anomalies)\n",
        "            pd_pro = compute_pro(test_masks_anomalies, pd_heatmaps_anomalies)\n",
        "        else:\n",
        "            pc_pixel_auroc = pc_pro = pd_pixel_auroc = pd_pro = None\n",
        "    else:\n",
        "        pc_pixel_auroc = pc_pro = pd_pixel_auroc = pd_pro = None\n",
        "\n",
        "    # Store results\n",
        "    results_global['patchcore'][class_name] = {\n",
        "        'auroc': pc_auroc,\n",
        "        'auprc': pc_auprc,\n",
        "        'f1': pc_metrics['f1'],\n",
        "        'accuracy': pc_metrics['accuracy'],\n",
        "        'precision': pc_metrics['precision'],\n",
        "        'recall': pc_metrics['recall'],\n",
        "        'pixel_auroc': pc_pixel_auroc,\n",
        "        'pro': pc_pro\n",
        "    }\n",
        "\n",
        "    results_global['padim'][class_name] = {\n",
        "        'auroc': pd_auroc,\n",
        "        'auprc': pd_auprc,\n",
        "        'f1': pd_metrics['f1'],\n",
        "        'accuracy': pd_metrics['accuracy'],\n",
        "        'precision': pd_metrics['precision'],\n",
        "        'recall': pd_metrics['recall'],\n",
        "        'pixel_auroc': pd_pixel_auroc,\n",
        "        'pro': pd_pro\n",
        "    }\n",
        "\n",
        "    print(f\"   PatchCore: AUROC={pc_auroc:.4f}, F1={pc_metrics['f1']:.4f}, Pixel AUROC={pc_pixel_auroc:.4f if pc_pixel_auroc else 'N/A'}\")\n",
        "    print(f\"   PaDiM:     AUROC={pd_auroc:.4f}, F1={pd_metrics['f1']:.4f}, Pixel AUROC={pd_pixel_auroc:.4f if pd_pixel_auroc else 'N/A'}\")\n",
        "\n",
        "print(\"\\nGlobal model evaluation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f569c90",
      "metadata": {
        "id": "2f569c90"
      },
      "source": [
        "## 6. Load Per-Class Models for Comparison\n",
        "\n",
        "Load the per-class trained models to compute the performance gap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cac9b3a0",
      "metadata": {
        "id": "cac9b3a0"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING PER-CLASS MODELS FOR COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load per-class results from PHASE 5 (clean domain evaluation)\n",
        "clean_results_path = paths.RESULTS / 'clean_results.json'\n",
        "with open(clean_results_path, 'r') as f:\n",
        "    clean_results = json.load(f)\n",
        "\n",
        "# Extract per-class model results\n",
        "# clean_results structure: {\"metadata\": {...}, \"patchcore\": {...}, \"padim\": {...}}\n",
        "results_per_class = {\n",
        "    'patchcore': clean_results['patchcore'],\n",
        "    'padim': clean_results['padim']\n",
        "}\n",
        "\n",
        "print(\"Per-class model results loaded\")\n",
        "print(f\"   Available classes: {list(results_per_class['patchcore'].keys())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda86126",
      "metadata": {
        "id": "dda86126"
      },
      "source": [
        "## 7. Performance Gap Analysis: Global vs Per-Class\n",
        "\n",
        "**Key Question**: How much performance do we lose by using a single global model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6221209b",
      "metadata": {
        "id": "6221209b"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"PERFORMANCE GAP ANALYSIS: Per-Class vs Global Model\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "gaps = {\n",
        "    'patchcore': {},\n",
        "    'padim': {}\n",
        "}\n",
        "\n",
        "comparison_data = []\n",
        "\n",
        "for method in ['patchcore', 'padim']:\n",
        "    print(f\"\\n{method.upper()}:\")\n",
        "    print(f\"{'Class':<12} {'Per-Class AUROC':<18} {'Global AUROC':<15} {'Gap':<10}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for class_name in CLASSES:\n",
        "        auroc_per_class = results_per_class[method][class_name]['image_level']['auroc']\n",
        "        auroc_global = results_global[method][class_name]['auroc']\n",
        "        gap = auroc_per_class - auroc_global\n",
        "\n",
        "        gaps[method][class_name] = gap\n",
        "\n",
        "        comparison_data.append({\n",
        "            'Method': method.upper(),\n",
        "            'Class': class_name,\n",
        "            'Per-Class AUROC': auroc_per_class,\n",
        "            'Global AUROC': auroc_global,\n",
        "            'Gap': gap\n",
        "        })\n",
        "\n",
        "        print(f\"{class_name:<12} {auroc_per_class:>16.4f}   {auroc_global:>13.4f}   {gap:>+8.4f}\")\n",
        "\n",
        "    # Macro average\n",
        "    avg_per_class = np.mean([results_per_class[method][c]['image_level']['auroc'] for c in CLASSES])\n",
        "    avg_global = np.mean([results_global[method][c]['auroc'] for c in CLASSES])\n",
        "    avg_gap = avg_per_class - avg_global\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'MACRO AVG':<12} {avg_per_class:>16.4f}   {avg_global:>13.4f}   {avg_gap:>+8.4f}\")\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Method': method.upper(),\n",
        "        'Class': 'MACRO_AVG',\n",
        "        'Per-Class AUROC': avg_per_class,\n",
        "        'Global AUROC': avg_global,\n",
        "        'Gap': avg_gap\n",
        "    })\n",
        "\n",
        "# Create DataFrame for export\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "comparison_csv_path = paths.RESULTS / 'global_vs_per_class_comparison.csv'\n",
        "df_comparison.to_csv(comparison_csv_path, index=False)\n",
        "print(f\"\\nComparison saved to: {comparison_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728feb35",
      "metadata": {
        "id": "728feb35"
      },
      "outputs": [],
      "source": [
        "# Visualization: Bar chart of performance gaps\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for idx, method in enumerate(['patchcore', 'padim']):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    gap_values = [gaps[method][c] for c in CLASSES]\n",
        "    colors = ['red' if g < 0 else 'green' for g in gap_values]\n",
        "\n",
        "    bars = ax.bar(CLASSES, gap_values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, gap_values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{val:+.3f}',\n",
        "                ha='center', va='bottom' if height > 0 else 'top',\n",
        "                fontsize=10, fontweight='bold')\n",
        "\n",
        "    ax.set_title(f'{method.upper()}: Performance Gap\\n(Per-Class - Global)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('AUROC Gap', fontsize=11)\n",
        "    ax.set_xlabel('Class', fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
        "    ax.set_ylim([min(gap_values) - 0.05, max(gap_values) + 0.05])\n",
        "\n",
        "plt.tight_layout()\n",
        "gap_plot_path = aths.VISUALIZATIONS / 'global_model_performance_gap.png'\n",
        "plt.savefig(gap_plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Plot saved to: {gap_plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8391ec3f",
      "metadata": {
        "id": "8391ec3f"
      },
      "source": [
        "## 8. Identical Shortcut Problem Analysis\n",
        "\n",
        "**Context**: In model-unified anomaly detection [CADA, Guo et al. 2024], a single model\n",
        "is trained on multiple classes but evaluated with per-class thresholds. This differs from\n",
        "absolute-unified settings where class information is unavailable at inference.\n",
        "\n",
        "**The \"Identical Shortcut\" Problem** [UniAD, You et al. 2022]: When training a unified model\n",
        "on heterogeneous classes, normal patterns from one class may appear anomalous for another.\n",
        "\n",
        "**Hypothesis**: Normal images from Class A may exceed the anomaly threshold calibrated for Class B,\n",
        "even with per-class thresholds, due to shared feature representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db705e4f",
      "metadata": {
        "id": "db705e4f"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"IDENTICAL SHORTCUT PROBLEM ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"Testing: Do normals from Class A trigger anomaly threshold for Class B?\\n\")\n",
        "\n",
        "# Focus on PatchCore for this analysis (same applies to PaDiM)\n",
        "confusion_matrix_cross = np.zeros((len(CLASSES), len(CLASSES)))\n",
        "\n",
        "for target_idx, target_class in enumerate(CLASSES):\n",
        "    threshold = thresholds_global['patchcore'][target_class]\n",
        "    print(f\"\\nTarget Class: {target_class} (threshold={threshold:.4f})\")\n",
        "\n",
        "    for source_idx, source_class in enumerate(CLASSES):\n",
        "        # Load NORMAL test images from source class\n",
        "        source_test = splits['splits'][source_class]['test']\n",
        "        normal_indices = [i for i, lbl in enumerate(source_test['labels']) if lbl == 0]\n",
        "\n",
        "        if len(normal_indices) == 0:\n",
        "            confusion_matrix_cross[target_idx, source_idx] = 0.0\n",
        "            continue\n",
        "\n",
        "        normal_images = [source_test['images'][i] for i in normal_indices]\n",
        "\n",
        "        # Create dataset\n",
        "        normal_dataset = MVTecDataset(\n",
        "            images=normal_images,\n",
        "            masks=[None] * len(normal_images),\n",
        "            labels=[0] * len(normal_images),\n",
        "            transform=transform_clean,\n",
        "            phase='test'\n",
        "        )\n",
        "\n",
        "        normal_loader = DataLoader(normal_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
        "        normal_images_tensor = torch.cat([batch[0] for batch in normal_loader]).to(device)\n",
        "\n",
        "        # Predict with global model\n",
        "        scores, _ = patchcore_global.predict(normal_images_tensor, return_heatmaps=False)\n",
        "\n",
        "        # Confusion rate: % of normals from source that exceed target threshold\n",
        "        false_positive_rate = (scores > threshold).mean()\n",
        "        confusion_matrix_cross[target_idx, source_idx] = false_positive_rate\n",
        "\n",
        "        if source_class == target_class:\n",
        "            print(f\"   {source_class:10s} (same class): {false_positive_rate:.2%} FP rate\")\n",
        "        else:\n",
        "            print(f\"   {source_class:10s} → confusion: {false_positive_rate:.2%}\")\n",
        "\n",
        "print(\"\\nCross-class confusion analysis complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43443568",
      "metadata": {
        "id": "43443568"
      },
      "outputs": [],
      "source": [
        "# Visualization: Confusion heatmap\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "sns.heatmap(\n",
        "    confusion_matrix_cross * 100,  # Convert to percentage\n",
        "    annot=True,\n",
        "    fmt='.1f',\n",
        "    cmap='RdYlGn_r',  # Red = high confusion, Green = low confusion\n",
        "    xticklabels=CLASSES,\n",
        "    yticklabels=CLASSES,\n",
        "    cbar_kws={'label': 'False Positive Rate (%)'},\n",
        "    vmin=0,\n",
        "    vmax=100,\n",
        "    linewidths=1,\n",
        "    linecolor='gray',\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "ax.set_title('Identical Shortcut Problem\\n'\n",
        "             'Cross-Class Confusion Matrix (PatchCore Global Model)',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('Source Class (Normal Images)', fontsize=12, fontweight='bold')\n",
        "ax.set_ylabel('Target Class (Threshold)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Add explanation text\n",
        "fig.text(0.5, 0.02,\n",
        "         'Higher values (red) indicate normals from Source Class are confused as anomalies for Target Class',\n",
        "         ha='center', fontsize=10, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "confusion_plot_path = aths.VISUALIZATIONS / 'identical_shortcut_confusion.png'\n",
        "plt.savefig(confusion_plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Confusion heatmap saved to: {confusion_plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59c95bf8",
      "metadata": {
        "id": "59c95bf8"
      },
      "source": [
        "## 9. Feature Space Visualization: T-SNE\n",
        "\n",
        "Visualize how the global model represents normal and anomalous samples from all classes in feature space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3619c1c",
      "metadata": {
        "id": "f3619c1c"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FEATURE SPACE VISUALIZATION (T-SNE)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Collect features from all classes\n",
        "all_features = []\n",
        "all_labels = []\n",
        "all_class_names = []\n",
        "\n",
        "n_samples_per_class = 30  # Limit for visualization\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    test_split = splits['splits'][class_name]['test']\n",
        "\n",
        "    # Sample normal and anomalous images\n",
        "    normal_indices = [i for i, lbl in enumerate(test_split['labels']) if lbl == 0][:n_samples_per_class]\n",
        "    anomaly_indices = [i for i, lbl in enumerate(test_split['labels']) if lbl == 1][:n_samples_per_class]\n",
        "\n",
        "    selected_indices = normal_indices + anomaly_indices\n",
        "    selected_images = [test_split['images'][i] for i in selected_indices]\n",
        "    selected_labels = [test_split['labels'][i] for i in selected_indices]\n",
        "\n",
        "    # Create dataset\n",
        "    sample_dataset = MVTecDataset(\n",
        "        images=selected_images,\n",
        "        masks=[None] * len(selected_images),\n",
        "        labels=selected_labels,\n",
        "        transform=transform_clean,\n",
        "        phase='test'\n",
        "    )\n",
        "\n",
        "    sample_loader = DataLoader(sample_dataset, batch_size=len(selected_images), shuffle=False)\n",
        "    sample_images_tensor = next(iter(sample_loader))[0].to(device)\n",
        "\n",
        "    # Extract features using PatchCore backbone\n",
        "    with torch.no_grad():\n",
        "        features = patchcore_global.backbone(sample_images_tensor)  # (B, C, H, W)\n",
        "        # Global average pooling to get image-level features\n",
        "        features_pooled = features.mean(dim=[2, 3])  # (B, C)\n",
        "        features_numpy = features_pooled.cpu().numpy()\n",
        "\n",
        "    all_features.append(features_numpy)\n",
        "    all_labels.extend(selected_labels)\n",
        "    all_class_names.extend([class_name] * len(selected_images))\n",
        "\n",
        "    print(f\"   {class_name}: {len(normal_indices)} normals, {len(anomaly_indices)} anomalies\")\n",
        "\n",
        "# Concatenate all features\n",
        "all_features = np.vstack(all_features)\n",
        "print(f\"\\nCollected {all_features.shape[0]} samples with {all_features.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c65d2d29",
      "metadata": {
        "id": "c65d2d29"
      },
      "outputs": [],
      "source": [
        "# Run T-SNE\n",
        "print(\"Running T-SNE (this may take a minute)...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
        "features_2d = tsne.fit_transform(all_features)\n",
        "print(\"T-SNE complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20343bc6",
      "metadata": {
        "id": "20343bc6"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# Define colors and markers\n",
        "class_colors = {'hazelnut': 'blue', 'carpet': 'green', 'zipper': 'purple'}\n",
        "label_markers = {0: 'o', 1: 'X'}  # o = normal, X = anomaly\n",
        "label_sizes = {0: 50, 1: 100}\n",
        "\n",
        "# Plot each combination\n",
        "for class_name in CLASSES:\n",
        "    for label_type in [0, 1]:\n",
        "        mask = [(c == class_name and l == label_type)\n",
        "                for c, l in zip(all_class_names, all_labels)]\n",
        "\n",
        "        if not any(mask):\n",
        "            continue\n",
        "\n",
        "        label_str = 'Normal' if label_type == 0 else 'Anomaly'\n",
        "\n",
        "        ax.scatter(\n",
        "            features_2d[mask, 0],\n",
        "            features_2d[mask, 1],\n",
        "            c=class_colors[class_name],\n",
        "            marker=label_markers[label_type],\n",
        "            s=label_sizes[label_type],\n",
        "            alpha=0.7,\n",
        "            edgecolors='black',\n",
        "            linewidth=0.5,\n",
        "            label=f'{class_name} - {label_str}'\n",
        "        )\n",
        "\n",
        "ax.set_title('T-SNE Visualization of Feature Space\\n'\n",
        "             'Global PatchCore Model (All Classes)',\n",
        "             fontsize=14, fontweight='bold', pad=20)\n",
        "ax.set_xlabel('T-SNE Dimension 1', fontsize=12)\n",
        "ax.set_ylabel('T-SNE Dimension 2', fontsize=12)\n",
        "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, frameon=True, shadow=True)\n",
        "ax.grid(alpha=0.3, linestyle=':')\n",
        "\n",
        "plt.tight_layout()\n",
        "tsne_plot_path = aths.VISUALIZATIONS / 'tsne_global_model.png'\n",
        "plt.savefig(tsne_plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"T-SNE plot saved to: {tsne_plot_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d558eabf",
      "metadata": {
        "id": "d558eabf"
      },
      "source": [
        "## 10. Summary Table and Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45135eeb",
      "metadata": {
        "id": "45135eeb"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive summary table\n",
        "summary_data = []\n",
        "\n",
        "for method in ['patchcore', 'padim']:\n",
        "    for class_name in CLASSES:\n",
        "        # Per-class model\n",
        "        per_class_res = results_per_class[method][class_name]['image_level']\n",
        "\n",
        "        # Global model\n",
        "        global_res = results_global[method][class_name]\n",
        "\n",
        "        summary_data.append({\n",
        "            'Method': method.upper(),\n",
        "            'Class': class_name,\n",
        "            'Model Type': 'Per-Class',\n",
        "            'AUROC': per_class_res['auroc'],\n",
        "            'AUPRC': per_class_res['auprc'],\n",
        "            'F1': per_class_res['f1'],\n",
        "            'Accuracy': per_class_res['accuracy']\n",
        "        })\n",
        "\n",
        "        summary_data.append({\n",
        "            'Method': method.upper(),\n",
        "            'Class': class_name,\n",
        "            'Model Type': 'Global',\n",
        "            'AUROC': global_res['auroc'],\n",
        "            'AUPRC': global_res['auprc'],\n",
        "            'F1': global_res['f1'],\n",
        "            'Accuracy': global_res['accuracy']\n",
        "        })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "\n",
        "# Display\n",
        "print(\"=\"*70)\n",
        "print(\"FINAL SUMMARY: Per-Class vs Global Model\")\n",
        "print(\"=\"*70)\n",
        "print(df_summary.to_string(index=False))\n",
        "\n",
        "# Save to CSV\n",
        "summary_path = paths.RESULTS / 'global_model_summary.csv'\n",
        "df_summary.to_csv(summary_path, index=False)\n",
        "print(f\"\\nSummary saved to: {summary_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5debcf",
      "metadata": {
        "id": "7c5debcf"
      },
      "outputs": [],
      "source": [
        "# Save complete results dictionary\n",
        "final_results = {\n",
        "    'global_model_results': results_global,\n",
        "    'per_class_model_results': results_per_class,\n",
        "    'performance_gaps': gaps,\n",
        "    'cross_class_confusion_matrix': confusion_matrix_cross.tolist(),\n",
        "    'thresholds_global': thresholds_global,\n",
        "    'metadata': {\n",
        "        'classes': CLASSES,\n",
        "        'global_train_size': len(global_train_images),\n",
        "        'patchcore_coreset_ratio': 0.05,\n",
        "        'seed': 42\n",
        "    }\n",
        "}\n",
        "\n",
        "results_json_path = paths.RESULTS / 'global_model_analysis.json'\n",
        "with open(results_json_path, 'w') as f:\n",
        "    json.dump(final_results, f, indent=2)\n",
        "\n",
        "print(f\"Complete results saved to: {results_json_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a635234a",
      "metadata": {
        "id": "a635234a"
      },
      "source": [
        "Save results (google drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e586331",
      "metadata": {
        "id": "0e586331"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COPY ALL RESULTS TO GOOGLE DRIVE FOR PERSISTENCE\n",
        "# ============================================================\n",
        "\n",
        "import shutil\n",
        "\n",
        "# Create destination folder in Drive\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/anomaly_detection_project')\n",
        "PHASE8_OUTPUTS = DRIVE_ROOT / '10_global_model_outputs'\n",
        "PHASE8_OUTPUTS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COPYING FILES TO GOOGLE DRIVE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nDestination: {PHASE8_OUTPUTS}\")\n",
        "\n",
        "# List of all generated files\n",
        "generated_files = []\n",
        "\n",
        "# Models (Global models)\n",
        "print(\"\\nCopying models...\")\n",
        "model_files = [\n",
        "    MODELS_DIR / 'patchcore_global_clean.npy',\n",
        "    MODELS_DIR / 'patchcore_global_clean_config.pth',\n",
        "    MODELS_DIR / 'padim_global_clean.pt',\n",
        "    MODELS_DIR / 'padim_global_clean.json'\n",
        "]\n",
        "generated_files.extend(model_files)\n",
        "\n",
        "# Results\n",
        "print(\"Copying results...\")\n",
        "result_files = [\n",
        "    RESULTS_DIR / 'global_model_analysis.json',\n",
        "    RESULTS_DIR / 'global_model_summary.csv',\n",
        "    RESULTS_DIR / 'global_vs_per_class_comparison.csv'\n",
        "]\n",
        "generated_files.extend(result_files)\n",
        "\n",
        "# Thresholds\n",
        "print(\"Copying thresholds...\")\n",
        "threshold_files = [\n",
        "    THRESHOLDS_DIR / 'global_thresholds.json'\n",
        "]\n",
        "generated_files.extend(threshold_files)\n",
        "\n",
        "# Visualizations\n",
        "print(\"Copying visualizations...\")\n",
        "viz_files = [\n",
        "    VIZ_DIR / 'global_model_performance_gap.png',\n",
        "    VIZ_DIR / 'identical_shortcut_confusion.png',\n",
        "    VIZ_DIR / 'tsne_global_model.png'\n",
        "]\n",
        "generated_files.extend(viz_files)\n",
        "\n",
        "# Copy all files\n",
        "copied_count = 0\n",
        "missing_count = 0\n",
        "\n",
        "for src_path in generated_files:\n",
        "    if src_path.exists():\n",
        "        # Preserve directory structure\n",
        "        if 'models' in str(src_path):\n",
        "            dst_dir = PHASE8_OUTPUTS / 'models'\n",
        "        elif 'results' in str(src_path):\n",
        "            dst_dir = PHASE8_OUTPUTS / 'results'\n",
        "        elif 'thresholds' in str(src_path):\n",
        "            dst_dir = PHASE8_OUTPUTS / 'thresholds'\n",
        "        elif 'visualizations' in str(src_path):\n",
        "            dst_dir = PHASE8_OUTPUTS / 'visualizations'\n",
        "        else:\n",
        "            dst_dir = PHASE8_OUTPUTS\n",
        "\n",
        "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "        dst_path = dst_dir / src_path.name\n",
        "\n",
        "        shutil.copy2(src_path, dst_path)\n",
        "        print(f\"  ✓ {src_path.name}\")\n",
        "        copied_count += 1\n",
        "    else:\n",
        "        print(f\"  ✗ MISSING: {src_path.name}\")\n",
        "        missing_count += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"✓ Copy complete: {copied_count} files copied, {missing_count} missing\")\n",
        "print(f\"✓ All results saved to: {PHASE8_OUTPUTS}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d01dc51",
      "metadata": {
        "id": "6d01dc51"
      },
      "source": [
        "// TO CHECK\n",
        "## 11. Key Findings and Interpretation\n",
        "\n",
        "### Setting Clarification:\n",
        "This experiment uses the **Model-Unified** setting [CADA, 2024; HierCore, 2025]:\n",
        "- Single model trained on all classes\n",
        "- **Per-class thresholds** at inference (class is known)\n",
        "- This is NOT \"absolute-unified\" (which would require class-agnostic thresholds)\n",
        "\n",
        "### Observations:\n",
        "1. **Performance Gap**: Model-unified approach shows degraded AUROC vs per-class models\n",
        "   - This validates CADA's observation that shared representations struggle with heterogeneous distributions\n",
        "   \n",
        "2. **Cross-Class Confusion**: The confusion matrix shows non-zero false positive rates across classes\n",
        "   - Normal textures from one class can trigger another class's threshold\n",
        "   - This is the \"identical shortcut\" phenomenon [UniAD, You et al. 2022]\n",
        "\n",
        "### Implications:\n",
        "- For industrial deployment with **single-category** quality control: **per-class models remain optimal**\n",
        "- Model-unified approaches are useful when:\n",
        "  - Storage/training efficiency is critical\n",
        "  - Class categories are related (e.g., similar textures)\n",
        "- Absolute-unified remains an open research challenge (see CADA, HierCore for solutions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87ecd0a",
      "metadata": {
        "id": "d87ecd0a"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "### Outputs Generated:\n",
        "1. **Models**: `patchcore_global_clean.npy`, `padim_global_clean.pt`\n",
        "2. **Thresholds**: `global_thresholds.json` (per-class thresholds for global models)\n",
        "3. **Results**: `global_model_analysis.json`, `global_model_summary.csv`\n",
        "4. **Visualizations**:\n",
        "   - `global_model_performance_gap.png` (bar chart)\n",
        "   - `identical_shortcut_confusion.png` (heatmap)\n",
        "   - `tsne_global_model.png` (feature space)\n",
        "\n",
        "- Compare with [You et al., 2022] findings on unified anomaly detection\n",
        "- Discuss implications for industrial deployment"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5fe9198e1540406d8ee5cf1baa5c7065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0401038b0764389b01aa841b569253b",
              "IPY_MODEL_64e0e256a7ef4259b632168cfba8549b",
              "IPY_MODEL_dc8b73a776604cccaed8f29781d28a2c"
            ],
            "layout": "IPY_MODEL_d38687ebe7b04132bcea2cbc4fce4f56"
          }
        },
        "e0401038b0764389b01aa841b569253b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce06ba0588ed4138832e11cb65de8f90",
            "placeholder": "​",
            "style": "IPY_MODEL_061d896c613241d195bb231043256e3f",
            "value": "model.safetensors: 100%"
          }
        },
        "64e0e256a7ef4259b632168cfba8549b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb1441a8812d41b2943cd0b923ac1d0b",
            "max": 275835296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46308d1fddec4d8a9686c3b981beb600",
            "value": 275835296
          }
        },
        "dc8b73a776604cccaed8f29781d28a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dab0f4bfc25f4ac9a0c2f984a55d62e0",
            "placeholder": "​",
            "style": "IPY_MODEL_cf0d3bdced1346d38dc33d1994a99964",
            "value": " 276M/276M [00:04&lt;00:00, 153MB/s]"
          }
        },
        "d38687ebe7b04132bcea2cbc4fce4f56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce06ba0588ed4138832e11cb65de8f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "061d896c613241d195bb231043256e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb1441a8812d41b2943cd0b923ac1d0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46308d1fddec4d8a9686c3b981beb600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dab0f4bfc25f4ac9a0c2f984a55d62e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf0d3bdced1346d38dc33d1994a99964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}