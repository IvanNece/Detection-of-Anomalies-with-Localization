{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e25d9c",
   "metadata": {},
   "source": [
    "# 02 - Data Preparation and Splitting\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/02_data_preparation.ipynb)\n",
    "\n",
    "**Project**: Detection of Anomalies with Localization  \n",
    "**Dataset**: MVTec AD (Hazelnut, Carpet, Zipper)  \n",
    "**Phase**: Data Preparation - Clean Domain Split\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Implement train/val/test split for clean domain\n",
    "2. Create reproducible splits with seed=42\n",
    "3. Verify split integrity and statistics\n",
    "4. Visualize samples from each split\n",
    "5. Test Dataset class with transforms\n",
    "6. Save splits to JSON for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca6602",
   "metadata": {},
   "source": [
    "## Setup - Mount Drive & Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"Cloning repository...\")\n",
    "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git 2>/dev/null || echo \"Repository already exists\"\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('/content/Detection-of-Anomalies-with-Localization')\n",
    "\n",
    "# Dataset location (direct from Drive, no duplication)\n",
    "DATASET_PATH = Path('/content/drive/MyDrive/mvtec_ad')\n",
    "\n",
    "# Output directory for splits\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify dataset exists\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {DATASET_PATH}\\n\"\n",
    "        f\"Please ensure mvtec_ad folder is in your Google Drive root.\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Project:  {PROJECT_ROOT}\")\n",
    "print(f\"Dataset:  {DATASET_PATH}\")\n",
    "print(f\"Output:   {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc6878",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80808d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import Config\n",
    "from src.data.splitter import (\n",
    "    create_clean_split,\n",
    "    create_all_clean_splits,\n",
    "    save_splits,\n",
    "    verify_split\n",
    ")\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.data.transforms import get_clean_transforms\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All imports loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b9f09",
   "metadata": {},
   "source": [
    "## Configuration & Reproducibility\n",
    "\n",
    "**CRITICAL**: Set seed BEFORE any data splitting to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b944b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project configuration\n",
    "config = Config.load(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
    "\n",
    "# âš ï¸ CRITICAL: Set random seed for reproducibility\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Classes to process\n",
    "CLASSES = config.dataset.classes\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Seed: {config.seed}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "print(f\"Train ratio: {config.split.train_ratio}\")\n",
    "print(f\"Val anomaly ratio: {config.split.val_anomaly_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7493766b",
   "metadata": {},
   "source": [
    "## 1. Create Clean Domain Splits\n",
    "\n",
    "### Split Strategy:\n",
    "- **Train-clean**: 80% of `train/good` (only normal images)\n",
    "- **Val-clean**: 20% of `train/good` + 30% of test anomalies\n",
    "- **Test-clean**: all `test/good` + 70% of test anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits for all classes\n",
    "print(\"Creating clean domain splits...\\n\")\n",
    "\n",
    "splits = create_all_clean_splits(\n",
    "    dataset_path=DATASET_PATH,\n",
    "    classes=CLASSES,\n",
    "    train_ratio=config.split.train_ratio,\n",
    "    val_anomaly_ratio=config.split.val_anomaly_ratio,\n",
    "    seed=config.seed\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Splits created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c85fb1",
   "metadata": {},
   "source": [
    "## 2. Verify Split Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a1ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying split integrity...\\n\")\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    stats = verify_split(splits[class_name])\n",
    "    \n",
    "    print(f\"Train:  {stats['train_count']} images\")\n",
    "    print(f\"Val:    {stats['val_count']} images\")\n",
    "    print(f\"Test:   {stats['test_count']} images\")\n",
    "    print(f\"Total:  {stats['total_count']} unique images\")\n",
    "    print(f\"\")\n",
    "    print(f\"No overlap: {'âœ“' if stats['no_overlap'] else 'âœ—'}\")\n",
    "    print(f\"All files exist: {'âœ“' if stats['all_files_exist'] else 'âœ—'}\")\n",
    "    print(f\"Label consistency: {'âœ“' if stats['label_consistency'] else 'âœ—'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ“ All splits verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b18b32",
   "metadata": {},
   "source": [
    "## 3. Detailed Split Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4295a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_data = splits[class_name][split_name]\n",
    "        normal_count = sum(1 for l in split_data['labels'] if l == 0)\n",
    "        anomalous_count = sum(1 for l in split_data['labels'] if l == 1)\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Class': class_name,\n",
    "            'Split': split_name,\n",
    "            'Normal': normal_count,\n",
    "            'Anomalous': anomalous_count,\n",
    "            'Total': len(split_data['images'])\n",
    "        })\n",
    "\n",
    "# Display as table\n",
    "import pandas as pd\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nSPLIT SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Calculate and display percentages\n",
    "print(\"\\n\\nVERIFICATION OF SPLIT RATIOS:\")\n",
    "print(\"=\"*70)\n",
    "for class_name in CLASSES:\n",
    "    # Total train/good images\n",
    "    total_train_good = (\n",
    "        len([l for l in splits[class_name]['train']['labels'] if l == 0]) +\n",
    "        len([l for l in splits[class_name]['val']['labels'] if l == 0])\n",
    "    )\n",
    "    \n",
    "    train_count = len(splits[class_name]['train']['images'])\n",
    "    train_pct = (train_count / total_train_good * 100) if total_train_good > 0 else 0\n",
    "    \n",
    "    # Total anomalies\n",
    "    total_anomalies = (\n",
    "        len([l for l in splits[class_name]['val']['labels'] if l == 1]) +\n",
    "        len([l for l in splits[class_name]['test']['labels'] if l == 1])\n",
    "    )\n",
    "    \n",
    "    val_anomalies = len([l for l in splits[class_name]['val']['labels'] if l == 1])\n",
    "    val_anomaly_pct = (val_anomalies / total_anomalies * 100) if total_anomalies > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Train/Val split from train/good: {train_pct:.1f}% / {100-train_pct:.1f}%\")\n",
    "    print(f\"  Val/Test split for anomalies: {val_anomaly_pct:.1f}% / {100-val_anomaly_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590ca3f",
   "metadata": {},
   "source": [
    "## 4. Visualize Split Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, class_name in enumerate(CLASSES):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for stacked bar chart\n",
    "    splits_names = ['Train', 'Val', 'Test']\n",
    "    normal_counts = []\n",
    "    anomalous_counts = []\n",
    "    \n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        labels = splits[class_name][split_name]['labels']\n",
    "        normal_counts.append(sum(1 for l in labels if l == 0))\n",
    "        anomalous_counts.append(sum(1 for l in labels if l == 1))\n",
    "    \n",
    "    # Stacked bar chart\n",
    "    x = np.arange(len(splits_names))\n",
    "    width = 0.6\n",
    "    \n",
    "    ax.bar(x, normal_counts, width, label='Normal', color='#2ecc71')\n",
    "    ax.bar(x, anomalous_counts, width, bottom=normal_counts, \n",
    "           label='Anomalous', color='#e74c3c')\n",
    "    \n",
    "    ax.set_xlabel('Split', fontsize=12)\n",
    "    ax.set_ylabel('Number of Images', fontsize=12)\n",
    "    ax.set_title(f'{class_name.capitalize()}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(splits_names)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, (n, a) in enumerate(zip(normal_counts, anomalous_counts)):\n",
    "        total = n + a\n",
    "        ax.text(i, total + 5, str(total), ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR.parent.parent / 'outputs' / 'visualizations' / 'split_distribution.png', \n",
    "            dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51fb10",
   "metadata": {},
   "source": [
    "## 5. Test Dataset Class with Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a17063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "transform = get_clean_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    normalize_mean=config.dataset.normalize.mean,\n",
    "    normalize_std=config.dataset.normalize.std\n",
    ")\n",
    "\n",
    "# Create dataset for hazelnut train split\n",
    "test_dataset = MVTecDataset.from_split(\n",
    "    splits['hazelnut']['train'],\n",
    "    transform=transform,\n",
    "    phase='train'\n",
    ")\n",
    "\n",
    "print(f\"Dataset created: {len(test_dataset)} samples\")\n",
    "print(f\"Stats: {test_dataset.get_stats()}\")\n",
    "\n",
    "# Test __getitem__\n",
    "image, mask, label, path = test_dataset[0]\n",
    "print(f\"\\nSample 0:\")\n",
    "print(f\"  Image shape: {image.shape}\")\n",
    "print(f\"  Image dtype: {image.dtype}\")\n",
    "print(f\"  Image range: [{image.min():.3f}, {image.max():.3f}]\")\n",
    "print(f\"  Mask: {mask}\")\n",
    "print(f\"  Label: {label}\")\n",
    "print(f\"  Path: {Path(path).name}\")\n",
    "\n",
    "print(\"\\nâœ“ Dataset class works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a54f5",
   "metadata": {},
   "source": [
    "## 6. Visualize Samples from Each Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"Denormalize image tensor for visualization.\"\"\"\n",
    "    tensor = tensor.clone()\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "def visualize_samples(class_name, splits, transform, num_samples=3):\n",
    "    \"\"\"Visualize samples from train/val/test splits.\"\"\"\n",
    "    fig, axes = plt.subplots(3, num_samples, figsize=(12, 10))\n",
    "    \n",
    "    split_names = ['train', 'val', 'test']\n",
    "    \n",
    "    for row_idx, split_name in enumerate(split_names):\n",
    "        # Create dataset\n",
    "        dataset = MVTecDataset.from_split(\n",
    "            splits[class_name][split_name],\n",
    "            transform=transform,\n",
    "            phase=split_name\n",
    "        )\n",
    "        \n",
    "        # Sample indices (try to get at least one anomalous if available)\n",
    "        indices = []\n",
    "        labels = splits[class_name][split_name]['labels']\n",
    "        \n",
    "        # Get normal samples\n",
    "        normal_indices = [i for i, l in enumerate(labels) if l == 0]\n",
    "        indices.extend(normal_indices[:num_samples-1] if normal_indices else [])\n",
    "        \n",
    "        # Try to get one anomalous sample\n",
    "        anomalous_indices = [i for i, l in enumerate(labels) if l == 1]\n",
    "        if anomalous_indices:\n",
    "            indices.append(anomalous_indices[0])\n",
    "        \n",
    "        # Fill remaining if needed\n",
    "        while len(indices) < num_samples:\n",
    "            indices.append(indices[0] if indices else 0)\n",
    "        \n",
    "        indices = indices[:num_samples]\n",
    "        \n",
    "        for col_idx, idx in enumerate(indices):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            \n",
    "            # Get sample\n",
    "            image, mask, label, path = dataset[idx]\n",
    "            \n",
    "            # Denormalize for visualization\n",
    "            image_vis = denormalize(\n",
    "                image,\n",
    "                config.dataset.normalize.mean,\n",
    "                config.dataset.normalize.std\n",
    "            )\n",
    "            image_vis = image_vis.permute(1, 2, 0).numpy()\n",
    "            image_vis = np.clip(image_vis, 0, 1)\n",
    "            \n",
    "            ax.imshow(image_vis)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            # Title\n",
    "            label_str = 'Normal' if label == 0 else 'Anomalous'\n",
    "            color = 'green' if label == 0 else 'red'\n",
    "            ax.set_title(f\"{split_name.upper()}: {label_str}\", \n",
    "                        fontsize=10, fontweight='bold', color=color)\n",
    "    \n",
    "    fig.suptitle(f'{class_name.capitalize()} - Sample Images', \n",
    "                fontsize=14, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for each class\n",
    "for class_name in CLASSES:\n",
    "    visualize_samples(class_name, splits, transform, num_samples=3)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935c6b7",
   "metadata": {},
   "source": [
    "## 7. Save Splits to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdb593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'created_at': datetime.now().isoformat(),\n",
    "    'seed': config.seed,\n",
    "    'train_ratio': config.split.train_ratio,\n",
    "    'val_anomaly_ratio': config.split.val_anomaly_ratio,\n",
    "    'classes': CLASSES,\n",
    "    'dataset_path': str(DATASET_PATH),\n",
    "    'image_size': config.dataset.image_size,\n",
    "    'normalize_mean': config.dataset.normalize.mean,\n",
    "    'normalize_std': config.dataset.normalize.std\n",
    "}\n",
    "\n",
    "# Save splits\n",
    "output_path = OUTPUT_DIR / 'clean_splits.json'\n",
    "save_splits(splits, output_path, metadata=metadata)\n",
    "\n",
    "print(f\"\\nâœ“ Splits saved to: {output_path}\")\n",
    "print(f\"  File size: {output_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c88f88",
   "metadata": {},
   "source": [
    "## 8. Test Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e08e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing reproducibility...\\n\")\n",
    "\n",
    "# Create splits again with same seed\n",
    "splits_test = create_clean_split(\n",
    "    class_name='hazelnut',\n",
    "    dataset_path=DATASET_PATH,\n",
    "    train_ratio=config.split.train_ratio,\n",
    "    val_anomaly_ratio=config.split.val_anomaly_ratio,\n",
    "    seed=config.seed\n",
    ")\n",
    "\n",
    "# Compare with original\n",
    "original = splits['hazelnut']\n",
    "\n",
    "matches = {\n",
    "    'train': original['train']['images'] == splits_test['train']['images'],\n",
    "    'val': original['val']['images'] == splits_test['val']['images'],\n",
    "    'test': original['test']['images'] == splits_test['test']['images']\n",
    "}\n",
    "\n",
    "print(\"Reproducibility check:\")\n",
    "for split_name, match in matches.items():\n",
    "    status = \"âœ“\" if match else \"âœ—\"\n",
    "    print(f\"  {split_name}: {status}\")\n",
    "\n",
    "if all(matches.values()):\n",
    "    print(\"\\nâœ“ Splits are perfectly reproducible!\")\n",
    "else:\n",
    "    print(\"\\nâœ— Warning: Splits are not reproducible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4898d0a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### âœ… Completed Tasks:\n",
    "\n",
    "1. **Split Creation**: Created train/val/test splits for all 3 classes\n",
    "2. **Integrity Verification**: Verified no overlap and file existence\n",
    "3. **Statistics**: Calculated and displayed split statistics\n",
    "4. **Visualization**: Visualized split distribution and sample images\n",
    "5. **Dataset Class**: Tested MVTecDataset with transforms\n",
    "6. **Persistence**: Saved splits to `clean_splits.json`\n",
    "7. **Reproducibility**: Verified splits are reproducible with seed=42\n",
    "\n",
    "### ðŸ“ Output Files:\n",
    "\n",
    "- `data/processed/clean_splits.json` - Split definitions for all classes\n",
    "- `outputs/visualizations/split_distribution.png` - Split distribution chart\n",
    "\n",
    "### ðŸ“Š Next Steps:\n",
    "\n",
    "- **Step 2.1**: Implement domain shift transformations\n",
    "- **Step 2.2**: Generate MVTec-Shift dataset\n",
    "- **Step 3.1**: Implement PatchCore backbone"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
