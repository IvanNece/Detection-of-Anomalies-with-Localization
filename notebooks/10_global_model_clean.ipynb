{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/10_global_model_clean.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Model Training and Evaluation\n",
    "\n",
    "This notebook implements the \"Global Model\" strategy, where a single anomaly detection model (PatchCore and PaDiM) is trained on **all 3 classes** (Hazelnut, Carpet, Zipper) simultaneously.\n",
    "\n",
    "The goal is to evaluate the performance degradation caused by modeling a heterogeneous normal distribution, as discussed in the UniAD paper (You et al., 2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.models.patchcore import PatchCore\n",
    "from src.models.padim_wrapper import PadimWrapper\n",
    "from src.utils.paths import ProjectPaths\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.evaluation.evaluator import Evaluator, evaluate_model_on_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Setup\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "paths = ProjectPaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Classes\n",
    "CLASSES = ['hazelnut', 'carpet', 'zipper']\n",
    "\n",
    "# Load clean splits\n",
    "splits_path = paths.data_processed / 'clean_splits.json'\n",
    "with open(splits_path, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "print(\"Splits loaded for classes:\", list(splits.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Global Training Data\n",
    "\n",
    "We merge the training sets of all three classes into a single Global Training Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Global Training Data\n",
    "global_train_images = []\n",
    "global_train_labels = []\n",
    "global_train_masks = []\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    class_split = splits[class_name]\n",
    "    # Training data is always normal (label 0, mask None)\n",
    "    train_imgs = class_split['train']['images']\n",
    "    train_lbls = class_split['train']['labels']\n",
    "    \n",
    "    global_train_images.extend(train_imgs)\n",
    "    global_train_labels.extend(train_lbls)\n",
    "    global_train_masks.extend([None] * len(train_imgs))\n",
    "\n",
    "print(f\"Global Training Set Size: {len(global_train_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define standard transform (ImageNet normalization)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create Global Dataset and Standard standard DataLoader\n",
    "global_train_dataset = MVTecDataset(\n",
    "    images=global_train_images,\n",
    "    masks=global_train_masks,\n",
    "    labels=global_train_labels,\n",
    "    transform=transform,\n",
    "    phase='train'\n",
    ")\n",
    "\n",
    "global_train_loader = DataLoader(\n",
    "    global_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False, # No need to shuffle for feature extraction/stats\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Global PatchCore\n",
    "\n",
    "We train a single PatchCore model on the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Global PatchCore...\")\n",
    "patchcore = PatchCore(\n",
    "    backbone_layers=['layer2', 'layer3'],\n",
    "    patch_size=3,\n",
    "    coreset_ratio=0.01, # Keep 1% of total patches\n",
    "    n_neighbors=9,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Training Global PatchCore...\")\n",
    "patchcore.fit(global_train_loader)\n",
    "\n",
    "print(\"Saving Global PatchCore...\")\n",
    "patchcore.save(paths.models, class_name='global', domain='clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Global PaDiM\n",
    "\n",
    "We train a single PaDiM model on the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Global PaDiM...\")\n",
    "padim = PadimWrapper(\n",
    "    backbone='wide_resnet50_2',\n",
    "    layers=['layer1', 'layer2', 'layer3'],\n",
    "    n_features=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Training Global PaDiM...\")\n",
    "padim.fit(global_train_loader)\n",
    "\n",
    "print(\"Saving Global PaDiM...\")\n",
    "padim.save(paths.models / 'padim_global_clean.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Global Models\n",
    "\n",
    "We evaluate the Global Models on each class's Test Set individually.\n",
    "We calibrate a threshold for each class using its specific Validation Set, then compute metrics on its Test Set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_global_model(model, model_name, classes, splits, transform, device):\n",
    "    print(f\"\\nEvaluating Global {model_name}...\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        print(f\"\\n--- Evaluating on {class_name} ---\")\n",
    "        class_split = splits[class_name]\n",
    "        \n",
    "        # 1. Create DataLoaders\n",
    "        val_dataset = MVTecDataset(\n",
    "            images=class_split['val']['images'],\n",
    "            masks=class_split['val']['masks'],\n",
    "            labels=class_split['val']['labels'],\n",
    "            transform=transform,\n",
    "            phase='test' # Val is treated as test for loading\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        test_dataset = MVTecDataset(\n",
    "            images=class_split['test']['images'],\n",
    "            masks=class_split['test']['masks'],\n",
    "            labels=class_split['test']['labels'],\n",
    "            transform=transform,\n",
    "            phase='test'\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # 2. Initialize Evaluator\n",
    "        evaluator = Evaluator(model_name, class_name, domain='clean')\n",
    "        \n",
    "        # 3. Validation & Calibration\n",
    "        print(\"Running Validation...\")\n",
    "        val_results = evaluate_model_on_dataloader(\n",
    "            model, val_loader, device, return_heatmaps=False, verbose=False\n",
    "        )\n",
    "        evaluator.calibrate_threshold(val_results['scores'], val_results['labels'])\n",
    "        \n",
    "        # 4. Test Evaluation\n",
    "        print(\"Running Test Evaluation...\")\n",
    "        test_results = evaluate_model_on_dataloader(\n",
    "            model, test_loader, device, return_heatmaps=True, verbose=True\n",
    "        )\n",
    "        \n",
    "        # Image-level metrics\n",
    "        evaluator.evaluate_image_level(test_results['scores'], test_results['labels'])\n",
    "        \n",
    "        # Pixel-level metrics\n",
    "        evaluator.evaluate_pixel_level(test_results['masks'], test_results['heatmaps'])\n",
    "        \n",
    "        # Save results with 'global' prefix\n",
    "        evaluator.save_results(paths.results, prefix='global')\n",
    "        \n",
    "        # Store results + curves + raw scores for visualization\n",
    "        class_res = evaluator.get_results()\n",
    "        class_res['roc_curve'] = evaluator.roc_curve\n",
    "        class_res['pr_curve'] = evaluator.pr_curve\n",
    "        class_res['test_scores'] = test_results['scores']\n",
    "        class_res['test_labels'] = test_results['labels']\n",
    "        \n",
    "        results[class_name] = class_res\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PatchCore\n",
    "patchcore_results = evaluate_global_model(\n",
    "    patchcore, 'patchcore', CLASSES, splits, transform, device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PaDiM\n",
    "padim_results = evaluate_global_model(\n",
    "    padim, 'padim', CLASSES, splits, transform, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Summary & Visualization\n",
    "\n",
    "Comparison of per-class performance for the Global Models and qualitative visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plotting Functions\n",
    "def plot_curves(results_dict, model_name):\n",
    "    \"\"\"Plot ROC and PR curves for each class.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle(f'Global {model_name} - Evaluation Curves', fontsize=16)\n",
    "    \n",
    "    classes = list(results_dict.keys())\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        res = results_dict[class_name]\n",
    "        \n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = res['roc_curve']\n",
    "        auroc = res['image_level']['auroc']\n",
    "        \n",
    "        axes[0, i].plot(fpr, tpr, label=f'AUROC = {auroc:.4f}')\n",
    "        axes[0, i].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        axes[0, i].set_title(f'{class_name.title()} - ROC Curve')\n",
    "        axes[0, i].set_xlabel('False Positive Rate')\n",
    "        axes[0, i].set_ylabel('True Positive Rate')\n",
    "        axes[0, i].legend()\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # PR Curve\n",
    "        precision, recall, _ = res['pr_curve']\n",
    "        auprc = res['image_level']['auprc']\n",
    "        \n",
    "        axes[1, i].plot(recall, precision, label=f'AUPRC = {auprc:.4f}')\n",
    "        axes[1, i].set_title(f'{class_name.title()} - PR Curve')\n",
    "        axes[1, i].set_xlabel('Recall')\n",
    "        axes[1, i].set_ylabel('Precision')\n",
    "        axes[1, i].legend()\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(results_dict, model_name):\n",
    "    \"\"\"Plot Confusion Matrix for each class.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    fig.suptitle(f'Global {model_name} - Confusion Matrices', fontsize=16)\n",
    "    \n",
    "    classes = list(results_dict.keys())\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        res = results_dict[class_name]\n",
    "        \n",
    "        # Recompute confusion matrix at optimal threshold\n",
    "        scores = res['test_scores']\n",
    "        labels = res['test_labels']\n",
    "        threshold = res['threshold']\n",
    "        \n",
    "        preds = (scores >= threshold).astype(int)\n",
    "        cm = confusion_matrix(labels, preds)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                    xticklabels=['Normal', 'Anomaly'],\n",
    "                    yticklabels=['Normal', 'Anomaly'])\n",
    "        axes[i].set_title(f'{class_name.title()}')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('True')\n",
    "        \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def visualize_global_results(results_dict, model_name):\n",
    "    # Existing table summary\n",
    "    print(f\"\\nGlobal {model_name} Performance Summary:\")\n",
    "    print(f\"{'Class':<15} {'AUROC':<10} {'F1':<10} {'PRO':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    avg_auroc, avg_f1, avg_pro = 0, 0, 0\n",
    "    \n",
    "    for class_name, res in results_dict.items():\n",
    "        auroc = res['image_level']['auroc']\n",
    "        f1 = res['image_level']['f1']\n",
    "        pro = res['pixel_level'].get('pro', 0)\n",
    "        if pro is None: pro = 0\n",
    "        \n",
    "        print(f\"{class_name:<15} {auroc:.4f}     {f1:.4f}     {pro:.4f}\")\n",
    "        \n",
    "        avg_auroc += auroc\n",
    "        avg_f1 += f1\n",
    "        avg_pro += pro\n",
    "        \n",
    "    print(\"-\" * 45)\n",
    "    print(f\"{'AVERAGE':<15} {avg_auroc/3:.4f}     {avg_f1/3:.4f}     {avg_pro/3:.4f}\")\n",
    "    \n",
    "    # New Plots\n",
    "    plot_curves(results_dict, model_name)\n",
    "    plot_confusion_matrices(results_dict, model_name)\n",
    "\n",
    "# Run Visualization\n",
    "visualize_global_results(patchcore_results, 'PatchCore')\n",
    "visualize_global_results(padim_results, 'PaDiM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Function\n",
    "def show_sample_predictions(model, class_name, splits, transform, device, n_samples=3):\n",
    "    print(f\"Visualizing anomalies for {class_name}...\")\n",
    "    class_split = splits[class_name]\n",
    "    dataset = MVTecDataset(\n",
    "        images=class_split['test']['images'],\n",
    "        masks=class_split['test']['masks'],\n",
    "        labels=class_split['test']['labels'],\n",
    "        transform=transform,\n",
    "        phase='test'\n",
    "    )\n",
    "    # Shuffle to get random samples\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model.eval()\n",
    "    images, masks, labels, _ = next(iter(loader))\n",
    "    \n",
    "    # Filter for anomalies\n",
    "    anom_indices = [i for i, l in enumerate(labels) if l == 1]\n",
    "    if len(anom_indices) < n_samples:\n",
    "        indices = range(min(n_samples, len(images)))\n",
    "    else:\n",
    "        indices = anom_indices[:n_samples]\n",
    "        \n",
    "    batch_images = images[indices].to(device)\n",
    "    _, heatmaps = model.predict(batch_images, return_heatmaps=True)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(len(indices), 3, figsize=(15, 5*len(indices)))\n",
    "    if len(indices) == 1:\n",
    "        axes = np.array([axes])\n",
    "    if len(indices) > 1 and len(axes.shape) == 1: # Handle 1D array if subplots returns 1D\n",
    "         axes = axes.reshape(-1, 3)\n",
    "        \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Original Image\n",
    "        img = images[idx].permute(1, 2, 0).numpy()\n",
    "        img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406] # Denormalize\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Mask\n",
    "        mask = masks[idx]\n",
    "        if mask is not None:\n",
    "            mask = mask.numpy().squeeze()\n",
    "            \n",
    "        # Heatmap\n",
    "        hm = heatmaps[i]\n",
    "        \n",
    "        ax_row = axes[i]\n",
    "        \n",
    "        ax_row[0].imshow(img)\n",
    "        ax_row[0].set_title(f\"{class_name} Image\")\n",
    "        ax_row[0].axis('off')\n",
    "        \n",
    "        if mask is not None:\n",
    "            ax_row[1].imshow(mask, cmap='gray')\n",
    "        ax_row[1].set_title(\"Ground Truth\")\n",
    "            \n",
    "        ax_row[1].axis('off')\n",
    "        \n",
    "        im = ax_row[2].imshow(hm, cmap='jet')\n",
    "        ax_row[2].set_title(\"Global Model Heatmap\")\n",
    "        ax_row[2].axis('off')\n",
    "        plt.colorbar(im, ax=ax_row[2])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize for each class (using PatchCore as example)\n",
    "print(\"\\n--- Visualizing Global PatchCore Predictions ---\")\n",
    "for class_name in CLASSES:\n",
    "    show_sample_predictions(patchcore, class_name, splits, transform, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
