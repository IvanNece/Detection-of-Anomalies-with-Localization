% ============================================================================
% 4. Experimental Setup
% ============================================================================
\section{Experimental Setup}
\label{sec:experimental_setup}

This section describes the dataset, evaluation protocol, and experimental scenarios employed to validate the proposed anomaly detection methods. We detail the data splits, threshold calibration procedure, evaluation metrics, and the different domain settings under which models are tested.

% ----------------------------------------------------------------------------
\subsection{Dataset and Splits}
\label{sec:dataset_splits}

\paragraph{MVTec AD.}
We conduct experiments on the MVTec Anomaly Detection (MVTec~AD) dataset~\cite{bergmann2019mvtec}, a comprehensive benchmark for unsupervised anomaly detection in industrial inspection. We select three representative categories: \textbf{Hazelnut} (object), \textbf{Carpet} (texture), and \textbf{Zipper} (mixed), covering the diversity of defect types and visual structures present in the dataset. Each category contains defect-free (normal) training images and a test set with both normal and anomalous samples, accompanied by pixel-level ground truth masks for localization evaluation.

\paragraph{Data Splitting Protocol.}
Following the one-class classification paradigm, we construct separate train/validation/test splits for each category. The original MVTec~AD structure provides a \texttt{train/good} folder (normal images only) and \texttt{test/} folder containing both normal (\texttt{good}) and anomalous images organized by defect type. We partition these as follows:

\begin{itemize}
    \item \textbf{Train-clean:} 80\% of images from \texttt{train/good}, used exclusively to build the nominal representation (\ie, memory bank for PatchCore, Gaussian parameters for PaDiM).
    \item \textbf{Val-clean:} Remaining 20\% of \texttt{train/good} (normal) plus 30\% of the anomalous images from \texttt{test/<defect>}, used for threshold calibration and hyperparameter tuning.
    \item \textbf{Test-clean:} All remaining normal images from \texttt{test/good} and all remaining anomalous images from \texttt{test/<defect>}, with ground truth masks. Reserved strictly for final evaluation.
\end{itemize}

The 80/20 train/validation ratio ensures sufficient training data for robust feature extraction while retaining a representative validation set. Crucially, \textbf{anomalous samples are included in the validation set} (at 30\% of available anomalies) to enable threshold calibration that accounts for the score distribution of both classes. This design follows established practice in anomaly detection, where access to a small number of labeled validation anomalies is realistic and necessary for setting decision thresholds~\cite{roth2022patchcore}.

All splits are generated reproducibly using a fixed random seed (42), with split indices stored in JSON format to ensure consistency across experiments.

\paragraph{Split Statistics.}
Table~\ref{tab:split_statistics} summarizes the dataset composition for each category and split.

\begin{table}[t]
\centering
\caption{Dataset split statistics for the three selected MVTec~AD categories. Val-clean includes both normal (from \texttt{train/good}) and anomalous samples (30\% from \texttt{test/}) for threshold calibration.}
\label{tab:split_statistics}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l cc cc cc}
\toprule
\multirow{2}{*}{\textbf{Category}} & \multicolumn{2}{c}{\textbf{Train-clean}} & \multicolumn{2}{c}{\textbf{Val-clean}} & \multicolumn{2}{c}{\textbf{Test-clean}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& Normal & Anom. & Normal & Anom. & Normal & Anom. \\
\midrule
Hazelnut & 312 & 0 & 79 & 21 & 40 & 49 \\
Carpet   & 224 & 0 & 56 & 26 & 28 & 63 \\
Zipper   & 192 & 0 & 48 & 36 & 32 & 84 \\
\midrule
\textbf{Total} & 728 & 0 & 183 & 83 & 100 & 196 \\
\bottomrule
\end{tabular}%
}
\end{table}

% ----------------------------------------------------------------------------
\subsection{Evaluation Protocol}
\label{sec:evaluation_protocol}

\paragraph{Threshold Selection.}
For each class and method, we calibrate a decision threshold on the validation set using an F1-optimal grid search. Specifically, we:
\begin{enumerate}
    \item Compute image-level anomaly scores for all validation samples (normal + anomalous).
    \item Search over 1000 uniformly spaced thresholds between the minimum and maximum validation scores.
    \item Select the threshold $\tau^*$ that maximizes the F1 score on the validation set:
\end{enumerate}
\begin{equation}
    \tau^* = \arg\max_{\tau} \text{F1}(\tau) = \arg\max_{\tau} \frac{2 \cdot \text{Prec}(\tau) \cdot \text{Rec}(\tau)}{\text{Prec}(\tau) + \text{Rec}(\tau)}
\end{equation}

\paragraph{Image-Level Metrics.}
We evaluate detection performance using both threshold-independent and threshold-dependent metrics:
\begin{itemize}
    \item \textbf{AUROC:} Area Under the Receiver Operating Characteristic curve, measuring the model's ability to rank anomalous images higher than normal ones across all thresholds.
    \item \textbf{AUPRC:} Area Under the Precision-Recall Curve, particularly informative for imbalanced datasets where anomalies are the minority class.
    \item \textbf{F1 Score:} Harmonic mean of precision and recall at the calibrated threshold $\tau^*$.
\end{itemize}

\paragraph{Pixel-Level Metrics.}
For localization evaluation, we compute:
\begin{itemize}
    \item \textbf{Pixel AUROC:} ROC-AUC computed over all pixels from the entire test set, measuring discrimination between defective and non-defective pixels.
    \item \textbf{PRO (Per-Region Overlap):} Following the protocol of Bergmann~\etal~\cite{bergmann2019mvtec}, we compute the area under the per-region overlap curve integrated up to a false positive rate of 0.3. PRO penalizes predictions that miss or only partially cover ground truth anomaly regions.
\end{itemize}

All metrics are reported per-class and as macro-averages across the three categories to provide both granular and aggregate performance views.

% ----------------------------------------------------------------------------
\subsection{Evaluation Scenarios}
\label{sec:evaluation_scenarios}

We evaluate models under four distinct scenarios to assess both baseline performance and robustness to domain shift:

\begin{enumerate}
    \item \textbf{Test-Clean (Baseline):} Models trained on Train-clean are evaluated on Test-clean using thresholds calibrated on Val-clean. This represents the idealized scenario where training and test conditions are matched.
    
    \item \textbf{Test-Shift (No Adaptation):} Clean-trained models are evaluated on the shifted test set (Test-shift) using the \textit{same thresholds calibrated on Val-clean}. This scenario measures pure performance degradation without any adaptation mechanism.
    
    \item \textbf{Test-Shift (Threshold-Only Adaptation):} Models remain trained on Train-clean (no retraining), but thresholds are \textit{re-calibrated on Val-shift}. This isolates the contribution of threshold adaptation.
    
    \item \textbf{Test-Shift (Full Adaptation):} Models are \textit{retrained from scratch on Train-shift} and thresholds are calibrated on Val-shift. This represents the upper bound of adaptation performance.
\end{enumerate}

% ----------------------------------------------------------------------------
\subsubsection{Global Model (Model-Unified Setting)}
\label{sec:global_model}

In addition to per-class models, we train a \textbf{single global model} on pooled normal data from all three categories (Figure~\ref{fig:schema_unified}). This setup follows the \textit{Model-Unified} setting as defined by Heo and Kang~\cite{heo2025hiercore}:

\begin{itemize}
    \item \textbf{Training:} One shared model is fitted on the concatenated training data from Hazelnut, Carpet, and Zipper.
    \item \textbf{Inference:} Class identity is assumed \textit{known} at test time.
    \item \textbf{Thresholds:} \textit{Per-class thresholds} are calibrated on each class's validation set using the global model's predictions.
\end{itemize}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{img/SchemaUnified.jpg}
    \caption{Overview of the Model-Unified setting. Normal training data from all three categories (Hazelnut, Carpet, Zipper) is merged into a single training set. One unified PatchCore model and one unified PaDiM model are trained on this pooled data. At inference time, class identity is known and per-class thresholds are applied accordingly.}
    \label{fig:schema_unified}
\end{figure}

This is distinct from the more challenging \textit{Absolute-Unified} setting~\cite{guo2024cada}, where class identity is unknown and a single global threshold must be used. The Model-Unified setting is specifically designed to investigate the \textbf{``identical shortcut'' problem}~\cite{you2022uniad}: when a single model is trained on multiple classes, it may learn to distinguish between classes rather than between normal and anomalous samples within each class. We analyze this phenomenon by examining cross-class confusion patterns and feature space separation (see Section~\ref{sec:results}).

% ----------------------------------------------------------------------------
\subsection{Methods Under Evaluation}
\label{sec:methods_evaluated}

All evaluation scenarios described above are conducted on both PatchCore and PaDiM to enable direct comparison. PaDiM serves as a baseline method, providing a reference point for assessing PatchCore's performance across all experimental conditions. The two methods share the same preprocessing pipeline, data splits, and evaluation metrics, differing only in their core anomaly scoring mechanisms.

\paragraph{Coreset Ablation (PatchCore Only).}
The coreset ratio ablation study (Section~\ref{sec:ablation_coreset}) is conducted exclusively on PatchCore, as coreset subsampling is not applicable to PaDiM's parametric Gaussian modeling.

% ----------------------------------------------------------------------------
\subsection{Ablation Study: Coreset Ratio}
\label{sec:ablation_coreset}

The coreset sampling ratio $\rho$ is a critical hyperparameter for PatchCore, controlling the trade-off between memory bank coverage and computational efficiency. Following the range suggested by Roth~\etal~\cite{roth2022patchcore} (1--10\%), we conduct an ablation study to evaluate the impact of this parameter.

\paragraph{Experimental Design.}
We train three PatchCore variants with coreset ratios $\rho \in \{1\%, 5\%, 10\%\}$ on the shifted domain (Train-shift), calibrate thresholds on Val-shift, and evaluate on Test-shift. This configuration isolates the effect of coreset size by keeping the domain fixed (shifted), avoiding confounding factors from domain mismatch.

For each configuration, we record:
\begin{itemize}
    \item \textbf{Detection metrics:} AUROC, AUPRC, F1, and Accuracy on Test-shift.
    \item \textbf{Efficiency metrics:} Memory bank size (number of retained patches) and training time.
\end{itemize}

Smaller coreset ratios reduce memory usage and accelerate nearest neighbor search during inference, but may underfit by discarding representative patches. Larger ratios improve coverage but increase computational cost quadratically during coreset selection. The ablation quantifies this trade-off empirically, justifying the selection of $\rho = 0.05$ as the default configuration for all other experiments. Results are reported in Section~\ref{sec:results}.

% ----------------------------------------------------------------------------
\subsection{Implementation Details}
\label{sec:implementation}

All experiments are executed on a single NVIDIA T4 GPU provided by Google Colab. The complete codebase, including data preprocessing, model training, and evaluation pipelines, is implemented as a collection of Jupyter notebooks designed for reproducibility. All random operations use a fixed seed of 42.

For implementation specifics of PatchCore (backbone, feature layers, coreset subsampling, score reweighting) and PaDiM (layers, dimensionality reduction, Gaussian modeling), we refer to Section~\ref{sec:methodology}. The \texttt{anomalib} library is used for PaDiM, while PatchCore is implemented from scratch following Roth~\etal~\cite{roth2022patchcore}.

