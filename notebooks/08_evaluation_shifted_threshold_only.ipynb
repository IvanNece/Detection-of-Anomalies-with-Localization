{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS0qKQwoLocM"
      },
      "source": [
        "# 08 - Evaluation on Test-Shift with Threshold-Only Adaptation\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/08_evaluation_shifted_threshold_only.ipynb)\n",
        "\n",
        "**Phase 6.2: Threshold-Only Adaptation**\n",
        "\n",
        "This notebook implements an ablation study to isolate the contribution of **threshold recalibration** from full model adaptation.\n",
        "\n",
        "**Experimental Setup:**\n",
        "- Models: Trained on Clean data (NO retraining)\n",
        "- Thresholds: RE-calibrated on Val-shift (F1-optimal)\n",
        "- Test: Evaluated on Test-shift\n",
        "\n",
        "**Goal:**\n",
        "Measure how much performance can be recovered through threshold adaptation alone, without model retraining.\n",
        "\n",
        "**Metrics computed:**\n",
        "- Image-level: AUROC, AUPRC, F1, Accuracy, Precision, Recall\n",
        "- Pixel-level: Pixel AUROC, PRO (Per-Region Overlap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6FLGl_sLocO"
      },
      "source": [
        "## 1. Setup & Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cWKmXSGLocO"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SETUP - Mount Google Drive & Clone Repository\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# Clone repository on main branch\n",
        "print(\"Cloning repository (branch: main)...\")\n",
        "repo_dir = '/content/Detection-of-Anomalies-with-Localization'\n",
        "\n",
        "# Remove if exists\n",
        "if os.path.exists(repo_dir):\n",
        "    print(\"Removing existing repository...\")\n",
        "    !rm -rf {repo_dir}\n",
        "\n",
        "# Clone from main branch\n",
        "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git {repo_dir}\n",
        "print(\"Done!\\n\")\n",
        "\n",
        "# Setup paths\n",
        "PROJECT_ROOT = Path(repo_dir)\n",
        "\n",
        "# Dataset location\n",
        "DATASET_PATH = Path('/content/drive/MyDrive/mvtec_shifted')\n",
        "\n",
        "# Output directories on Drive\n",
        "DRIVE_ROOT = Path('/content/drive/MyDrive/anomaly_detection_project')\n",
        "PATCHCORE_MODELS_DIR = DRIVE_ROOT / '04_patchcore_clean_outputs'\n",
        "PADIM_MODELS_DIR = DRIVE_ROOT / '05_padim_clean_outputs'\n",
        "\n",
        "RESULTS_DIR = PROJECT_ROOT / 'outputs' / 'results'\n",
        "THRESHOLDS_DIR = PROJECT_ROOT / 'outputs' / 'thresholds'\n",
        "VIZ_DIR = PROJECT_ROOT / 'outputs' / 'visualizations' / 'shift_threshold_only'\n",
        "\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "THRESHOLDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Verify dataset exists\n",
        "if not DATASET_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Dataset not found at {DATASET_PATH}\\n\"\n",
        "        f\"Please ensure mvtec_shifted folder is in your Google Drive root.\"\n",
        "    )\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SETUP COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Project:   {PROJECT_ROOT}\")\n",
        "print(f\"Dataset:   {DATASET_PATH}\")\n",
        "print(f\"PatchCore: {PATCHCORE_MODELS_DIR}\")\n",
        "print(f\"PaDiM:     {PADIM_MODELS_DIR}\")\n",
        "print(f\"Results:   {RESULTS_DIR}\")\n",
        "print(f\"Viz:       {VIZ_DIR}\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clCBlHP22zrm"
      },
      "source": [
        "Install both `faiss` and `anomalib` libraries, required for running PatchCore and PaDiM models respectively. **Must be done before any imports.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_4LD7xQN4eG"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu --quiet\n",
        "!pip install anomalib --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q75OIqcq2zrn"
      },
      "source": [
        "Import necessary libraries and modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHW-601iLocP"
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Project imports\n",
        "from src.utils.reproducibility import set_seed\n",
        "from src.utils.config import load_config\n",
        "from src.utils.paths import ProjectPaths\n",
        "from src.data.splitter import load_splits\n",
        "from src.data.dataset import MVTecDataset\n",
        "from src.data.transforms import get_clean_transforms\n",
        "from src.models.patchcore import PatchCore\n",
        "from src.models.padim_wrapper import PadimWrapper\n",
        "\n",
        "# Metrics imports\n",
        "from src.metrics import (\n",
        "    calibrate_threshold,\n",
        "    ThresholdCalibrator,\n",
        "    compute_image_metrics,\n",
        "    compute_pixel_metrics,\n",
        "    compute_roc_curve,\n",
        "    compute_pr_curve,\n",
        "    compute_confusion_matrix,\n",
        "    aggregate_metrics,\n",
        "    aggregate_pixel_metrics\n",
        ")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Load configuration\n",
        "config = load_config(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
        "paths = ProjectPaths(PROJECT_ROOT)\n",
        "\n",
        "# Classes to evaluate\n",
        "CLASSES = config.dataset.classes  # ['hazelnut', 'carpet', 'zipper']\n",
        "\n",
        "# Device\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def evaluation_collate(batch):\n",
        "    \"\"\"Custom collate function to handle None masks in batches.\"\"\"\n",
        "    batch = list(zip(*batch))\n",
        "    images = torch.stack(batch[0])\n",
        "    masks = batch[1] # Keep as tuple/list to handle None\n",
        "    labels = torch.tensor(batch[2])\n",
        "    paths = batch[3]\n",
        "    return images, masks, labels, paths\n",
        "\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "print(f\"Classes: {CLASSES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr33gpjFLocQ"
      },
      "source": [
        "## 2. Load Shifted Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJGKAl9OLocQ"
      },
      "outputs": [],
      "source": [
        "# Load shifted splits\n",
        "SPLITS_PATH = paths.get_split_path('shifted')\n",
        "splits = load_splits(SPLITS_PATH)\n",
        "\n",
        "# Print split statistics\n",
        "print(\"\\nSplit Statistics:\")\n",
        "print(\"-\" * 50)\n",
        "for class_name in CLASSES:\n",
        "    val_n = len(splits[class_name]['val']['images'])\n",
        "    test_n = len(splits[class_name]['test']['images'])\n",
        "\n",
        "    val_normal = sum(1 for l in splits[class_name]['val']['labels'] if l == 0)\n",
        "    val_anom = sum(1 for l in splits[class_name]['val']['labels'] if l == 1)\n",
        "    test_normal = sum(1 for l in splits[class_name]['test']['labels'] if l == 0)\n",
        "    test_anom = sum(1 for l in splits[class_name]['test']['labels'] if l == 1)\n",
        "\n",
        "    print(f\"{class_name}:\")\n",
        "    print(f\"  Val: {val_n} ({val_normal} normal, {val_anom} anomalous)\")\n",
        "    print(f\"  Test: {test_n} ({test_normal} normal, {test_anom} anomalous)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drvMhSzLLocR"
      },
      "source": [
        "## 3. Threshold Calibration on Val-Shift\n",
        "\n",
        "**CRITICAL STEP:** We recalibrate thresholds using predictions from **clean-trained models** on **Val-shift** data.\n",
        "\n",
        "This isolates the effect of threshold adaptation from model adaptation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBV60yftLocR"
      },
      "outputs": [],
      "source": [
        "# Initialize threshold calibrators\n",
        "patchcore_calibrator = ThresholdCalibrator('patchcore')\n",
        "padim_calibrator = ThresholdCalibrator('padim')\n",
        "\n",
        "# Get transforms\n",
        "transform = get_clean_transforms(image_size=config.dataset.image_size)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"THRESHOLD CALIBRATION (F1-Optimal on Val-shift)\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PATCHCORE: Calibrate thresholds on Val-shift\n",
        "print(\"\\n>>> PATCHCORE <<<\\n\")\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    print(f\"\\n--- {class_name.upper()} ---\")\n",
        "\n",
        "    # Create validation dataset\n",
        "    val_split = splits[class_name]['val']\n",
        "    val_dataset = MVTecDataset.from_split(\n",
        "        val_split,\n",
        "        transform=transform,\n",
        "        phase='val'\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=evaluation_collate\n",
        "    )\n",
        "\n",
        "    # Load model trained on CLEAN data\n",
        "    model = PatchCore(\n",
        "        backbone_layers=config.patchcore.layers,\n",
        "        patch_size=config.patchcore.patch_size,\n",
        "        coreset_ratio=config.patchcore.coreset_sampling_ratio,\n",
        "        n_neighbors=config.patchcore.n_neighbors,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    model.load(PATCHCORE_MODELS_DIR, class_name, domain='clean')\n",
        "\n",
        "    # Collect predictions on VAL-SHIFT\n",
        "    val_scores = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks, labels, paths in tqdm(val_loader, desc=f'Validating {class_name}'):\n",
        "            images = images.to(DEVICE)\n",
        "            scores, _ = model.predict(images, return_heatmaps=False)\n",
        "\n",
        "            val_scores.extend(scores.tolist())\n",
        "            val_labels.extend(labels.numpy().tolist())\n",
        "\n",
        "    # Convert to arrays\n",
        "    val_scores = np.array(val_scores)\n",
        "    val_labels = np.array(val_labels)\n",
        "\n",
        "    # Calibrate threshold on Val-shift\n",
        "    threshold = patchcore_calibrator.calibrate(class_name, val_scores, val_labels)\n",
        "    print(f\"  Calibrated threshold: {threshold:.4f}\")\n",
        "\n",
        "# Save calibrated thresholds\n",
        "patchcore_calibrator.save(THRESHOLDS_DIR / 'shift_threshold_only_patchcore.json')\n",
        "print(f\"\\n[OK] Thresholds saved: shift_threshold_only_patchcore.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PADIM: Calibrate thresholds on Val-shift\n",
        "print(\"\\n>>> PADIM <<<\\n\")\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    print(f\"\\n--- {class_name.upper()} ---\")\n",
        "\n",
        "    # Create validation dataset\n",
        "    val_split = splits[class_name]['val']\n",
        "    val_dataset = MVTecDataset.from_split(\n",
        "        val_split,\n",
        "        transform=transform,\n",
        "        phase='val'\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=evaluation_collate\n",
        "    )\n",
        "\n",
        "    # Load model trained on CLEAN data\n",
        "    model = PadimWrapper(\n",
        "        backbone=config.padim.backbone,\n",
        "        layers=config.padim.layers,\n",
        "        n_features=config.padim.n_features,\n",
        "        image_size=config.dataset.image_size,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    model.load(PADIM_MODELS_DIR / f'padim_{class_name}_clean.pt')\n",
        "\n",
        "    # Collect predictions on VAL-SHIFT\n",
        "    val_scores = []\n",
        "    val_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks, labels, paths in tqdm(val_loader, desc=f'Validating {class_name}'):\n",
        "            images = images.to(DEVICE)\n",
        "            scores, _ = model.predict(images, return_heatmaps=False)\n",
        "\n",
        "            val_scores.extend(scores.tolist())\n",
        "            val_labels.extend(labels.numpy().tolist())\n",
        "\n",
        "    # Convert to arrays\n",
        "    val_scores = np.array(val_scores)\n",
        "    val_labels = np.array(val_labels)\n",
        "\n",
        "    # Calibrate threshold on Val-shift\n",
        "    threshold = padim_calibrator.calibrate(class_name, val_scores, val_labels)\n",
        "    print(f\"  Calibrated threshold: {threshold:.4f}\")\n",
        "\n",
        "# Save calibrated thresholds\n",
        "padim_calibrator.save(THRESHOLDS_DIR / 'shift_threshold_only_padim.json')\n",
        "print(f\"\\n[OK] Thresholds saved: shift_threshold_only_padim.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\\\n",
        "# VISUALIZATION: Score Distributions and Recalibrated Thresholds (Val-Shift)\\\n",
        "# ============================================================\\\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create figure\n",
        "fig, axes = plt.subplots(len(CLASSES), 2, figsize=(14, 4*len(CLASSES)))\n",
        "\n",
        "# If only 1 class, axes is 1D array, make it 2D for consistency\n",
        "if len(CLASSES) == 1:\n",
        "    axes = axes.reshape(1, -1)\n",
        "\n",
        "print(\"\\nGenerating Score Distribution Plots...\")\n",
        "\n",
        "for i, class_name in enumerate(CLASSES):\n",
        "    # --- PatchCore ---\n",
        "    ax1 = axes[i, 0]\n",
        "    \n",
        "    # We need to access the calibration data stored inside the calibrator\n",
        "    # Note: access internal storage assuming it was populated during calibration loop\n",
        "    # For this visualization to work, we need to have stored the scores/labels during calibration loop\n",
        "    # Let's re-extract them seamlessly if they aren't readily available\n",
        "    \n",
        "    # Re-run prediction on VAL set for visualization if data not persistent (safer approach here)\n",
        "    # OR better: Assume we are inside the loop or have data.\n",
        "    # To make this robust as a standalone cell, let's presume we ran the calibration loop above.\n",
        "    # We will use temporary lists if available, or just plot what we just computed.\n",
        "    \n",
        "    # Since the previous cells overwrote 'val_scores' and 'val_labels' in the loop, \n",
        "    # we can only plot the LAST class if we don't store them. \n",
        "    # TO FIX THIS: I provided a robust version below that doesn't depend on loop variables.\n",
        "    pass \n",
        "\n",
        "plt.close() # Close the placeholder\n",
        "\n",
        "# ROBUST PLOTTING CELL (Copy this one)\n",
        "# This cell re-runs inference on VAL subset (very fast) to ensure data is available for plotting\n",
        "# without relying on transient loop variables from previous cells.\n",
        "\n",
        "fig, axes = plt.subplots(len(CLASSES), 2, figsize=(15, 5*len(CLASSES)))\n",
        "if len(CLASSES) == 1: axes = axes.reshape(1, -1)\n",
        "\n",
        "print(\"Generating Score Distribution & Threshold Plots (Val-Shift)...\")\n",
        "\n",
        "for i, class_name in enumerate(CLASSES):\n",
        "    # 1. PatchCore Data\n",
        "    # Load model clean\n",
        "    model_pc = PatchCore(\n",
        "        backbone_layers=config.patchcore.layers,\n",
        "        patch_size=config.patchcore.patch_size,\n",
        "        coreset_ratio=config.patchcore.coreset_sampling_ratio,\n",
        "        n_neighbors=config.patchcore.n_neighbors,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    model_pc.load(PATCHCORE_MODELS_DIR, class_name, domain='clean')\n",
        "    \n",
        "    # Get Val Data\n",
        "    val_dataset = MVTecDataset.from_split(splits[class_name]['val'], transform=transform, phase='val')\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=evaluation_collate)\n",
        "    \n",
        "    pc_scores = []\n",
        "    pc_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, _, lbls, _ in val_loader:\n",
        "            s, _ = model_pc.predict(imgs.to(DEVICE), return_heatmaps=False)\n",
        "            pc_scores.extend(s.cpu().numpy())\n",
        "            pc_labels.extend(lbls.numpy())\n",
        "    \n",
        "    pc_scores = np.array(pc_scores)\n",
        "    pc_labels = np.array(pc_labels)\n",
        "    pc_thresh = thresholds['patchcore'][class_name]\n",
        "\n",
        "    # Plot PatchCore\n",
        "    ax1 = axes[i, 0]\n",
        "    sns.histplot(pc_scores[pc_labels==0], color='blue', label='Normal', kde=True, ax=ax1, alpha=0.5)\n",
        "    sns.histplot(pc_scores[pc_labels==1], color='red', label='Anomalous', kde=True, ax=ax1, alpha=0.5)\n",
        "    ax1.axvline(pc_thresh, color='green', linestyle='--', linewidth=2, label=f'New Threshold: {pc_thresh:.2f}')\n",
        "    ax1.set_title(f'PatchCore (Val-Shift): {class_name}', fontsize=12, fontweight='bold')\n",
        "    ax1.set_xlabel('Anomaly Score')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 2. PaDiM Data\n",
        "    model_pd = PadimWrapper(\n",
        "        backbone=config.padim.backbone,\n",
        "        layers=config.padim.layers,\n",
        "        n_features=config.padim.n_features,\n",
        "        image_size=config.dataset.image_size,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    model_pd.load(PADIM_MODELS_DIR / f'padim_{class_name}_clean.pt')\n",
        "    \n",
        "    pd_scores = []\n",
        "    pd_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, _, lbls, _ in val_loader:\n",
        "            s, _ = model_pd.predict(imgs.to(DEVICE), return_heatmaps=False)\n",
        "            pd_scores.extend(s.cpu().numpy())\n",
        "            pd_labels.extend(lbls.numpy())\n",
        "            \n",
        "    pd_scores = np.array(pd_scores)\n",
        "    pd_labels = np.array(pd_labels)\n",
        "    pd_thresh = thresholds['padim'][class_name]\n",
        "\n",
        "    # Plot PaDiM\n",
        "    ax2 = axes[i, 1]\n",
        "    sns.histplot(pd_scores[pd_labels==0], color='blue', label='Normal', kde=True, ax=ax2, alpha=0.5)\n",
        "    sns.histplot(pd_scores[pd_labels==1], color='red', label='Anomalous', kde=True, ax=ax2, alpha=0.5)\n",
        "    ax2.axvline(pd_thresh, color='green', linestyle='--', linewidth=2, label=f'New Threshold: {pd_thresh:.2f}')\n",
        "    ax2.set_title(f'PaDiM (Val-Shift): {class_name}', fontsize=12, fontweight='bold')\n",
        "    ax2.set_xlabel('Anomaly Score')\n",
        "    ax2.legend()\n",
        "\n",
        "plt.suptitle(f'Score Distributions & Recalibrated Thresholds on Val-Shift\\n(Clean Models)', fontsize=16, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(VIZ_DIR / 'shifted_score_distributions_recalibrated.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"[OK] Plot saved to {VIZ_DIR / 'shifted_score_distributions_recalibrated.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluation on Test-Shift with Recalibrated Thresholds\n",
        "\n",
        "Now we evaluate on the test set using the **recalibrated thresholds from Val-shift**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load recalibrated thresholds\n",
        "thresholds = {\n",
        "    'patchcore': patchcore_calibrator.thresholds,\n",
        "    'padim': padim_calibrator.thresholds\n",
        "}\n",
        "\n",
        "# Storage for results\n",
        "all_results = {\n",
        "    'patchcore': {},\n",
        "    'padim': {}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PATCHCORE: Evaluate on Test-shift\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST-SHIFT EVALUATION (PATCHCORE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    print(f\"\\n--- {class_name.upper()} ---\")\n",
        "\n",
        "    # Create test dataset\n",
        "    test_split = splits[class_name]['test']\n",
        "    test_dataset = MVTecDataset.from_split(\n",
        "        test_split,\n",
        "        transform=transform,\n",
        "        phase='test'\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=evaluation_collate\n",
        "    )\n",
        "\n",
        "    # Load model trained on CLEAN data\n",
        "    model = PatchCore(\n",
        "        backbone_layers=config.patchcore.layers,\n",
        "        patch_size=config.patchcore.patch_size,\n",
        "        coreset_ratio=config.patchcore.coreset_sampling_ratio,\n",
        "        n_neighbors=config.patchcore.n_neighbors,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    model.load(PATCHCORE_MODELS_DIR, class_name, domain='clean')\n",
        "\n",
        "    # Collect predictions\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "    all_heatmaps = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks, labels, paths in tqdm(test_loader, desc=f'Testing {class_name}'):\n",
        "            images = images.to(DEVICE)\n",
        "            scores, heatmaps = model.predict(images, return_heatmaps=True)\n",
        "\n",
        "            all_scores.extend(scores.tolist())\n",
        "            all_labels.extend(labels.numpy().tolist())\n",
        "\n",
        "            for mask in masks:\n",
        "                if mask is not None:\n",
        "                    all_masks.append(mask.numpy().squeeze())\n",
        "                else:\n",
        "                    all_masks.append(None)\n",
        "            \n",
        "            # Convert heatmaps to cpu list\n",
        "            all_heatmaps.extend([h.cpu().numpy() for h in heatmaps])\n",
        "\n",
        "    # Convert to arrays\n",
        "    test_scores = np.array(all_scores)\n",
        "    test_labels = np.array(all_labels)\n",
        "\n",
        "    # Get recalibrated threshold\n",
        "    threshold = thresholds['patchcore'][class_name]\n",
        "\n",
        "    # Compute image-level metrics\n",
        "    image_metrics = compute_image_metrics(test_labels, test_scores, threshold=threshold)\n",
        "\n",
        "    # Compute pixel-level metrics\n",
        "    pixel_metrics = compute_pixel_metrics(all_masks, all_heatmaps, compute_pro_metric=True)\n",
        "\n",
        "    # Store results\n",
        "    all_results['patchcore'][class_name] = {\n",
        "        'threshold': threshold,\n",
        "        'image_level': image_metrics,\n",
        "        'pixel_level': pixel_metrics,\n",
        "        'test_scores': test_scores.tolist(),\n",
        "        'test_labels': test_labels.tolist()\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"  Threshold (recalibrated on val-shift): {threshold:.4f}\")\n",
        "    print(f\"  AUROC: {image_metrics['auroc']:.4f}\")\n",
        "    print(f\"  AUPRC: {image_metrics['auprc']:.4f}\")\n",
        "    print(f\"  F1: {image_metrics['f1']:.4f}\")\n",
        "    print(f\"  Accuracy: {image_metrics.get('accuracy', 'N/A'):.4f}\")\n",
        "    print(f\"  Pixel AUROC: {pixel_metrics.get('pixel_auroc', 'N/A')}\")\n",
        "    print(f\"  PRO: {pixel_metrics.get('pro', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PADIM: Evaluate on Test-shift\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEST-SHIFT EVALUATION (PADIM)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for class_name in CLASSES:\n",
        "    print(f\"\\n--- {class_name.upper()} ---\")\n",
        "\n",
        "    # Create test dataset\n",
        "    test_split = splits[class_name]['test']\n",
        "    test_dataset = MVTecDataset.from_split(\n",
        "        test_split,\n",
        "        transform=transform,\n",
        "        phase='test'\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=8,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        collate_fn=evaluation_collate\n",
        "    )\n",
        "\n",
        "    # Load model trained on CLEAN data\n",
        "    model = PadimWrapper(\n",
        "        backbone=config.padim.backbone,\n",
        "        layers=config.padim.layers,\n",
        "        n_features=config.padim.n_features,\n",
        "        image_size=config.dataset.image_size,\n",
        "        device=DEVICE\n",
        "    )\n",
        "    model.load(PADIM_MODELS_DIR / f'padim_{class_name}_clean.pt')\n",
        "\n",
        "    # Collect predictions\n",
        "    all_scores = []\n",
        "    all_labels = []\n",
        "    all_heatmaps = []\n",
        "    all_masks = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks, labels, paths in tqdm(test_loader, desc=f'Testing {class_name}'):\n",
        "            images = images.to(DEVICE)\n",
        "            scores, heatmaps = model.predict(images, return_heatmaps=True)\n",
        "\n",
        "            all_scores.extend(scores.tolist())\n",
        "            all_labels.extend(labels.numpy().tolist())\n",
        "\n",
        "            for mask in masks:\n",
        "                if mask is not None:\n",
        "                    all_masks.append(mask.numpy().squeeze())\n",
        "                else:\n",
        "                    all_masks.append(None)\n",
        "            \n",
        "            # Convert heatmaps to cpu list\n",
        "            all_heatmaps.extend([h.cpu().numpy() for h in heatmaps])\n",
        "\n",
        "    # Convert to arrays\n",
        "    test_scores = np.array(all_scores)\n",
        "    test_labels = np.array(all_labels)\n",
        "\n",
        "    # Get recalibrated threshold\n",
        "    threshold = thresholds['padim'][class_name]\n",
        "\n",
        "    # Compute image-level metrics\n",
        "    image_metrics = compute_image_metrics(test_labels, test_scores, threshold=threshold)\n",
        "\n",
        "    # Compute pixel-level metrics\n",
        "    pixel_metrics = compute_pixel_metrics(all_masks, all_heatmaps, compute_pro_metric=True)\n",
        "\n",
        "    # Store results\n",
        "    all_results['padim'][class_name] = {\n",
        "        'threshold': threshold,\n",
        "        'image_level': image_metrics,\n",
        "        'pixel_level': pixel_metrics,\n",
        "        'test_scores': test_scores.tolist(),\n",
        "        'test_labels': test_labels.tolist()\n",
        "    }\n",
        "\n",
        "    # Print results\n",
        "    print(f\"  Threshold (recalibrated on val-shift): {threshold:.4f}\")\n",
        "    print(f\"  AUROC: {image_metrics['auroc']:.4f}\")\n",
        "    print(f\"  AUPRC: {image_metrics['auprc']:.4f}\")\n",
        "    print(f\"  F1: {image_metrics['f1']:.4f}\")\n",
        "    print(f\"  Accuracy: {image_metrics.get('accuracy', 'N/A'):.4f}\")\n",
        "    print(f\"  Pixel AUROC: {pixel_metrics.get('pixel_auroc', 'N/A')}\")\n",
        "    print(f\"  PRO: {pixel_metrics.get('pro', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute Aggregated Metrics (Macro-Average)\n",
        "aggregated_results = {\n",
        "    'patchcore': aggregate_metrics(all_results['patchcore']),\n",
        "    'padim': aggregate_metrics(all_results['padim'])\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AGGREGATED RESULTS (MACRO-AVERAGE)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nPATCHCORE:\")\n",
        "for metric, value in aggregated_results['patchcore'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nPADIM:\")\n",
        "for metric, value in aggregated_results['padim'].items():\n",
        "    print(f\"  {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save full results to JSON\n",
        "results_path = RESULTS_DIR / 'shifted_threshold_only_results.json'\n",
        "\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(all_results, f, indent=2, default=lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
        "\n",
        "print(f\"\\n[OK] Results saved to {results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary CSV\n",
        "import pandas as pd\n",
        "\n",
        "summary_rows = []\n",
        "\n",
        "for method, results in all_results.items():\n",
        "    for class_name, metrics in results.items():\n",
        "        row = {\n",
        "            'Method': method.upper(),\n",
        "            'Class': class_name.capitalize(),\n",
        "            'Threshold': f\"{metrics['threshold']:.2f}\",\n",
        "            'AUROC': f\"{metrics['image_level']['auroc']:.4f}\",\n",
        "            'AUPRC': f\"{metrics['image_level']['auprc']:.4f}\",\n",
        "            'F1': f\"{metrics['image_level']['f1']:.4f}\",\n",
        "            'Accuracy': f\"{metrics['image_level']['accuracy']:.4f}\",\n",
        "            'Pixel AUROC': f\"{metrics['pixel_level']['pixel_auroc']:.4f}\",\n",
        "            'PRO': f\"{metrics['pixel_level']['pro']:.4f}\"\n",
        "        }\n",
        "        summary_rows.append(row)\n",
        "\n",
        "    # Add macro Average\n",
        "    macro_avg = aggregated_results[method]\n",
        "    row = {\n",
        "        'Method': method.upper(),\n",
        "        'Class': 'Macro_average',\n",
        "        'Threshold': '-',\n",
        "        'AUROC': f\"{macro_avg['auroc']:.4f}\",\n",
        "        'AUPRC': f\"{macro_avg['auprc']:.4f}\",\n",
        "        'F1': f\"{macro_avg['f1']:.4f}\",\n",
        "        'Accuracy': f\"{macro_avg['accuracy']:.4f}\",\n",
        "        'Pixel AUROC': f\"{macro_avg['pixel_auroc']:.4f}\",\n",
        "        'PRO': f\"{macro_avg['pro']:.4f}\"\n",
        "    }\n",
        "    summary_rows.append(row)\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows)\n",
        "csv_path = RESULTS_DIR / 'shifted_threshold_only_summary.csv'\n",
        "df_summary.to_csv(csv_path, index=False)\n",
        "\n",
        "print(\"\\nSUMMARY TABLE:\")\n",
        "print(df_summary)\n",
        "print(f\"\\n[OK] Summary CSV saved to {csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Comparison: Threshold Adaptation vs No Adaptation\n",
        "# Note: This requires loading results from the previous no-adaptation run if available\n",
        "\n",
        "no_adapt_path = RESULTS_DIR / 'shifted_no_adaptation_results.json'\n",
        "if no_adapt_path.exists():\n",
        "    with open(no_adapt_path, 'r') as f:\n",
        "        no_adapt_results = json.load(f)\n",
        "    \n",
        "    # Function to plot comparison\n",
        "    def plot_adaptation_comparison(no_adapt, adapt, metric='auroc', title='AUROC Comparison'):\n",
        "        labels = [c.capitalize() for c in CLASSES] + ['Average']\n",
        "        \n",
        "        # Extract no-adapt values\n",
        "        na_patchcore = [no_adapt['patchcore'][c]['image_level'][metric] for c in CLASSES]\n",
        "        na_padim = [no_adapt['padim'][c]['image_level'][metric] for c in CLASSES]\n",
        "        \n",
        "        # Calculate averages\n",
        "        na_patchcore.append(np.mean(na_patchcore))\n",
        "        na_padim.append(np.mean(na_padim))\n",
        "        \n",
        "        # Extract adapt values\n",
        "        a_patchcore = [adapt['patchcore'][c]['image_level'][metric] for c in CLASSES]\n",
        "        a_padim = [adapt['padim'][c]['image_level'][metric] for c in CLASSES]\n",
        "        \n",
        "        # Calculate averages\n",
        "        a_patchcore.append(np.mean(a_patchcore))\n",
        "        a_padim.append(np.mean(a_padim))\n",
        "        \n",
        "        x = np.arange(len(labels))\n",
        "        width = 0.2\n",
        "        \n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        \n",
        "        ax.bar(x - 1.5*width, na_patchcore, width, label='PatchCore (No Adapt)', color='#1f77b4', alpha=0.7)\n",
        "        ax.bar(x - 0.5*width, a_patchcore, width, label='PatchCore (Thresh Adapt)', color='#1f77b4')\n",
        "        \n",
        "        ax.bar(x + 0.5*width, na_padim, width, label='PaDiM (No Adapt)', color='#ff7f0e', alpha=0.7)\n",
        "        ax.bar(x + 1.5*width, a_padim, width, label='PaDiM (Thresh Adapt)', color='#ff7f0e')\n",
        "        \n",
        "        ax.set_ylabel(metric.upper())\n",
        "        ax.set_title(title)\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(labels)\n",
        "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        ax.grid(True, linestyle='--', alpha=0.6)\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(VIZ_DIR / f'comparison_{metric}.png')\n",
        "        plt.show()\n",
        "\n",
        "    plot_adaptation_comparison(no_adapt_results, all_results, metric='auroc', title='Image AUROC: Threshold Adaptation vs No Adaptation')\n",
        "    plot_adaptation_comparison(no_adapt_results, all_results, metric='f1', title='F1 Score: Threshold Adaptation vs No Adaptation')\n",
        "    \n",
        "    print(f\"\\n[OK] Comparison plots saved to {VIZ_DIR}\")\n",
        "else:\n",
        "    print(\"\\n[INFO] No adaptation results not found, skipping comparison plots.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Files Generated\n",
        "\n",
        "This notebook generates the following files, representing the results of the **Threshold-Only Adaptation Ablation Study**:\n",
        "\n",
        "**Thresholds** (`outputs/thresholds/`):\n",
        "- `shift_threshold_only_patchcore.json` - Thresholds recalibrated on *Val-shift* for PatchCore\n",
        "- `shift_threshold_only_padim.json` - Thresholds recalibrated on *Val-shift* for PaDiM\n",
        "\n",
        "**Results** (`outputs/results/`):\n",
        "- `shifted_threshold_only_results.json` - Detailed metrics (AUROC, F1, Pixel-AUROC, etc.) for each class\n",
        "- `shifted_threshold_only_summary.csv` - Summary table of performance metrics (easy to read/plot)\n",
        "\n",
        "**Visualizations** (`outputs/visualizations/shift_threshold_only/`):\n",
        "- `comparison_auroc.png` - Bar chart comparing *No-Adaptation* vs *Threshold-Adaptation* AUROC\n",
        "- `comparison_f1.png` - Bar chart comparing *No-Adaptation* vs *Threshold-Adaptation* F1 Scores\n",
        "\n",
        "These files quantify the performance gain achievable purely by recalibrating the decision boundary on the target domain, serving as a baseline for the full model adaptation in Phase 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\\\n",
        "# BACKUP RESULTS TO DRIVE\n",
        "# ============================================================\\\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define source and destination paths\n",
        "SOURCE_RESULTS = RESULTS_DIR\n",
        "SOURCE_THRESHOLDS = THRESHOLDS_DIR\n",
        "SOURCE_VIZ = VIZ_DIR\n",
        "\n",
        "# Define Drive destination folder for this specific phase\n",
        "DRIVE_OUTPUT_DIR = DRIVE_ROOT / '08_evaluation_shifted_threshold_only_outputs'\n",
        "\n",
        "# Create Drive directory if it doesn't exist\n",
        "if not os.path.exists(DRIVE_OUTPUT_DIR):\n",
        "    os.makedirs(DRIVE_OUTPUT_DIR)\n",
        "    print(f\"Created Drive directory: {DRIVE_OUTPUT_DIR}\")\n",
        "\n",
        "print(f\"\\nBacking up results to: {DRIVE_OUTPUT_DIR}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# 1. Copy Results JSONs and CSVs\n",
        "print(\"Copying results files...\")\n",
        "for file_path in SOURCE_RESULTS.glob('*.*'):\n",
        "    if file_path.is_file():\n",
        "        dest_path = DRIVE_OUTPUT_DIR / file_path.name\n",
        "        shutil.copy2(file_path, dest_path)\n",
        "        print(f\"  ✓ Copied: {file_path.name}\")\n",
        "\n",
        "# 2. Copy Threshold files\n",
        "print(\"\\nCopying threshold files...\")\n",
        "for file_path in SOURCE_THRESHOLDS.glob('*shift_threshold_only*.json'):\n",
        "    if file_path.is_file():\n",
        "        dest_path = DRIVE_OUTPUT_DIR / file_path.name\n",
        "        shutil.copy2(file_path, dest_path)\n",
        "        print(f\"  ✓ Copied: {file_path.name}\")\n",
        "\n",
        "# 3. Copy Visualizations (zip folder to keep structure clean)\n",
        "print(\"\\nArchiving and copying visualizations...\")\n",
        "viz_archive_path = shutil.make_archive(PROJECT_ROOT / 'shifted_threshold_viz', 'zip', SOURCE_VIZ)\n",
        "shutil.copy2(viz_archive_path, DRIVE_OUTPUT_DIR / 'visualizations.zip')\n",
        "print(f\"  ✓ Copied: visualizations.zip\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"Backup Complete!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
