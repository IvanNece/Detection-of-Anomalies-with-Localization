{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65f3c918",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6aa9d0",
   "metadata": {},
   "source": [
    "# 01 - Exploratory Data Analysis\n",
    "\n",
    "**Project**: Detection of Anomalies with Localization  \n",
    "**Dataset**: MVTec AD (Hazelnut, Carpet, Zipper)  \n",
    "**Phase**: Data Exploration\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Analyze dataset structure and organization\n",
    "2. Count images per class and split (train/test, normal/anomalous)\n",
    "3. Visualize representative samples (normal and anomalous)\n",
    "4. Compute image dimension statistics\n",
    "5. Analyze defect type distributions\n",
    "6. Visualize ground truth masks\n",
    "\n",
    "This analysis ensures data quality and informs preprocessing decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Important: Setup for Google Colab\n",
    "\n",
    "**Running directly on Colab (recommended workflow):**\n",
    "\n",
    "1. **On GitHub**: Push your code (without dataset)\n",
    "2. **On Google Drive**: Upload ONLY the dataset at `MyDrive/mvtec_dataset/`\n",
    "3. **On Colab**: Run the setup cell below - it will:\n",
    "   - Mount Google Drive (for dataset access)\n",
    "   - Clone the repository from GitHub (for code)\n",
    "   - Link everything together\n",
    "\n",
    "**What to upload to Google Drive:**\n",
    "```\n",
    "MyDrive/\n",
    "└── mvtec_dataset/\n",
    "    ├── hazelnut/\n",
    "    │   ├── train/good/\n",
    "    │   ├── test/good/\n",
    "    │   ├── test/<defects>/\n",
    "    │   └── ground_truth/<defects>/\n",
    "    ├── carpet/\n",
    "    └── zipper/\n",
    "```\n",
    "\n",
    "**No need to upload code to Drive - it comes from GitHub!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09279bb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mount failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2665861436.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Set project root (adjust this path to your Google Drive structure)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: mount failed"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Determine environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING ON GOOGLE COLAB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Mount Google Drive (for dataset)\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"\\n[1/3] Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"✓ Drive mounted successfully!\")\n",
    "    else:\n",
    "        print(\"\\n[1/3] Drive already mounted\")\n",
    "    \n",
    "    # 2. Clone repository from GitHub (for code)\n",
    "    repo_url = \"https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git\"\n",
    "    project_root = Path('/content/Detection-of-Anomalies-with-Localization')\n",
    "    \n",
    "    if not project_root.exists():\n",
    "        print(f\"\\n[2/3] Cloning repository from GitHub...\")\n",
    "        print(f\"Repository: {repo_url}\")\n",
    "        !git clone {repo_url} /content/Detection-of-Anomalies-with-Localization\n",
    "        print(\"✓ Repository cloned successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n[2/3] Repository already exists, pulling latest changes...\")\n",
    "        !cd /content/Detection-of-Anomalies-with-Localization && git pull\n",
    "        print(\"✓ Repository updated!\")\n",
    "    \n",
    "    # 3. Link dataset from Google Drive\n",
    "    print(\"\\n[3/3] Setting up dataset path...\")\n",
    "    # Dataset is on Google Drive\n",
    "    dataset_on_drive = Path('/content/drive/MyDrive/mvtec_dataset')\n",
    "    \n",
    "    # Create data/raw symlink to Drive dataset\n",
    "    data_raw = project_root / 'data' / 'raw'\n",
    "    if not data_raw.exists():\n",
    "        data_raw.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Create symbolic link\n",
    "        os.symlink(str(dataset_on_drive), str(data_raw))\n",
    "        print(f\"✓ Linked dataset from Drive: {dataset_on_drive}\")\n",
    "    else:\n",
    "        print(f\"✓ Dataset already linked\")\n",
    "    \n",
    "    # Verify dataset\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"VERIFYING DATASET STRUCTURE\")\n",
    "    print(\"-\"*60)\n",
    "    classes = ['hazelnut', 'carpet', 'zipper']\n",
    "    all_found = True\n",
    "    for cls in classes:\n",
    "        cls_path = data_raw / cls\n",
    "        if cls_path.exists():\n",
    "            print(f\"  ✓ {cls} found\")\n",
    "        else:\n",
    "            print(f\"  ✗ {cls} NOT FOUND!\")\n",
    "            all_found = False\n",
    "    \n",
    "    if not all_found:\n",
    "        print(\"\\n\" + \"!\"*60)\n",
    "        print(\"ERROR: Dataset not complete on Google Drive!\")\n",
    "        print(\"!\"*60)\n",
    "        print(\"\\nPlease upload the dataset to Google Drive at:\")\n",
    "        print(f\"  {dataset_on_drive}\")\n",
    "        print(\"\\nWith this structure:\")\n",
    "        print(\"  mvtec_dataset/\")\n",
    "        print(\"  ├── hazelnut/\")\n",
    "        print(\"  ├── carpet/\")\n",
    "        print(\"  └── zipper/\")\n",
    "        raise FileNotFoundError(\"Dataset incomplete\")\n",
    "    \n",
    "    print(\"\\n✓ All classes found!\")\n",
    "    \n",
    "else:\n",
    "    print(\"=\"*60)\n",
    "    print(\"RUNNING ON LOCAL ENVIRONMENT\")\n",
    "    print(\"=\"*60)\n",
    "    # Local environment\n",
    "    project_root = Path.cwd().parent\n",
    "    print(f\"\\nProject root: {project_root}\")\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Third-party imports\n",
    "print(\"\\nImporting libraries...\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Project imports\n",
    "print(\"Importing project modules...\")\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import Config\n",
    "from src.utils.paths import ProjectPaths\n",
    "\n",
    "# Helper function for config loading\n",
    "def load_config(config_path):\n",
    "    \"\"\"Load configuration from YAML file.\"\"\"\n",
    "    return Config.load(config_path)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# High-res figures\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Ready to proceed with EDA\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d40f8d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config(project_root / 'configs' / 'experiment_config.yaml')\n",
    "\n",
    "# Set reproducibility\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Classes to analyze\n",
    "CLASSES = config.dataset.classes\n",
    "print(f\"Analyzing classes: {CLASSES}\")\n",
    "\n",
    "# Dataset paths\n",
    "DATA_ROOT = project_root / config.paths.raw_data\n",
    "print(f\"Dataset location: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b35da4",
   "metadata": {},
   "source": [
    "## 1. Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade961e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_class_structure(class_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Explore directory structure for a given class.\n",
    "    \n",
    "    Args:\n",
    "        class_name: Name of the class (e.g., 'hazelnut')\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with structure information\n",
    "    \"\"\"\n",
    "    class_path = DATA_ROOT / class_name\n",
    "    \n",
    "    structure = {\n",
    "        'class': class_name,\n",
    "        'train_good': [],\n",
    "        'test_good': [],\n",
    "        'test_defects': {},\n",
    "        'ground_truth': {}\n",
    "    }\n",
    "    \n",
    "    # Train images (all normal)\n",
    "    train_path = class_path / 'train' / 'good'\n",
    "    if train_path.exists():\n",
    "        structure['train_good'] = sorted(list(train_path.glob('*.png')))\n",
    "    \n",
    "    # Test normal images\n",
    "    test_good_path = class_path / 'test' / 'good'\n",
    "    if test_good_path.exists():\n",
    "        structure['test_good'] = sorted(list(test_good_path.glob('*.png')))\n",
    "    \n",
    "    # Test anomalous images (by defect type)\n",
    "    test_path = class_path / 'test'\n",
    "    if test_path.exists():\n",
    "        for defect_dir in test_path.iterdir():\n",
    "            if defect_dir.is_dir() and defect_dir.name != 'good':\n",
    "                defect_type = defect_dir.name\n",
    "                structure['test_defects'][defect_type] = sorted(list(defect_dir.glob('*.png')))\n",
    "    \n",
    "    # Ground truth masks\n",
    "    gt_path = class_path / 'ground_truth'\n",
    "    if gt_path.exists():\n",
    "        for defect_dir in gt_path.iterdir():\n",
    "            if defect_dir.is_dir():\n",
    "                defect_type = defect_dir.name\n",
    "                structure['ground_truth'][defect_type] = sorted(list(defect_dir.glob('*.png')))\n",
    "    \n",
    "    return structure\n",
    "\n",
    "# Explore all classes\n",
    "dataset_structure = {}\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\nAnalyzing {class_name}...\")\n",
    "    structure = explore_class_structure(class_name)\n",
    "    dataset_structure[class_name] = structure\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Train (normal): {len(structure['train_good'])} images\")\n",
    "    print(f\"  Test (normal):  {len(structure['test_good'])} images\")\n",
    "    print(f\"  Defect types:   {len(structure['test_defects'])}\")\n",
    "    for defect, imgs in structure['test_defects'].items():\n",
    "        print(f\"    - {defect}: {len(imgs)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082d056",
   "metadata": {},
   "source": [
    "## 2. Image Count Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_dataframe(dataset_structure: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive dataframe with image counts.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with counts per class and split\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        # Normal images\n",
    "        data.append({\n",
    "            'Class': class_name,\n",
    "            'Split': 'Train',\n",
    "            'Type': 'Normal',\n",
    "            'Defect': 'good',\n",
    "            'Count': len(structure['train_good'])\n",
    "        })\n",
    "        \n",
    "        data.append({\n",
    "            'Class': class_name,\n",
    "            'Split': 'Test',\n",
    "            'Type': 'Normal',\n",
    "            'Defect': 'good',\n",
    "            'Count': len(structure['test_good'])\n",
    "        })\n",
    "        \n",
    "        # Anomalous images by defect type\n",
    "        for defect, imgs in structure['test_defects'].items():\n",
    "            data.append({\n",
    "                'Class': class_name,\n",
    "                'Split': 'Test',\n",
    "                'Type': 'Anomalous',\n",
    "                'Defect': defect,\n",
    "                'Count': len(imgs)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create dataframe\n",
    "df_counts = create_count_dataframe(dataset_structure)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_counts.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY BY CLASS\")\n",
    "print(\"=\"*60)\n",
    "summary = df_counts.groupby(['Class', 'Type'])['Count'].sum().reset_index()\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Total counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "total_normal = df_counts[df_counts['Type'] == 'Normal']['Count'].sum()\n",
    "total_anomalous = df_counts[df_counts['Type'] == 'Anomalous']['Count'].sum()\n",
    "print(f\"Total Normal Images:    {total_normal}\")\n",
    "print(f\"Total Anomalous Images: {total_anomalous}\")\n",
    "print(f\"Total Images:           {total_normal + total_anomalous}\")\n",
    "print(f\"Anomaly Ratio:          {total_anomalous / (total_normal + total_anomalous):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278e3b3",
   "metadata": {},
   "source": [
    "### Visualization: Image Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Normal vs Anomalous by Class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart\n",
    "pivot_data = df_counts.groupby(['Class', 'Type'])['Count'].sum().unstack()\n",
    "pivot_data.plot(kind='bar', stacked=True, ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Image Distribution: Normal vs Anomalous', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[0].legend(title='Type', loc='upper right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "# Train vs Test split\n",
    "split_data = df_counts[df_counts['Type'] == 'Normal'].groupby(['Class', 'Split'])['Count'].sum().unstack()\n",
    "split_data.plot(kind='bar', ax=axes[1], color=['#3498db', '#9b59b6'])\n",
    "axes[1].set_title('Normal Images: Train vs Test Split', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[1].legend(title='Split', loc='upper right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Defect Types Distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, class_name in enumerate(CLASSES):\n",
    "    class_data = df_counts[(df_counts['Class'] == class_name) & (df_counts['Type'] == 'Anomalous')]\n",
    "    \n",
    "    axes[idx].barh(class_data['Defect'], class_data['Count'], color='#e67e22')\n",
    "    axes[idx].set_title(f'{class_name.capitalize()} - Defect Types', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Number of Images', fontsize=10)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(class_data['Count']):\n",
    "        axes[idx].text(v + 0.5, i, str(v), va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f410d",
   "metadata": {},
   "source": [
    "## 3. Image Dimension Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_dimensions(dataset_structure: Dict, sample_size: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze image dimensions across dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "        sample_size: Number of images to sample per class\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with dimension statistics\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        print(f\"Analyzing dimensions for {class_name}...\")\n",
    "        \n",
    "        # Sample images from train\n",
    "        train_sample = structure['train_good'][:sample_size]\n",
    "        \n",
    "        for img_path in tqdm(train_sample, desc=f\"  {class_name}\"):\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            \n",
    "            data.append({\n",
    "                'Class': class_name,\n",
    "                'Width': width,\n",
    "                'Height': height,\n",
    "                'Aspect_Ratio': width / height,\n",
    "                'Megapixels': (width * height) / 1e6\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Analyze dimensions\n",
    "df_dimensions = analyze_image_dimensions(dataset_structure, sample_size=50)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMAGE DIMENSION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(df_dimensions.groupby('Class').describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7b498",
   "metadata": {},
   "source": [
    "### Visualization: Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Resolution distribution\n",
    "for class_name in CLASSES:\n",
    "    class_data = df_dimensions[df_dimensions['Class'] == class_name]\n",
    "    axes[0].scatter(class_data['Width'], class_data['Height'], \n",
    "                   label=class_name, alpha=0.6, s=100)\n",
    "\n",
    "axes[0].set_title('Image Resolution Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Width (pixels)', fontsize=12)\n",
    "axes[0].set_ylabel('Height (pixels)', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Aspect ratio distribution\n",
    "df_dimensions.boxplot(column='Aspect_Ratio', by='Class', ax=axes[1])\n",
    "axes[1].set_title('Aspect Ratio Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Aspect Ratio', fontsize=12)\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESOLUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "resolution_summary = df_dimensions.groupby('Class').agg({\n",
    "    'Width': ['mean', 'std', 'min', 'max'],\n",
    "    'Height': ['mean', 'std', 'min', 'max'],\n",
    "    'Aspect_Ratio': ['mean', 'std']\n",
    "}).round(2)\n",
    "print(resolution_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfbc7b",
   "metadata": {},
   "source": [
    "## 4. Visual Inspection: Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset_structure: Dict, n_samples: int = 3):\n",
    "    \"\"\"\n",
    "    Visualize sample images from each class.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "        n_samples: Number of samples per category\n",
    "    \"\"\"\n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLASS: {class_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Normal images (train)\n",
    "        fig, axes = plt.subplots(1, n_samples, figsize=(15, 5))\n",
    "        fig.suptitle(f'{class_name.capitalize()} - Normal Images (Train)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for idx in range(n_samples):\n",
    "            img_path = structure['train_good'][idx * len(structure['train_good']) // n_samples]\n",
    "            img = Image.open(img_path)\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].axis('off')\n",
    "            axes[idx].set_title(f\"Sample {idx+1}\\n{img.size[0]}x{img.size[1]}\", fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Anomalous images (sample from different defect types)\n",
    "        defect_types = list(structure['test_defects'].keys())[:n_samples]\n",
    "        \n",
    "        if defect_types:\n",
    "            fig, axes = plt.subplots(1, len(defect_types), figsize=(15, 5))\n",
    "            if len(defect_types) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            fig.suptitle(f'{class_name.capitalize()} - Anomalous Images', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            \n",
    "            for idx, defect in enumerate(defect_types):\n",
    "                img_path = structure['test_defects'][defect][0]\n",
    "                img = Image.open(img_path)\n",
    "                axes[idx].imshow(img)\n",
    "                axes[idx].axis('off')\n",
    "                axes[idx].set_title(f\"Defect: {defect}\\n{img.size[0]}x{img.size[1]}\", fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_samples(dataset_structure, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bfe66",
   "metadata": {},
   "source": [
    "## 5. Ground Truth Mask Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(dataset_structure: Dict, n_samples: int = 2):\n",
    "    \"\"\"\n",
    "    Visualize anomalous images with their ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "        n_samples: Number of samples to show per class\n",
    "    \"\"\"\n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MASKS: {class_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        defect_types = list(structure['test_defects'].keys())[:n_samples]\n",
    "        \n",
    "        for defect in defect_types:\n",
    "            if defect in structure['ground_truth']:\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                fig.suptitle(f'{class_name.capitalize()} - Defect: {defect}', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "                \n",
    "                # Image\n",
    "                img_path = structure['test_defects'][defect][0]\n",
    "                img = Image.open(img_path)\n",
    "                axes[0].imshow(img)\n",
    "                axes[0].axis('off')\n",
    "                axes[0].set_title('Anomalous Image', fontsize=12)\n",
    "                \n",
    "                # Mask\n",
    "                mask_path = structure['ground_truth'][defect][0]\n",
    "                mask = Image.open(mask_path)\n",
    "                axes[1].imshow(mask, cmap='gray')\n",
    "                axes[1].axis('off')\n",
    "                axes[1].set_title('Ground Truth Mask', fontsize=12)\n",
    "                \n",
    "                # Overlay\n",
    "                img_array = np.array(img)\n",
    "                mask_array = np.array(mask)\n",
    "                overlay = img_array.copy()\n",
    "                overlay[mask_array > 0] = [255, 0, 0]  # Red overlay on defects\n",
    "                axes[2].imshow(overlay)\n",
    "                axes[2].axis('off')\n",
    "                axes[2].set_title('Overlay (Defects in Red)', fontsize=12)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Visualize masks\n",
    "visualize_masks(dataset_structure, n_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6973f",
   "metadata": {},
   "source": [
    "## 6. Key Findings & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13098f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Class balance\n",
    "print(\"\\n1. CLASS DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "for class_name in CLASSES:\n",
    "    class_data = df_counts[df_counts['Class'] == class_name]\n",
    "    n_normal = class_data[class_data['Type'] == 'Normal']['Count'].sum()\n",
    "    n_anomalous = class_data[class_data['Type'] == 'Anomalous']['Count'].sum()\n",
    "    ratio = n_anomalous / (n_normal + n_anomalous) * 100\n",
    "    print(f\"  {class_name.capitalize():10s}: {n_normal:3d} normal, {n_anomalous:3d} anomalous ({ratio:.1f}% anomaly rate)\")\n",
    "\n",
    "# 2. Image dimensions\n",
    "print(\"\\n2. IMAGE DIMENSIONS\")\n",
    "print(\"-\" * 40)\n",
    "for class_name in CLASSES:\n",
    "    class_dims = df_dimensions[df_dimensions['Class'] == class_name]\n",
    "    avg_w = class_dims['Width'].mean()\n",
    "    avg_h = class_dims['Height'].mean()\n",
    "    print(f\"  {class_name.capitalize():10s}: {avg_w:.0f} x {avg_h:.0f} pixels (avg)\")\n",
    "\n",
    "# 3. Defect diversity\n",
    "print(\"\\n3. DEFECT TYPE DIVERSITY\")\n",
    "print(\"-\" * 40)\n",
    "for class_name in CLASSES:\n",
    "    n_defects = len(dataset_structure[class_name]['test_defects'])\n",
    "    defect_names = list(dataset_structure[class_name]['test_defects'].keys())\n",
    "    print(f\"  {class_name.capitalize():10s}: {n_defects} defect types - {', '.join(defect_names[:5])}\")\n",
    "\n",
    "# 4. Recommendations\n",
    "print(\"\\n4. RECOMMENDATIONS FOR PREPROCESSING\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - Resize all images to 224x224 (ResNet standard)\")\n",
    "print(\"  - Maintain aspect ratio during resizing to avoid distortion\")\n",
    "print(\"  - Apply ImageNet normalization for pre-trained backbone\")\n",
    "print(\"  - Consider data augmentation for anomalous samples (limited quantity)\")\n",
    "print(\"  - Ensure ground truth masks are resized consistently with images\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - Ready for data preparation phase\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6eca4",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **Notebook 02: Data Preparation** to:\n",
    "1. Implement data splitting logic (Train/Val/Test)\n",
    "2. Create MVTecDataset class\n",
    "3. Implement preprocessing transforms\n",
    "4. Save split configurations for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
