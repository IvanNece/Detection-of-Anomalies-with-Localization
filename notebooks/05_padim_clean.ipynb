{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e7b4bf",
   "metadata": {},
   "source": [
    "# PaDiM Baseline Training (Clean Domain)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/05_padim_clean.ipynb)\n",
    "\n",
    "**Phase 4: Baseline Model Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "## PaDiM Method\n",
    "\n",
    "**PaDiM** = Probabilistic Anomaly Detection with Multi-scale features\n",
    "\n",
    "- Extracts multi-scale features from ResNet\n",
    "- Models normal appearance using Gaussian distributions\n",
    "- Uses Mahalanobis distance for anomaly scoring\n",
    "- **No gradient-based training** (statistical approach)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8105a2",
   "metadata": {},
   "source": [
    "## Setup - Mount Drive & Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository on main branch\n",
    "print(\"Cloning repository (branch: main)...\")\n",
    "repo_dir = '/content/Detection-of-Anomalies-with-Localization'\n",
    "\n",
    "# Remove if exists\n",
    "if os.path.exists(repo_dir):\n",
    "    print(\"Removing existing repository...\")\n",
    "    !rm -rf {repo_dir}\n",
    "\n",
    "# Clone from main branch\n",
    "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git {repo_dir}\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = Path(repo_dir)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"[OK] Path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b84769",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALL ANOMALIB - REQUIRED FOR PADIM\n",
    "# ============================================================\n",
    "# anomalib provides PaDiM's underlying implementation\n",
    "\n",
    "print(\"Installing anomalib...\")\n",
    "!pip install anomalib --quiet\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import anomalib\n",
    "    from anomalib.models.image.padim import Padim\n",
    "    from anomalib.models.image.padim.torch_model import PadimModel\n",
    "    print(f\"Success! anomalib {anomalib.__version__} installed\")\n",
    "    print(\"PaDiM components available\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Retry: !pip install anomalib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d927aa",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf742dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS - All required modules\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "# Project imports\n",
    "from src.models.padim_wrapper import PadimWrapper\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.data.transforms import get_clean_transforms\n",
    "from src.utils.config import load_config\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.paths import get_paths\n",
    "\n",
    "print(\"[OK] All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD DATASET SPLITS\n",
    "# ============================================================\n",
    "\n",
    "# Load clean splits (generated in notebook 02)\n",
    "splits_path = PROJECT_ROOT / 'data' / 'processed' / 'clean_splits.json'\n",
    "\n",
    "with open(splits_path, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "print(\"[OK] Loaded clean splits\")\n",
    "print(\"\\nSplit summary:\")\n",
    "for class_name in CLASSES:\n",
    "    split_data = splits[class_name]\n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Train:  {len(split_data['train']['images'])} images (normal only)\")\n",
    "    print(f\"  Val:    {len(split_data['val']['images'])} images\")\n",
    "    print(f\"    - Normal: {sum(1 for l in split_data['val']['labels'] if l == 0)}\")\n",
    "    print(f\"    - Anomalous: {sum(1 for l in split_data['val']['labels'] if l == 1)}\")\n",
    "    print(f\"  Test:   {len(split_data['test']['images'])} images\")\n",
    "    print(f\"    - Normal: {sum(1 for l in split_data['test']['labels'] if l == 0)}\")\n",
    "    print(f\"    - Anomalous: {sum(1 for l in split_data['test']['labels'] if l == 1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efbae4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ce83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Load experiment config\n",
    "# ============================================================\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
    "paths = get_paths(config)\n",
    "\n",
    "# Settings\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "CLASSES = config.dataset.classes  # ['hazelnut', 'carpet', 'zipper']\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(config.seed)\n",
    "\n",
    "print(f\"\\n[INFO] PaDiM Configuration:\")\n",
    "print(f\"  Backbone: {config.padim.backbone}\")\n",
    "print(f\"  Layers: {config.padim.layers}\")\n",
    "print(f\"  Feature dimension: {config.padim.n_features}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Classes: {CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07dfd3b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c11d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform (SAME AS PATCHCORE - same preprocessing for fair comparison)\n",
    "transform = get_clean_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    normalize_mean=config.dataset.normalize.mean,\n",
    "    normalize_std=config.dataset.normalize.std\n",
    ")\n",
    "\n",
    "print(f\"[OK] Transform initialized:\")\n",
    "print(f\"  Size: {config.dataset.image_size}x{config.dataset.image_size}\")\n",
    "print(f\"  Normalization: ImageNet statistics\")\n",
    "print(f\"  Augmentation: None (deterministic preprocessing)\")\n",
    "\n",
    "# Helper function to create DataLoader (ALIGNED WITH PATCHCORE)\n",
    "def create_dataloader(split_dict, batch_size=32, shuffle=False):\n",
    "    \"\"\"Create DataLoader from split dict (same format as PatchCore).\"\"\"\n",
    "    dataset = MVTecDataset(\n",
    "        images=split_dict['images'],\n",
    "        masks=split_dict['masks'],\n",
    "        labels=split_dict['labels'],\n",
    "        transform=transform,\n",
    "        phase='train' if shuffle else 'val'\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if DEVICE == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "\n",
    "print(\"[OK] DataLoader helper function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c922c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results tracking\n",
    "MODEL_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_results = {\n",
    "    'classes': [],\n",
    "    'num_train_samples': [],\n",
    "    'training_time_seconds': [],\n",
    "    'memory_bank_size_mb': [],\n",
    "    'model_path': []\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "print(f\" Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddd57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PaDiM for each class\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[START] TRAINING PaDiM MODELS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[TRAIN] PaDiM: {class_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 1. Load data (ALIGNED WITH PATCHCORE)\n",
    "    split_data = splits[class_name]\n",
    "    train_loader = create_dataloader(\n",
    "        split_dict=split_data['train'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False  # No shuffle for deterministic training\n",
    "    )\n",
    "    \n",
    "    print(f\"[OK] Loaded {len(train_loader.dataset)} training samples\\n\")\n",
    "    \n",
    "    # 2. Initialize model\n",
    "    model = PadimWrapper(\n",
    "        backbone=config.padim.backbone,\n",
    "        layers=config.padim.layers,\n",
    "        n_features=config.padim.n_features,\n",
    "        image_size=config.dataset.image_size,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # 3. Train (fit on normal samples)\n",
    "    model.fit(train_loader, verbose=True)\n",
    "    \n",
    "    # 4. Save\n",
    "    model_path = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "    model.save(model_path, include_stats=True)\n",
    "    \n",
    "    # 5. Store results\n",
    "    training_results['classes'].append(class_name)\n",
    "    training_results['num_train_samples'].append(model.training_stats['num_samples'])\n",
    "    training_results['training_time_seconds'].append(model.training_stats['training_time_seconds'])\n",
    "    training_results['memory_bank_size_mb'].append(model.training_stats['memory_bank_size_mb'])\n",
    "    training_results['model_path'].append(str(model_path))\n",
    "    \n",
    "    trained_models[class_name] = model\n",
    "    \n",
    "    print(f\"\\n[COMPLETE] {class_name}\")\n",
    "    print(f\"   Model: {model_path.name}\")\n",
    "    print(f\"   Time: {model.training_stats['training_time_seconds']:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"[SUCCESS] ALL CLASSES TRAINED SUCCESSFULLY!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca768b",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results to JSON\n",
    "results_path = PROJECT_ROOT / 'outputs' / 'results' / 'padim_training_results_clean.json'\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "# Save as CSV for easy viewing\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(training_results)\n",
    "csv_path = PROJECT_ROOT / 'outputs' / 'results' / 'padim_training_stats_clean.csv'\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\n[OK] Results saved: {results_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time\n",
    "axes[0].bar(results_df['classes'], results_df['training_time_seconds'], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[0].set_title('PaDiM Training Time per Class', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['training_time_seconds']):\n",
    "    axes[0].text(i, v + 0.5, f\"{v:.1f}s\", ha='center', fontweight='bold')\n",
    "\n",
    "# Memory bank size\n",
    "axes[1].bar(results_df['classes'], results_df['memory_bank_size_mb'], color='coral', alpha=0.8)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Memory Bank Size (MB)', fontsize=12)\n",
    "axes[1].set_title('PaDiM Memory Usage per Class', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['memory_bank_size_mb']):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}MB\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b8f12",
   "metadata": {},
   "source": [
    "## Testing and Validation\n",
    "\n",
    "Comprehensive tests to verify models work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: Verify all models load correctly\n",
    "print(\"=\"*80)\n",
    "print(\"[TEST 1] Model Loading\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "all_load_success = True\n",
    "for class_name in CLASSES:\n",
    "    model_path = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "    try:\n",
    "        test_model = PadimWrapper(device=DEVICE)\n",
    "        test_model.load(model_path)\n",
    "        print(f\"[OK] {class_name}: Loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"[FAIL] {class_name}: Failed - {e}\")\n",
    "        all_load_success = False\n",
    "\n",
    "if all_load_success:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"[PASS] TEST 1 PASSED: All models load correctly\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"[FAIL] TEST 1 FAILED: Some models failed to load\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: Verify predictions work and anomalous > normal\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"[TEST 2] Prediction Validation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"Testing {class_name}...\")\n",
    "    model = trained_models[class_name]\n",
    "    split_data = splits[class_name]\n",
    "    \n",
    "    # Create val loader\n",
    "    val_loader = create_dataloader(\n",
    "        split_dict=split_data['val'],\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get one normal and one anomalous sample\n",
    "    normal_score = None\n",
    "    anomalous_score = None\n",
    "    \n",
    "    for images, masks, labels, paths in val_loader:\n",
    "        label = labels.item()\n",
    "        \n",
    "        if label == 0 and normal_score is None:\n",
    "            # Single image prediction\n",
    "            score, _ = model.predict(images, return_heatmaps=False)\n",
    "            normal_score = float(score)\n",
    "        elif label == 1 and anomalous_score is None:\n",
    "            score, _ = model.predict(images, return_heatmaps=False)\n",
    "            anomalous_score = float(score)\n",
    "        \n",
    "        if normal_score is not None and anomalous_score is not None:\n",
    "            break\n",
    "    \n",
    "    # Check result\n",
    "    passed = anomalous_score > normal_score if (normal_score and anomalous_score) else False\n",
    "    status = \"[OK]\" if passed else \"[FAIL]\"\n",
    "    \n",
    "    print(f\"  {status} Normal score: {normal_score:.4f}\")\n",
    "    print(f\"  {status} Anomalous score: {anomalous_score:.4f}\")\n",
    "    print(f\"  {status} Test: {'PASSED' if passed else 'FAILED'}\\n\")\n",
    "    \n",
    "    test_results.append(passed)\n",
    "\n",
    "if all(test_results):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"[PASS] TEST 2 PASSED: Anomalous scores > Normal scores for all classes\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"[WARNING] TEST 2 WARNING: Some anomalous scores not higher than normal\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: Visual validation\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"[TEST 3] Visual Validation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Select one class for visualization\n",
    "TEST_CLASS = CLASSES[0]\n",
    "model = trained_models[TEST_CLASS]\n",
    "split_data = splits[TEST_CLASS]\n",
    "\n",
    "# Create val loader\n",
    "val_loader = create_dataloader(\n",
    "    split_dict=split_data['val'],\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get one normal and one anomalous sample\n",
    "normal_images = None\n",
    "anomalous_images = None\n",
    "\n",
    "for images, masks, labels, paths in val_loader:\n",
    "    if labels.item() == 0 and normal_images is None:\n",
    "        normal_images = images\n",
    "    elif labels.item() == 1 and anomalous_images is None:\n",
    "        anomalous_images = images\n",
    "    \n",
    "    if normal_images is not None and anomalous_images is not None:\n",
    "        break\n",
    "\n",
    "# Predict\n",
    "normal_score, normal_heatmap = model.predict(normal_images, return_heatmaps=True)\n",
    "anomalous_score, anomalous_heatmap = model.predict(anomalous_images, return_heatmaps=True)\n",
    "\n",
    "# Handle scalar vs array\n",
    "if isinstance(normal_score, np.ndarray):\n",
    "    normal_score = normal_score.item()\n",
    "if isinstance(anomalous_score, np.ndarray):\n",
    "    anomalous_score = anomalous_score.item()\n",
    "\n",
    "print(f\"[OK] Predictions generated:\")\n",
    "print(f\"  Normal score: {normal_score:.4f}\")\n",
    "print(f\"  Anomalous score: {anomalous_score:.4f}\\n\")\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return torch.clamp(tensor * std + mean, 0, 1)\n",
    "\n",
    "normal_img = denormalize(normal_images[0].cpu(), config.dataset.normalize.mean, config.dataset.normalize.std)\n",
    "normal_img = normal_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "anomalous_img = denormalize(anomalous_images[0].cpu(), config.dataset.normalize.mean, config.dataset.normalize.std)\n",
    "anomalous_img = anomalous_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Normal sample\n",
    "axes[0, 0].imshow(normal_img)\n",
    "axes[0, 0].set_title(f'Normal Image\\nScore: {normal_score:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "if normal_heatmap is not None:\n",
    "    im1 = axes[0, 1].imshow(normal_heatmap, cmap='jet')\n",
    "    axes[0, 1].set_title('Anomaly Heatmap', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "    \n",
    "    axes[0, 2].imshow(normal_img)\n",
    "    axes[0, 2].imshow(normal_heatmap, cmap='jet', alpha=0.5)\n",
    "    axes[0, 2].set_title('Overlay', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "\n",
    "# Anomalous sample\n",
    "axes[1, 0].imshow(anomalous_img)\n",
    "axes[1, 0].set_title(f'Anomalous Image\\nScore: {anomalous_score:.4f}', fontsize=12, fontweight='bold', color='red')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "if anomalous_heatmap is not None:\n",
    "    im2 = axes[1, 1].imshow(anomalous_heatmap, cmap='jet')\n",
    "    axes[1, 1].set_title('Anomaly Heatmap', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[1, 1], fraction=0.046)\n",
    "    \n",
    "    axes[1, 2].imshow(anomalous_img)\n",
    "    axes[1, 2].imshow(anomalous_heatmap, cmap='jet', alpha=0.5)\n",
    "    axes[1, 2].set_title('Overlay', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle(f'PaDiM Predictions - {TEST_CLASS.upper()}', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"[PASS] TEST 3 PASSED: Visualizations generated successfully\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7e2a6",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Create a ZIP package with all outputs for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8695c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"[PACKAGE] Preparing Download Package\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create ZIP\n",
    "zip_path = PROJECT_ROOT / 'padim_outputs.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add models\n",
    "    for class_name in CLASSES:\n",
    "        model_file = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "        stats_file = MODEL_DIR / f\"padim_{class_name}_clean.json\"\n",
    "        \n",
    "        if model_file.exists():\n",
    "            zipf.write(model_file, f\"models/{model_file.name}\")\n",
    "            print(f\"[OK] Added: {model_file.name}\")\n",
    "        \n",
    "        if stats_file.exists():\n",
    "            zipf.write(stats_file, f\"models/{stats_file.name}\")\n",
    "            print(f\"[OK] Added: {stats_file.name}\")\n",
    "    \n",
    "    # Add results JSON\n",
    "    if results_path.exists():\n",
    "        zipf.write(results_path, f\"results/{results_path.name}\")\n",
    "        print(f\"[OK] Added: {results_path.name}\")\n",
    "\n",
    "print(f\"\\n[OK] Package created: {zip_path.name}\")\n",
    "print(f\"   Size: {zip_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Download\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"[DOWNLOAD] Starting download...\")\n",
    "print(\"=\"*80)\n",
    "files.download(str(zip_path))\n",
    "print(\"\\n[OK] Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc6451",
   "metadata": {},
   "source": [
    "## Save to Google Drive\n",
    "\n",
    "Copy all outputs to Google Drive for persistent storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b512b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Create output directory in Google Drive\n",
    "DRIVE_OUTPUT = Path('/content/drive/MyDrive/anomaly_detection_project/05_padim_clean_outputs')\n",
    "DRIVE_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"[SAVE] Copying outputs to Google Drive...\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Copy model files\n",
    "models_saved = []\n",
    "for class_name in CLASSES:\n",
    "    model_file = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "    stats_file = MODEL_DIR / f\"padim_{class_name}_clean.json\"\n",
    "    \n",
    "    if model_file.exists():\n",
    "        shutil.copy2(model_file, DRIVE_OUTPUT / model_file.name)\n",
    "        models_saved.append(model_file.name)\n",
    "        print(f\"[OK] Saved: {model_file.name}\")\n",
    "    \n",
    "    if stats_file.exists():\n",
    "        shutil.copy2(stats_file, DRIVE_OUTPUT / stats_file.name)\n",
    "        print(f\"[OK] Saved: {stats_file.name}\")\n",
    "\n",
    "# 2. Copy results files\n",
    "results_files = [\n",
    "    (PROJECT_ROOT / 'outputs' / 'results' / 'padim_training_stats_clean.csv', 'padim_training_stats_clean.csv'),\n",
    "    (PROJECT_ROOT / 'outputs' / 'results' / 'padim_training_results_clean.json', 'padim_training_results_clean.json')\n",
    "]\n",
    "\n",
    "for src, dst_name in results_files:\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, DRIVE_OUTPUT / dst_name)\n",
    "        print(f\"[OK] Saved: {dst_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[SUCCESS] OUTPUTS SAVED TO GOOGLE DRIVE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Location: {DRIVE_OUTPUT}\")\n",
    "print(f\"Total files: {len(list(DRIVE_OUTPUT.iterdir()))}\")\n",
    "print(f\"Total size: {sum(f.stat().st_size for f in DRIVE_OUTPUT.iterdir() if f.is_file()) / (1024*1024):.2f} MB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140763d",
   "metadata": {},
   "source": [
    "---\n",
    "### Outputs\n",
    "\n",
    "```\n",
    "outputs/models/\n",
    "├── padim_hazelnut_clean.pt      (~50-100 MB each)\n",
    "├── padim_hazelnut_clean.json\n",
    "├── padim_carpet_clean.pt\n",
    "├── padim_carpet_clean.json\n",
    "├── padim_zipper_clean.pt\n",
    "└── padim_zipper_clean.json\n",
    "\n",
    "outputs/results/\n",
    "└── padim_training_results_clean.json\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
