{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6aa9d0",
   "metadata": {},
   "source": [
    "# 01 - Exploratory Data Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/01_data_exploration.ipynb)\n",
    "\n",
    "**Project**: Detection of Anomalies with Localization  \n",
    "**Dataset**: MVTec AD (Hazelnut, Carpet, Zipper)  \n",
    "**Phase**: Data Exploration\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Analyze dataset structure and organization\n",
    "2. Count images per class and split (train/test, normal/anomalous)\n",
    "3. Visualize representative samples (normal and anomalous)\n",
    "4. Compute image dimension statistics\n",
    "5. Analyze defect type distributions\n",
    "6. Visualize ground truth masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09279bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"Cloning repository...\")\n",
    "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('/content/Detection-of-Anomalies-with-Localization')\n",
    "DRIVE_DATASET = Path('/content/drive/MyDrive/mvtec_ad')\n",
    "LOCAL_DATASET = Path('/content/mvtec_ad')\n",
    "OUTPUT_DIR = Path('/content/drive/MyDrive/anomaly_detection_project/eda_outputs')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy dataset from Drive to local (faster access)\n",
    "print(\"Copying dataset from Drive to local storage...\")\n",
    "if not LOCAL_DATASET.exists():\n",
    "    !mkdir -p /content/mvtec_ad\n",
    "    !cp -r /content/drive/MyDrive/mvtecad/hazelnut /content/mvtec_ad/\n",
    "    !cp -r /content/drive/MyDrive/mvtecad/carpet /content/mvtec_ad/\n",
    "    !cp -r /content/drive/MyDrive/mvtecad/zipper /content/mvtec_ad/\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset already in local storage\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Project:  {PROJECT_ROOT}\")\n",
    "print(f\"Dataset:  {LOCAL_DATASET}\")\n",
    "print(f\"Outputs:  {OUTPUT_DIR}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fd8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import Config\n",
    "\n",
    "# Plotting settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All imports loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Load project configuration\n",
    "config = Config.load(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
    "\n",
    "# Classes to analyze\n",
    "CLASSES = ['hazelnut', 'carpet', 'zipper']\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(config.seed)\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Analyzing classes: {CLASSES}\")\n",
    "print(f\"Random seed: {config.seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b35da4",
   "metadata": {},
   "source": [
    "## 1. Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade961e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_class_structure(class_name: str) -> dict:\n",
    "    \"\"\"Explore directory structure for a given class.\"\"\"\n",
    "    class_path = LOCAL_DATASET / class_name\n",
    "    \n",
    "    structure = {\n",
    "        'class': class_name,\n",
    "        'train_good': [],\n",
    "        'test_good': [],\n",
    "        'test_defects': {},\n",
    "        'ground_truth': {}\n",
    "    }\n",
    "    \n",
    "    # Train images (all normal)\n",
    "    train_path = class_path / 'train' / 'good'\n",
    "    if train_path.exists():\n",
    "        structure['train_good'] = sorted(list(train_path.glob('*.png')))\n",
    "    \n",
    "    # Test normal images\n",
    "    test_good_path = class_path / 'test' / 'good'\n",
    "    if test_good_path.exists():\n",
    "        structure['test_good'] = sorted(list(test_good_path.glob('*.png')))\n",
    "    \n",
    "    # Test anomalous images (by defect type)\n",
    "    test_path = class_path / 'test'\n",
    "    if test_path.exists():\n",
    "        for defect_dir in test_path.iterdir():\n",
    "            if defect_dir.is_dir() and defect_dir.name != 'good':\n",
    "                defect_type = defect_dir.name\n",
    "                structure['test_defects'][defect_type] = sorted(list(defect_dir.glob('*.png')))\n",
    "    \n",
    "    # Ground truth masks\n",
    "    gt_path = class_path / 'ground_truth'\n",
    "    if gt_path.exists():\n",
    "        for defect_dir in gt_path.iterdir():\n",
    "            if defect_dir.is_dir():\n",
    "                defect_type = defect_dir.name\n",
    "                structure['ground_truth'][defect_type] = sorted(list(defect_dir.glob('*.png')))\n",
    "    \n",
    "    return structure\n",
    "\n",
    "# Explore all classes\n",
    "dataset_structure = {}\n",
    "for class_name in CLASSES:\n",
    "    print(f\"Analyzing {class_name}...\")\n",
    "    structure = explore_class_structure(class_name)\n",
    "    dataset_structure[class_name] = structure\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"  Train (normal): {len(structure['train_good'])}\")\n",
    "    print(f\"  Test (normal):  {len(structure['test_good'])}\")\n",
    "    print(f\"  Defect types:   {len(structure['test_defects'])}\")\n",
    "    for defect, imgs in structure['test_defects'].items():\n",
    "        print(f\"    - {defect}: {len(imgs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082d056",
   "metadata": {},
   "source": [
    "## 2. Image Count Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_dataframe(dataset_structure: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive dataframe with image counts.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with counts per class and split\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        # Normal images\n",
    "        data.append({\n",
    "            'Class': class_name,\n",
    "            'Split': 'Train',\n",
    "            'Type': 'Normal',\n",
    "            'Defect': 'good',\n",
    "            'Count': len(structure['train_good'])\n",
    "        })\n",
    "        \n",
    "        data.append({\n",
    "            'Class': class_name,\n",
    "            'Split': 'Test',\n",
    "            'Type': 'Normal',\n",
    "            'Defect': 'good',\n",
    "            'Count': len(structure['test_good'])\n",
    "        })\n",
    "        \n",
    "        # Anomalous images by defect type\n",
    "        for defect, imgs in structure['test_defects'].items():\n",
    "            data.append({\n",
    "                'Class': class_name,\n",
    "                'Split': 'Test',\n",
    "                'Type': 'Anomalous',\n",
    "                'Defect': defect,\n",
    "                'Count': len(imgs)\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create dataframe\n",
    "df_counts = create_count_dataframe(dataset_structure)\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(df_counts.to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY BY CLASS\")\n",
    "print(\"=\"*60)\n",
    "summary = df_counts.groupby(['Class', 'Type'])['Count'].sum().reset_index()\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# Total counts\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "total_normal = df_counts[df_counts['Type'] == 'Normal']['Count'].sum()\n",
    "total_anomalous = df_counts[df_counts['Type'] == 'Anomalous']['Count'].sum()\n",
    "print(f\"Total Normal Images:    {total_normal}\")\n",
    "print(f\"Total Anomalous Images: {total_anomalous}\")\n",
    "print(f\"Total Images:           {total_normal + total_anomalous}\")\n",
    "print(f\"Anomaly Ratio:          {total_anomalous / (total_normal + total_anomalous):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278e3b3",
   "metadata": {},
   "source": [
    "### Visualization: Image Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Normal vs Anomalous by Class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Stacked bar chart\n",
    "pivot_data = df_counts.groupby(['Class', 'Type'])['Count'].sum().unstack()\n",
    "pivot_data.plot(kind='bar', stacked=True, ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Image Distribution: Normal vs Anomalous', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[0].legend(title='Type', loc='upper right')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "# Train vs Test split\n",
    "split_data = df_counts[df_counts['Type'] == 'Normal'].groupby(['Class', 'Split'])['Count'].sum().unstack()\n",
    "split_data.plot(kind='bar', ax=axes[1], color=['#3498db', '#9b59b6'])\n",
    "axes[1].set_title('Normal Images: Train vs Test Split', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Number of Images', fontsize=12)\n",
    "axes[1].legend(title='Split', loc='upper right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Defect Types Distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for idx, class_name in enumerate(CLASSES):\n",
    "    class_data = df_counts[(df_counts['Class'] == class_name) & (df_counts['Type'] == 'Anomalous')]\n",
    "    \n",
    "    axes[idx].barh(class_data['Defect'], class_data['Count'], color='#e67e22')\n",
    "    axes[idx].set_title(f'{class_name.capitalize()} - Defect Types', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Number of Images', fontsize=10)\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(class_data['Count']):\n",
    "        axes[idx].text(v + 0.5, i, str(v), va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900f410d",
   "metadata": {},
   "source": [
    "## 3. Image Dimension Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_dimensions(dataset_structure: Dict, sample_size: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze image dimensions across dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "        sample_size: Number of images to sample per class\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with dimension statistics\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        print(f\"Analyzing dimensions for {class_name}...\")\n",
    "        \n",
    "        # Sample images from train\n",
    "        train_sample = structure['train_good'][:sample_size]\n",
    "        \n",
    "        for img_path in tqdm(train_sample, desc=f\"  {class_name}\"):\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            \n",
    "            data.append({\n",
    "                'Class': class_name,\n",
    "                'Width': width,\n",
    "                'Height': height,\n",
    "                'Aspect_Ratio': width / height,\n",
    "                'Megapixels': (width * height) / 1e6\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Analyze dimensions\n",
    "df_dimensions = analyze_image_dimensions(dataset_structure, sample_size=50)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMAGE DIMENSION STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(df_dimensions.groupby('Class').describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7b498",
   "metadata": {},
   "source": [
    "### Visualization: Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Resolution distribution\n",
    "for class_name in CLASSES:\n",
    "    class_data = df_dimensions[df_dimensions['Class'] == class_name]\n",
    "    axes[0].scatter(class_data['Width'], class_data['Height'], \n",
    "                   label=class_name, alpha=0.6, s=100)\n",
    "\n",
    "axes[0].set_title('Image Resolution Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Width (pixels)', fontsize=12)\n",
    "axes[0].set_ylabel('Height (pixels)', fontsize=12)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Aspect ratio distribution\n",
    "df_dimensions.boxplot(column='Aspect_Ratio', by='Class', ax=axes[1])\n",
    "axes[1].set_title('Aspect Ratio Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Aspect Ratio', fontsize=12)\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESOLUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "resolution_summary = df_dimensions.groupby('Class').agg({\n",
    "    'Width': ['mean', 'std', 'min', 'max'],\n",
    "    'Height': ['mean', 'std', 'min', 'max'],\n",
    "    'Aspect_Ratio': ['mean', 'std']\n",
    "}).round(2)\n",
    "print(resolution_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfbc7b",
   "metadata": {},
   "source": [
    "## 4. Visual Inspection: Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset_structure: Dict, n_samples: int = 3):\n",
    "    \"\"\"\n",
    "    Visualize sample images from each class.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "        n_samples: Number of samples per category\n",
    "    \"\"\"\n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"CLASS: {class_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Normal images (train)\n",
    "        fig, axes = plt.subplots(1, n_samples, figsize=(15, 5))\n",
    "        fig.suptitle(f'{class_name.capitalize()} - Normal Images (Train)', \n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        for idx in range(n_samples):\n",
    "            img_path = structure['train_good'][idx * len(structure['train_good']) // n_samples]\n",
    "            img = Image.open(img_path)\n",
    "            axes[idx].imshow(img)\n",
    "            axes[idx].axis('off')\n",
    "            axes[idx].set_title(f\"Sample {idx+1}\\n{img.size[0]}x{img.size[1]}\", fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Anomalous images (sample from different defect types)\n",
    "        defect_types = list(structure['test_defects'].keys())[:n_samples]\n",
    "        \n",
    "        if defect_types:\n",
    "            fig, axes = plt.subplots(1, len(defect_types), figsize=(15, 5))\n",
    "            if len(defect_types) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            fig.suptitle(f'{class_name.capitalize()} - Anomalous Images', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            \n",
    "            for idx, defect in enumerate(defect_types):\n",
    "                img_path = structure['test_defects'][defect][0]\n",
    "                img = Image.open(img_path)\n",
    "                axes[idx].imshow(img)\n",
    "                axes[idx].axis('off')\n",
    "                axes[idx].set_title(f\"Defect: {defect}\\n{img.size[0]}x{img.size[1]}\", fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_samples(dataset_structure, n_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491bfe66",
   "metadata": {},
   "source": [
    "## 5. Ground Truth Mask Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_masks(dataset_structure: Dict, n_samples: int = 2):\n",
    "    \"\"\"\n",
    "    Visualize anomalous images with their ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        dataset_structure: Dataset structure dictionary\n",
    "        n_samples: Number of samples to show per class\n",
    "    \"\"\"\n",
    "    for class_name, structure in dataset_structure.items():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"MASKS: {class_name.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        defect_types = list(structure['test_defects'].keys())[:n_samples]\n",
    "        \n",
    "        for defect in defect_types:\n",
    "            if defect in structure['ground_truth']:\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                fig.suptitle(f'{class_name.capitalize()} - Defect: {defect}', \n",
    "                            fontsize=14, fontweight='bold')\n",
    "                \n",
    "                # Image\n",
    "                img_path = structure['test_defects'][defect][0]\n",
    "                img = Image.open(img_path)\n",
    "                axes[0].imshow(img)\n",
    "                axes[0].axis('off')\n",
    "                axes[0].set_title('Anomalous Image', fontsize=12)\n",
    "                \n",
    "                # Mask\n",
    "                mask_path = structure['ground_truth'][defect][0]\n",
    "                mask = Image.open(mask_path)\n",
    "                axes[1].imshow(mask, cmap='gray')\n",
    "                axes[1].axis('off')\n",
    "                axes[1].set_title('Ground Truth Mask', fontsize=12)\n",
    "                \n",
    "                # Overlay\n",
    "                img_array = np.array(img)\n",
    "                mask_array = np.array(mask)\n",
    "                overlay = img_array.copy()\n",
    "                overlay[mask_array > 0] = [255, 0, 0]  # Red overlay on defects\n",
    "                axes[2].imshow(overlay)\n",
    "                axes[2].axis('off')\n",
    "                axes[2].set_title('Overlay (Defects in Red)', fontsize=12)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Visualize masks\n",
    "visualize_masks(dataset_structure, n_samples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6973f",
   "metadata": {},
   "source": [
    "## 6. Key Findings & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13098f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS FROM EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Class balance\n",
    "print(\"\\n1. CLASS DISTRIBUTION\")\n",
    "print(\"-\" * 40)\n",
    "for class_name in CLASSES:\n",
    "    class_data = df_counts[df_counts['Class'] == class_name]\n",
    "    n_normal = class_data[class_data['Type'] == 'Normal']['Count'].sum()\n",
    "    n_anomalous = class_data[class_data['Type'] == 'Anomalous']['Count'].sum()\n",
    "    ratio = n_anomalous / (n_normal + n_anomalous) * 100\n",
    "    print(f\"  {class_name.capitalize():10s}: {n_normal:3d} normal, {n_anomalous:3d} anomalous ({ratio:.1f}% anomaly rate)\")\n",
    "\n",
    "# 2. Image dimensions\n",
    "print(\"\\n2. IMAGE DIMENSIONS\")\n",
    "print(\"-\" * 40)\n",
    "for class_name in CLASSES:\n",
    "    class_dims = df_dimensions[df_dimensions['Class'] == class_name]\n",
    "    avg_w = class_dims['Width'].mean()\n",
    "    avg_h = class_dims['Height'].mean()\n",
    "    print(f\"  {class_name.capitalize():10s}: {avg_w:.0f} x {avg_h:.0f} pixels (avg)\")\n",
    "\n",
    "# 3. Defect diversity\n",
    "print(\"\\n3. DEFECT TYPE DIVERSITY\")\n",
    "print(\"-\" * 40)\n",
    "for class_name in CLASSES:\n",
    "    n_defects = len(dataset_structure[class_name]['test_defects'])\n",
    "    defect_names = list(dataset_structure[class_name]['test_defects'].keys())\n",
    "    print(f\"  {class_name.capitalize():10s}: {n_defects} defect types - {', '.join(defect_names[:5])}\")\n",
    "\n",
    "# 4. Recommendations\n",
    "print(\"\\n4. RECOMMENDATIONS FOR PREPROCESSING\")\n",
    "print(\"-\" * 40)\n",
    "print(\"  - Resize all images to 224x224 (ResNet standard)\")\n",
    "print(\"  - Maintain aspect ratio during resizing to avoid distortion\")\n",
    "print(\"  - Apply ImageNet normalization for pre-trained backbone\")\n",
    "print(\"  - Consider data augmentation for anomalous samples (limited quantity)\")\n",
    "print(\"  - Ensure ground truth masks are resized consistently with images\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETE - Ready for data preparation phase\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6eca4",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Proceed to **Notebook 02: Data Preparation** to:\n",
    "1. Implement data splitting logic (Train/Val/Test)\n",
    "2. Create MVTecDataset class\n",
    "3. Implement preprocessing transforms\n",
    "4. Save split configurations for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b8d60",
   "metadata": {},
   "source": [
    "## Save Results to Google Drive\n",
    "\n",
    "All outputs will be automatically saved to your Google Drive for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9763b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE RESULTS TO GOOGLE DRIVE\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"Saving results to Google Drive...\")\n",
    "\n",
    "# Save dataset statistics\n",
    "df_counts.to_csv(OUTPUT_DIR / 'dataset_counts.csv', index=False)\n",
    "df_dimensions.to_csv(OUTPUT_DIR / 'image_dimensions.csv', index=False)\n",
    "\n",
    "# Save analysis report\n",
    "report_path = OUTPUT_DIR / 'eda_report.txt'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"EXPLORATORY DATA ANALYSIS REPORT\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. CLASS DISTRIBUTION\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    for class_name in CLASSES:\n",
    "        class_data = df_counts[df_counts['Class'] == class_name]\n",
    "        n_normal = class_data[class_data['Type'] == 'Normal']['Count'].sum()\n",
    "        n_anomalous = class_data[class_data['Type'] == 'Anomalous']['Count'].sum()\n",
    "        ratio = n_anomalous / (n_normal + n_anomalous) * 100\n",
    "        f.write(f\"  {class_name.capitalize():10s}: {n_normal:3d} normal, {n_anomalous:3d} anomalous ({ratio:.1f}% anomaly rate)\\n\")\n",
    "    \n",
    "    f.write(\"\\n2. IMAGE DIMENSIONS\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    for class_name in CLASSES:\n",
    "        class_dims = df_dimensions[df_dimensions['Class'] == class_name]\n",
    "        avg_w = class_dims['Width'].mean()\n",
    "        avg_h = class_dims['Height'].mean()\n",
    "        f.write(f\"  {class_name.capitalize():10s}: {avg_w:.0f} x {avg_h:.0f} pixels (avg)\\n\")\n",
    "    \n",
    "    f.write(\"\\n3. DEFECT TYPE DIVERSITY\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\")\n",
    "    for class_name in CLASSES:\n",
    "        n_defects = len(dataset_structure[class_name]['test_defects'])\n",
    "        defect_names = list(dataset_structure[class_name]['test_defects'].keys())\n",
    "        f.write(f\"  {class_name.capitalize():10s}: {n_defects} defect types - {', '.join(defect_names[:5])}\\n\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'classes': CLASSES,\n",
    "    'total_images': {\n",
    "        'normal': int(df_counts[df_counts['Type'] == 'Normal']['Count'].sum()),\n",
    "        'anomalous': int(df_counts[df_counts['Type'] == 'Anomalous']['Count'].sum())\n",
    "    },\n",
    "    'class_details': {}\n",
    "}\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    class_data = df_counts[df_counts['Class'] == class_name]\n",
    "    metadata['class_details'][class_name] = {\n",
    "        'normal': int(class_data[class_data['Type'] == 'Normal']['Count'].sum()),\n",
    "        'anomalous': int(class_data[class_data['Type'] == 'Anomalous']['Count'].sum()),\n",
    "        'defect_types': list(dataset_structure[class_name]['test_defects'].keys())\n",
    "    }\n",
    "\n",
    "with open(OUTPUT_DIR / 'dataset_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"  - dataset_counts.csv\")\n",
    "print(\"  - image_dimensions.csv\")\n",
    "print(\"  - eda_report.txt\")\n",
    "print(\"  - dataset_metadata.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593a71e",
   "metadata": {},
   "source": [
    "## Download Results ZIP\n",
    "\n",
    "Download all generated files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DOWNLOAD RESULTS ZIP\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Creating ZIP archive...\")\n",
    "output_zip = 'notebook_01_eda_outputs.zip'\n",
    "!zip -r {output_zip} {OUTPUT_DIR}\n",
    "\n",
    "print(f\"Downloading: {output_zip}\")\n",
    "files.download(output_zip)\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "print(\"Next steps:\")\n",
    "print(\"  1. Extract ZIP file\")\n",
    "print(\"  2. Copy files to local project\")\n",
    "print(\"  3. git add results/\")\n",
    "print(\"  4. git commit -m 'Add notebook 01 outputs'\")\n",
    "print(\"  5. git push origin main\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
