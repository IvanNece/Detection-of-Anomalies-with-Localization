{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e7b4bf",
   "metadata": {},
   "source": [
    "# 05 - PaDiM Baseline Training (Clean Domain)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/05_padim_baseline_clean.ipynb)\n",
    "\n",
    "**Phase 4: Baseline Model Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ Objectives\n",
    "\n",
    "1. âœ… Train PaDiM on **Train-clean** split (normal images only)\n",
    "2. âœ… Use consistent preprocessing with PatchCore\n",
    "3. âœ… Save trained models for each class\n",
    "4. âœ… Comprehensive testing and validation\n",
    "\n",
    "## ğŸ”¬ PaDiM Method\n",
    "\n",
    "**PaDiM** = Probabilistic Anomaly Detection with Multi-scale features\n",
    "\n",
    "- Extracts multi-scale features from ResNet\n",
    "- Models normal appearance using Gaussian distributions\n",
    "- Uses Mahalanobis distance for anomaly scoring\n",
    "- **No gradient-based training** (statistical approach)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8105a2",
   "metadata": {},
   "source": [
    "## Setup - Mount Drive & Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737fa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository\n",
    "print(\"Cloning repository...\")\n",
    "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git 2>/dev/null || echo \"Repository already exists\"\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path('/content/Detection-of-Anomalies-with-Localization')\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"ğŸ“ Project root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ“ Path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b84769",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import json\n",
    "import time\n",
    "import zipfile\n",
    "from typing import Dict\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project modules (shared with PatchCore)\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import Config\n",
    "from src.data.transforms import get_clean_transforms\n",
    "from src.data.anomalib_adapter import load_split_data, create_anomalib_dataloader, get_split_statistics\n",
    "from src.models.padim_wrapper import PadimWrapper\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"âœ“ All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efbae4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ce83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config.load(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Setup\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CLASSES = config.dataset.classes\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2  # Colab-optimized\n",
    "\n",
    "print(\"ğŸ“‹ Configuration:\")\n",
    "print(f\"  Seed: {config.seed}\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Classes: {CLASSES}\")\n",
    "print(f\"  Image size: {config.dataset.image_size}\")\n",
    "print(f\"\\nğŸ”§ PaDiM Configuration:\")\n",
    "print(f\"  Backbone: {config.padim.backbone}\")\n",
    "print(f\"  Layers: {config.padim.layers}\")\n",
    "print(f\"  N features: {config.padim.n_features}\")\n",
    "print(f\"  Distance: {config.padim.distance_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07dfd3b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2420fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits (shared with PatchCore - same splits for fair comparison)\n",
    "splits_path = PROJECT_ROOT / 'data' / 'processed' / 'clean_splits.json'\n",
    "\n",
    "with open(splits_path, 'r') as f:\n",
    "    all_splits = json.load(f)\n",
    "\n",
    "print(\"âœ“ Loaded clean splits\")\n",
    "print(f\"  Classes: {list(all_splits.keys())}\")\n",
    "\n",
    "# Display split statistics\n",
    "print(\"\\nğŸ“Š Split Statistics:\\n\")\n",
    "for class_name in CLASSES:\n",
    "    split_data = all_splits[class_name]\n",
    "    train_stats = get_split_statistics(split_data, 'train')\n",
    "    val_stats = get_split_statistics(split_data, 'val')\n",
    "    \n",
    "    print(f\"{class_name.upper()}:\")\n",
    "    print(f\"  Train: {train_stats['num_samples']} samples (100% normal)\")\n",
    "    print(f\"  Val:   {val_stats['num_samples']} samples ({val_stats['num_normal']} normal, {val_stats['num_anomalous']} anomalous)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c11d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transform (shared with PatchCore - same preprocessing)\n",
    "transform = get_clean_transforms(\n",
    "    image_size=config.dataset.image_size,\n",
    "    normalize_mean=config.dataset.normalize.mean,\n",
    "    normalize_std=config.dataset.normalize.std\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Transform initialized:\")\n",
    "print(f\"  Size: {config.dataset.image_size}x{config.dataset.image_size}\")\n",
    "print(f\"  Normalization: ImageNet statistics\")\n",
    "print(f\"  Augmentation: None (deterministic preprocessing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c922c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results tracking\n",
    "MODEL_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "training_results = {\n",
    "    'classes': [],\n",
    "    'num_train_samples': [],\n",
    "    'training_time_seconds': [],\n",
    "    'memory_bank_size_mb': [],\n",
    "    'model_path': []\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "print(f\"ğŸ“ Model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fddd57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PaDiM for each class\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ”§ Training PaDiM: {class_name.upper()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    split_data = all_splits[class_name]\n",
    "    train_loader = create_anomalib_dataloader(\n",
    "        split_data=split_data,\n",
    "        split_type='train',\n",
    "        transform=transform,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        shuffle=False,\n",
    "        pin_memory=True if DEVICE == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Loaded {len(train_loader.dataset)} training samples\\n\")\n",
    "    \n",
    "    # 2. Initialize model\n",
    "    model = PadimWrapper(\n",
    "        backbone=config.padim.backbone,\n",
    "        layers=config.padim.layers,\n",
    "        n_features=config.padim.n_features,\n",
    "        image_size=config.dataset.image_size,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # 3. Train\n",
    "    model.fit(train_loader, verbose=True)\n",
    "    \n",
    "    # 4. Save\n",
    "    model_path = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "    model.save(model_path, include_stats=True)\n",
    "    \n",
    "    # 5. Store results\n",
    "    training_results['classes'].append(class_name)\n",
    "    training_results['num_train_samples'].append(model.training_stats['num_samples'])\n",
    "    training_results['training_time_seconds'].append(model.training_stats['training_time_seconds'])\n",
    "    training_results['memory_bank_size_mb'].append(model.training_stats['memory_bank_size_mb'])\n",
    "    training_results['model_path'].append(str(model_path))\n",
    "    \n",
    "    trained_models[class_name] = model\n",
    "    \n",
    "    print(f\"\\nâœ… Completed: {class_name}\")\n",
    "    print(f\"   Model: {model_path.name}\")\n",
    "    print(f\"   Time: {model.training_stats['training_time_seconds']:.2f}s\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ‰ ALL CLASSES TRAINED SUCCESSFULLY!\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca768b",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(training_results)\n",
    "\n",
    "print(\"ğŸ“Š Training Summary:\\n\")\n",
    "display(results_df)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Overall Statistics:\")\n",
    "print(f\"  Total training time: {results_df['training_time_seconds'].sum():.2f}s\")\n",
    "print(f\"  Average time per class: {results_df['training_time_seconds'].mean():.2f}s\")\n",
    "print(f\"  Total model size: {results_df['memory_bank_size_mb'].sum():.2f} MB\")\n",
    "\n",
    "# Save results\n",
    "results_path = PROJECT_ROOT / 'outputs' / 'results' / 'padim_training_results_clean.json'\n",
    "results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved: {results_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training time\n",
    "axes[0].bar(results_df['classes'], results_df['training_time_seconds'], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "axes[0].set_title('PaDiM Training Time per Class', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['training_time_seconds']):\n",
    "    axes[0].text(i, v + 0.5, f\"{v:.1f}s\", ha='center', fontweight='bold')\n",
    "\n",
    "# Memory bank size\n",
    "axes[1].bar(results_df['classes'], results_df['memory_bank_size_mb'], color='coral', alpha=0.8)\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Memory Bank Size (MB)', fontsize=12)\n",
    "axes[1].set_title('PaDiM Memory Usage per Class', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['memory_bank_size_mb']):\n",
    "    axes[1].text(i, v + 1, f\"{v:.1f}MB\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902b8f12",
   "metadata": {},
   "source": [
    "## Testing and Validation\n",
    "\n",
    "Comprehensive tests to verify models work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804bb6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: Verify all models load correctly\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ§ª TEST 1: Model Loading\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "all_load_success = True\n",
    "for class_name in CLASSES:\n",
    "    model_path = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "    try:\n",
    "        test_model = PadimWrapper(device=DEVICE)\n",
    "        test_model.load(model_path)\n",
    "        print(f\"âœ“ {class_name}: Loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— {class_name}: Failed - {e}\")\n",
    "        all_load_success = False\n",
    "\n",
    "if all_load_success:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… TEST 1 PASSED: All models load correctly\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âŒ TEST 1 FAILED: Some models failed to load\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: Verify predictions work and anomalous > normal\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"ğŸ§ª TEST 2: Prediction Validation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"Testing {class_name}...\")\n",
    "    model = trained_models[class_name]\n",
    "    split_data = all_splits[class_name]\n",
    "    \n",
    "    # Create val loader\n",
    "    val_loader = create_anomalib_dataloader(\n",
    "        split_data=split_data,\n",
    "        split_type='val',\n",
    "        transform=transform,\n",
    "        batch_size=1,\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Get one normal and one anomalous sample\n",
    "    normal_score = None\n",
    "    anomalous_score = None\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        label = batch['label'].item()\n",
    "        \n",
    "        if label == 0 and normal_score is None:\n",
    "            score, _ = model.predict(batch['image'], return_heatmap=False)\n",
    "            normal_score = score\n",
    "        elif label == 1 and anomalous_score is None:\n",
    "            score, _ = model.predict(batch['image'], return_heatmap=False)\n",
    "            anomalous_score = score\n",
    "        \n",
    "        if normal_score is not None and anomalous_score is not None:\n",
    "            break\n",
    "    \n",
    "    # Check result\n",
    "    passed = anomalous_score > normal_score if (normal_score and anomalous_score) else False\n",
    "    status = \"âœ“\" if passed else \"âœ—\"\n",
    "    \n",
    "    print(f\"  {status} Normal score: {normal_score:.4f}\")\n",
    "    print(f\"  {status} Anomalous score: {anomalous_score:.4f}\")\n",
    "    print(f\"  {status} Test: {'PASSED' if passed else 'FAILED'}\\n\")\n",
    "    \n",
    "    test_results.append(passed)\n",
    "\n",
    "if all(test_results):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"âœ… TEST 2 PASSED: Anomalous scores > Normal scores for all classes\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"âš ï¸  TEST 2 WARNING: Some anomalous scores not higher than normal\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: Visual validation\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"ğŸ§ª TEST 3: Visual Validation\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "TEST_CLASS = 'hazelnut'\n",
    "print(f\"Testing visual predictions for: {TEST_CLASS}\\n\")\n",
    "\n",
    "model = trained_models[TEST_CLASS]\n",
    "split_data = all_splits[TEST_CLASS]\n",
    "\n",
    "val_loader = create_anomalib_dataloader(\n",
    "    split_data=split_data,\n",
    "    split_type='val',\n",
    "    transform=transform,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Get samples\n",
    "normal_sample = None\n",
    "anomalous_sample = None\n",
    "\n",
    "for batch in val_loader:\n",
    "    if batch['label'].item() == 0 and normal_sample is None:\n",
    "        normal_sample = batch\n",
    "    elif batch['label'].item() == 1 and anomalous_sample is None:\n",
    "        anomalous_sample = batch\n",
    "    if normal_sample and anomalous_sample:\n",
    "        break\n",
    "\n",
    "# Predict\n",
    "normal_score, normal_heatmap = model.predict(normal_sample['image'], return_heatmap=True)\n",
    "anomalous_score, anomalous_heatmap = model.predict(anomalous_sample['image'], return_heatmap=True)\n",
    "\n",
    "print(f\"âœ“ Predictions generated:\")\n",
    "print(f\"  Normal score: {normal_score:.4f}\")\n",
    "print(f\"  Anomalous score: {anomalous_score:.4f}\\n\")\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(tensor, mean, std):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return torch.clamp(tensor * std + mean, 0, 1)\n",
    "\n",
    "normal_img = denormalize(normal_sample['image'][0].cpu(), config.dataset.normalize.mean, config.dataset.normalize.std)\n",
    "normal_img = normal_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "anomalous_img = denormalize(anomalous_sample['image'][0].cpu(), config.dataset.normalize.mean, config.dataset.normalize.std)\n",
    "anomalous_img = anomalous_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Normal sample\n",
    "axes[0, 0].imshow(normal_img)\n",
    "axes[0, 0].set_title(f'Normal Image\\nScore: {normal_score:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "if normal_heatmap is not None:\n",
    "    im1 = axes[0, 1].imshow(normal_heatmap, cmap='jet')\n",
    "    axes[0, 1].set_title('Anomaly Heatmap', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[0, 1], fraction=0.046)\n",
    "    \n",
    "    axes[0, 2].imshow(normal_img)\n",
    "    axes[0, 2].imshow(normal_heatmap, cmap='jet', alpha=0.5)\n",
    "    axes[0, 2].set_title('Overlay', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "\n",
    "# Anomalous sample\n",
    "axes[1, 0].imshow(anomalous_img)\n",
    "axes[1, 0].set_title(f'Anomalous Image\\nScore: {anomalous_score:.4f}', fontsize=12, fontweight='bold', color='red')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "if anomalous_heatmap is not None:\n",
    "    im2 = axes[1, 1].imshow(anomalous_heatmap, cmap='jet')\n",
    "    axes[1, 1].set_title('Anomaly Heatmap', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[1, 1], fraction=0.046)\n",
    "    \n",
    "    axes[1, 2].imshow(anomalous_img)\n",
    "    axes[1, 2].imshow(anomalous_heatmap, cmap='jet', alpha=0.5)\n",
    "    axes[1, 2].set_title('Overlay', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle(f'PaDiM Predictions - {TEST_CLASS.upper()}', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"âœ… TEST 3 PASSED: Visualizations generated successfully\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e7e2a6",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "Create a ZIP package with all outputs for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8695c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ“¦ Preparing Download Package\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Create ZIP\n",
    "zip_path = PROJECT_ROOT / 'padim_outputs.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add models\n",
    "    for class_name in CLASSES:\n",
    "        model_file = MODEL_DIR / f\"padim_{class_name}_clean.pt\"\n",
    "        stats_file = MODEL_DIR / f\"padim_{class_name}_clean.json\"\n",
    "        \n",
    "        if model_file.exists():\n",
    "            zipf.write(model_file, f\"models/{model_file.name}\")\n",
    "            print(f\"âœ“ Added: {model_file.name}\")\n",
    "        \n",
    "        if stats_file.exists():\n",
    "            zipf.write(stats_file, f\"models/{stats_file.name}\")\n",
    "            print(f\"âœ“ Added: {stats_file.name}\")\n",
    "    \n",
    "    # Add results JSON\n",
    "    if results_path.exists():\n",
    "        zipf.write(results_path, f\"results/{results_path.name}\")\n",
    "        print(f\"âœ“ Added: {results_path.name}\")\n",
    "\n",
    "print(f\"\\nâœ… Package created: {zip_path.name}\")\n",
    "print(f\"   Size: {zip_path.stat().st_size / (1024**2):.2f} MB\")\n",
    "\n",
    "# Download\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"â¬‡ï¸  DOWNLOADING...\")\n",
    "print(\"=\"*80)\n",
    "files.download(str(zip_path))\n",
    "print(\"\\nâœ… Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140763d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### âœ… Completed Tasks\n",
    "\n",
    "1. âœ“ Trained PaDiM models for 3 classes (hazelnut, carpet, zipper)\n",
    "2. âœ“ Saved models to `outputs/models/padim_{class}_clean.pt`\n",
    "3. âœ“ Generated training statistics and visualizations\n",
    "4. âœ“ Validated models load correctly\n",
    "5. âœ“ Verified predictions work (anomalous > normal scores)\n",
    "6. âœ“ Created visual validation\n",
    "7. âœ“ Downloaded results package\n",
    "\n",
    "### ğŸ“ Outputs\n",
    "\n",
    "```\n",
    "outputs/models/\n",
    "â”œâ”€â”€ padim_hazelnut_clean.pt      (~50-100 MB each)\n",
    "â”œâ”€â”€ padim_hazelnut_clean.json\n",
    "â”œâ”€â”€ padim_carpet_clean.pt\n",
    "â”œâ”€â”€ padim_carpet_clean.json\n",
    "â”œâ”€â”€ padim_zipper_clean.pt\n",
    "â””â”€â”€ padim_zipper_clean.json\n",
    "\n",
    "outputs/results/\n",
    "â””â”€â”€ padim_training_results_clean.json\n",
    "```\n",
    "\n",
    "### ğŸ”œ Next Steps\n",
    "\n",
    "1. Wait for PatchCore completion (other branch)\n",
    "2. Threshold calibration (notebook 06)\n",
    "3. Evaluation on clean domain\n",
    "4. Model comparison (PaDiM vs PatchCore)\n",
    "5. Domain shift evaluation\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Notes\n",
    "\n",
    "- This notebook uses **shared components** with PatchCore:\n",
    "  - `src/data/transforms.py` - Transform classes\n",
    "  - `src/data/anomalib_adapter.py` - Data adapter\n",
    "  - `data/processed/clean_splits.json` - Splits\n",
    "  - `configs/experiment_config.yaml` - Configuration\n",
    "\n",
    "- Ensure consistency when merging branches for fair model comparison\n",
    "\n",
    "---\n",
    "\n",
    "**Status:** âœ… Phase 4 Complete - PaDiM Baseline Ready for Evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
