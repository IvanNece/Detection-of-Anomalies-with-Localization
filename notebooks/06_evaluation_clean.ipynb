{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/06_evaluation_clean.ipynb)\n",
    "\n",
    "# 06 - Evaluation Clean Domain\n",
    "\n",
    "**Phase 5: Threshold Calibration & Evaluation - Clean Domain**\n",
    "\n",
    "This notebook implements the complete evaluation pipeline:\n",
    "1. Load trained models (PatchCore & PaDiM)\n",
    "2. Calibrate thresholds on Val-clean (F1-optimal)\n",
    "3. Evaluate on Test-clean (image-level & pixel-level metrics)\n",
    "4. Generate visualizations (ROC, PR curves, confusion matrices)\n",
    "5. Save all results\n",
    "\n",
    "**Metrics computed:**\n",
    "- Image-level: AUROC, AUPRC, F1, Accuracy, Precision, Recall\n",
    "- Pixel-level: Pixel AUROC, PRO (Per-Region Overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository on main branch\n",
    "print(\"Cloning repository (branch: main)...\")\n",
    "repo_dir = '/content/Detection-of-Anomalies-with-Localization'\n",
    "\n",
    "# Remove if exists\n",
    "if os.path.exists(repo_dir):\n",
    "    print(\"Removing existing repository...\")\n",
    "    !rm -rf {repo_dir}\n",
    "\n",
    "# Clone from main branch\n",
    "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git {repo_dir}\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(repo_dir)\n",
    "\n",
    "# Dataset location (direct from Drive, no duplication)\n",
    "DATASET_PATH = Path('/content/drive/MyDrive/mvtec_ad')\n",
    "\n",
    "# Output directories on Drive (User specific structure)\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/anomaly_detection_prject')\n",
    "PATCHCORE_MODELS_DIR = DRIVE_ROOT / '04_patchore_clean_outputs'\n",
    "PADIM_MODELS_DIR = DRIVE_ROOT / '05_padim_clean_outputs'\n",
    "\n",
    "RESULTS_DIR = PROJECT_ROOT / 'outputs' / 'results'\n",
    "THRESHOLDS_DIR = PROJECT_ROOT / 'outputs' / 'thresholds'\n",
    "VIZ_DIR = PROJECT_ROOT / 'outputs' / 'visualizations' / 'phase5_clean'\n",
    "\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "THRESHOLDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify dataset exists\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {DATASET_PATH}\\n\"\n",
    "        f\"Please ensure mvtec_ad folder is in your Google Drive root.\"\n",
    "    )\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Project:   {PROJECT_ROOT}\")\n",
    "print(f\"Dataset:   {DATASET_PATH}\")\n",
    "print(f\"PatchCore: {PATCHCORE_MODELS_DIR}\")\n",
    "print(f\"PaDiM:     {PADIM_MODELS_DIR}\")\n",
    "print(f\"Results:   {RESULTS_DIR}\")\n",
    "print(f\"Viz:       {VIZ_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import load_config\n",
    "from src.utils.paths import ProjectPaths\n",
    "from src.data.splitter import load_splits\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.data.transforms import get_clean_transforms\n",
    "from src.models.patchcore import PatchCore\n",
    "from src.models.padim_wrapper import PadimWrapper\n",
    "\n",
    "# Metrics imports\n",
    "from src.metrics import (\n",
    "    calibrate_threshold,\n",
    "    calibrate_threshold_with_curve,\n",
    "    ThresholdCalibrator,\n",
    "    compute_image_metrics,\n",
    "    compute_pixel_metrics,\n",
    "    compute_roc_curve,\n",
    "    compute_pr_curve,\n",
    "    compute_confusion_matrix,\n",
    "    aggregate_metrics,\n",
    "    aggregate_pixel_metrics\n",
    ")\n",
    "\n",
    "from src.evaluation import Evaluator, MultiClassEvaluator\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
    "paths = ProjectPaths(PROJECT_ROOT)\n",
    "\n",
    "# Classes to evaluate\n",
    "CLASSES = config.dataset.classes  # ['hazelnut', 'carpet', 'zipper']\n",
    "\n",
    "# Device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 5: Threshold Calibration & Evaluation - Clean Domain\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Classes: {CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Splits and Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean splits\n",
    "SPLITS_PATH = paths.get_split_path('clean')\n",
    "splits = load_splits(SPLITS_PATH)\n",
    "\n",
    "# Print split statistics\n",
    "print(\"\\nSplit Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for class_name in CLASSES:\n",
    "    train_n = len(splits[class_name]['train']['images'])\n",
    "    val_n = len(splits[class_name]['val']['images'])\n",
    "    test_n = len(splits[class_name]['test']['images'])\n",
    "    \n",
    "    val_normal = sum(1 for l in splits[class_name]['val']['labels'] if l == 0)\n",
    "    val_anom = sum(1 for l in splits[class_name]['val']['labels'] if l == 1)\n",
    "    test_normal = sum(1 for l in splits[class_name]['test']['labels'] if l == 0)\n",
    "    test_anom = sum(1 for l in splits[class_name]['test']['labels'] if l == 1)\n",
    "    \n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Train: {train_n} (all normal)\")\n",
    "    print(f\"  Val: {val_n} ({val_normal} normal, {val_anom} anomalous)\")\n",
    "    print(f\"  Test: {test_n} ({test_normal} normal, {test_anom} anomalous)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training metadata (contains validation predictions)\n",
    "patchcore_metadata_path = RESULTS_DIR / 'patchcore_clean_metadata.json'\n",
    "padim_metadata_path = RESULTS_DIR / 'padim_clean_metadata.json'\n",
    "\n",
    "# Copy metadata from Drive if not present locally\n",
    "if not patchcore_metadata_path.exists() and (PATCHCORE_MODELS_DIR / 'patchcore_clean_metadata.json').exists():\n",
    "    import shutil\n",
    "    shutil.copy(PATCHCORE_MODELS_DIR / 'patchcore_clean_metadata.json', patchcore_metadata_path)\n",
    "    print(\"Copied patchcore metadata from Drive\")\n",
    "\n",
    "if not padim_metadata_path.exists() and (PADIM_MODELS_DIR / 'padim_clean_metadata.json').exists():\n",
    "    import shutil\n",
    "    shutil.copy(PADIM_MODELS_DIR / 'padim_clean_metadata.json', padim_metadata_path)\n",
    "    print(\"Copied padim metadata from Drive\")\n",
    "\n",
    "with open(patchcore_metadata_path, 'r') as f:\n",
    "    patchcore_metadata = json.load(f)\n",
    "    \n",
    "with open(padim_metadata_path, 'r') as f:\n",
    "    padim_metadata = json.load(f)\n",
    "\n",
    "print(\"Loaded training metadata for both methods.\")\n",
    "print(f\"PatchCore trained on: {list(patchcore_metadata['training_statistics'].keys())}\")\n",
    "print(f\"PaDiM trained on: {list(padim_metadata['training_statistics'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Threshold Calibration (Val-clean)\n",
    "\n",
    "For each class and method, we find the F1-optimal threshold on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize threshold calibrators\n",
    "patchcore_calibrator = ThresholdCalibrator('patchcore')\n",
    "padim_calibrator = ThresholdCalibrator('padim')\n",
    "\n",
    "# Store calibration data for visualization\n",
    "calibration_data = {\n",
    "    'patchcore': {},\n",
    "    'padim': {}\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THRESHOLD CALIBRATION (F1-Optimal on Val-clean)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n--- {class_name.upper()} ---\")\n",
    "    \n",
    "    # PatchCore\n",
    "    pc_val = patchcore_metadata['validation_predictions'][class_name]\n",
    "    pc_scores = np.array(pc_val['scores'])\n",
    "    pc_labels = np.array(pc_val['labels'])\n",
    "    \n",
    "    pc_threshold = patchcore_calibrator.calibrate(class_name, pc_scores, pc_labels)\n",
    "    pc_thresh, pc_thresholds, pc_f1_scores = calibrate_threshold_with_curve(pc_scores, pc_labels)\n",
    "    \n",
    "    calibration_data['patchcore'][class_name] = {\n",
    "        'scores': pc_scores,\n",
    "        'labels': pc_labels,\n",
    "        'threshold': pc_threshold,\n",
    "        'threshold_curve': (pc_thresholds, pc_f1_scores)\n",
    "    }\n",
    "    \n",
    "    # PaDiM\n",
    "    pd_val = padim_metadata['validation_predictions'][class_name]\n",
    "    pd_scores = np.array(pd_val['scores'])\n",
    "    pd_labels = np.array(pd_val['labels'])\n",
    "    \n",
    "    pd_threshold = padim_calibrator.calibrate(class_name, pd_scores, pd_labels)\n",
    "    pd_thresh, pd_thresholds, pd_f1_scores = calibrate_threshold_with_curve(pd_scores, pd_labels)\n",
    "    \n",
    "    calibration_data['padim'][class_name] = {\n",
    "        'scores': pd_scores,\n",
    "        'labels': pd_labels,\n",
    "        'threshold': pd_threshold,\n",
    "        'threshold_curve': (pd_thresholds, pd_f1_scores)\n",
    "    }\n",
    "\n",
    "# Save thresholds\n",
    "patchcore_calibrator.save(THRESHOLDS_DIR / 'patchcore_clean_thresholds.json')\n",
    "padim_calibrator.save(THRESHOLDS_DIR / 'padim_clean_thresholds.json')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Thresholds saved to outputs/thresholds/\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Score distributions and thresholds\n",
    "fig, axes = plt.subplots(len(CLASSES), 2, figsize=(14, 4*len(CLASSES)))\n",
    "\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    # PatchCore\n",
    "    ax1 = axes[i, 0]\n",
    "    pc_data = calibration_data['patchcore'][class_name]\n",
    "    normal_scores = pc_data['scores'][pc_data['labels'] == 0]\n",
    "    anomalous_scores = pc_data['scores'][pc_data['labels'] == 1]\n",
    "    \n",
    "    ax1.hist(normal_scores, bins=30, alpha=0.6, label='Normal', color='blue')\n",
    "    ax1.hist(anomalous_scores, bins=30, alpha=0.6, label='Anomalous', color='red')\n",
    "    ax1.axvline(pc_data['threshold'], color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Threshold: {pc_data[\"threshold\"]:.2f}')\n",
    "    ax1.set_title(f'PatchCore - {class_name}', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Anomaly Score')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # PaDiM\n",
    "    ax2 = axes[i, 1]\n",
    "    pd_data = calibration_data['padim'][class_name]\n",
    "    normal_scores = pd_data['scores'][pd_data['labels'] == 0]\n",
    "    anomalous_scores = pd_data['scores'][pd_data['labels'] == 1]\n",
    "    \n",
    "    ax2.hist(normal_scores, bins=30, alpha=0.6, label='Normal', color='blue')\n",
    "    ax2.hist(anomalous_scores, bins=30, alpha=0.6, label='Anomalous', color='red')\n",
    "    ax2.axvline(pd_data['threshold'], color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Threshold: {pd_data[\"threshold\"]:.2f}')\n",
    "    ax2.set_title(f'PaDiM - {class_name}', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Anomaly Score')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.legend()\n",
    "\n",
    "plt.suptitle('Score Distributions & Calibrated Thresholds (Val-clean)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image-Level Evaluation (Test-clean)\n",
    "\n",
    "Now we evaluate on the test set using the calibrated thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "transform = get_clean_transforms(image_size=config.dataset.image_size)\n",
    "\n",
    "# Load models and evaluate on test set\n",
    "all_results = {\n",
    "    'patchcore': {},\n",
    "    'padim': {}\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"IMAGE-LEVEL EVALUATION (Test-clean)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PatchCore on each class\n",
    "print(\"\\n>>> PATCHCORE <<<\")\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n--- {class_name.upper()} ---\")\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_split = splits[class_name]['test']\n",
    "    test_dataset = MVTecDataset.from_split(\n",
    "        test_split,\n",
    "        transform=transform,\n",
    "        phase='test'\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Load model\n",
    "    model = PatchCore(\n",
    "        backbone_layers=config.patchcore.layers,\n",
    "        patch_size=config.patchcore.patch_size,\n",
    "        coreset_ratio=config.patchcore.coreset_sampling_ratio,\n",
    "        n_neighbors=config.patchcore.n_neighbors,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    model.load(PATCHCORE_MODELS_DIR, class_name, domain='clean')\n",
    "    \n",
    "    # Collect predictions\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    all_heatmaps = []\n",
    "    all_masks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels, paths in tqdm(test_loader, desc=f'Testing {class_name}'):\n",
    "            images = images.to(DEVICE)\n",
    "            scores, heatmaps = model.predict(images, return_heatmaps=True)\n",
    "            \n",
    "            all_scores.extend(scores.tolist())\n",
    "            all_labels.extend(labels.numpy().tolist())\n",
    "            \n",
    "            for mask in masks:\n",
    "                if mask is not None:\n",
    "                    all_masks.append(mask.numpy().squeeze())\n",
    "                else:\n",
    "                    all_masks.append(None)\n",
    "            \n",
    "            all_heatmaps.extend([h for h in heatmaps])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    test_scores = np.array(all_scores)\n",
    "    test_labels = np.array(all_labels)\n",
    "    \n",
    "    # Get threshold\n",
    "    threshold = patchcore_calibrator.get_threshold(class_name)\n",
    "    \n",
    "    # Compute image-level metrics\n",
    "    image_metrics = compute_image_metrics(test_labels, test_scores, threshold=threshold)\n",
    "    \n",
    "    # Compute pixel-level metrics\n",
    "    pixel_metrics = compute_pixel_metrics(all_masks, all_heatmaps, compute_pro_metric=True)\n",
    "    \n",
    "    # Store results\n",
    "    all_results['patchcore'][class_name] = {\n",
    "        'threshold': threshold,\n",
    "        'image_level': image_metrics,\n",
    "        'pixel_level': pixel_metrics,\n",
    "        'test_scores': test_scores.tolist(),\n",
    "        'test_labels': test_labels.tolist()\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Threshold: {threshold:.4f}\")\n",
    "    print(f\"  AUROC: {image_metrics['auroc']:.4f}\")\n",
    "    print(f\"  AUPRC: {image_metrics['auprc']:.4f}\")\n",
    "    print(f\"  F1: {image_metrics['f1']:.4f}\")\n",
    "    print(f\"  Pixel AUROC: {pixel_metrics.get('pixel_auroc', 'N/A')}\")\n",
    "    print(f\"  PRO: {pixel_metrics.get('pro', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate PaDiM on each class\n",
    "print(\"\\n>>> PADIM <<<\")\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\n--- {class_name.upper()} ---\")\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_split = splits[class_name]['test']\n",
    "    test_dataset = MVTecDataset.from_split(\n",
    "        test_split,\n",
    "        transform=transform,\n",
    "        phase='test'\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Load model\n",
    "    model = PadimWrapper(\n",
    "        backbone=config.padim.backbone,\n",
    "        layers=config.padim.layers,\n",
    "        n_features=config.padim.n_features,\n",
    "        image_size=config.dataset.image_size,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    model.load(PADIM_MODELS_DIR / f'padim_{class_name}_clean.pt')\n",
    "    \n",
    "    # Collect predictions\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    all_heatmaps = []\n",
    "    all_masks = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks, labels, paths in tqdm(test_loader, desc=f'Testing {class_name}'):\n",
    "            images = images.to(DEVICE)\n",
    "            scores, heatmaps = model.predict(images, return_heatmaps=True)\n",
    "            \n",
    "            if isinstance(scores, np.ndarray):\n",
    "                all_scores.extend(scores.flatten().tolist())\n",
    "            else:\n",
    "                all_scores.append(float(scores))\n",
    "            all_labels.extend(labels.numpy().tolist())\n",
    "            \n",
    "            for mask in masks:\n",
    "                if mask is not None:\n",
    "                    all_masks.append(mask.numpy().squeeze())\n",
    "                else:\n",
    "                    all_masks.append(None)\n",
    "            \n",
    "            if heatmaps is not None:\n",
    "                if heatmaps.ndim == 2:\n",
    "                    all_heatmaps.append(heatmaps)\n",
    "                else:\n",
    "                    all_heatmaps.extend([h for h in heatmaps])\n",
    "    \n",
    "    # Convert to arrays\n",
    "    test_scores = np.array(all_scores)\n",
    "    test_labels = np.array(all_labels)\n",
    "    \n",
    "    # Get threshold\n",
    "    threshold = padim_calibrator.get_threshold(class_name)\n",
    "    \n",
    "    # Compute image-level metrics\n",
    "    image_metrics = compute_image_metrics(test_labels, test_scores, threshold=threshold)\n",
    "    \n",
    "    # Compute pixel-level metrics\n",
    "    pixel_metrics = compute_pixel_metrics(all_masks, all_heatmaps, compute_pro_metric=True)\n",
    "    \n",
    "    # Store results\n",
    "    all_results['padim'][class_name] = {\n",
    "        'threshold': threshold,\n",
    "        'image_level': image_metrics,\n",
    "        'pixel_level': pixel_metrics,\n",
    "        'test_scores': test_scores.tolist(),\n",
    "        'test_labels': test_labels.tolist()\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  Threshold: {threshold:.4f}\")\n",
    "    print(f\"  AUROC: {image_metrics['auroc']:.4f}\")\n",
    "    print(f\"  AUPRC: {image_metrics['auprc']:.4f}\")\n",
    "    print(f\"  F1: {image_metrics['f1']:.4f}\")\n",
    "    print(f\"  Pixel AUROC: {pixel_metrics.get('pixel_auroc', 'N/A')}\")\n",
    "    print(f\"  PRO: {pixel_metrics.get('pro', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Macro-Average Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute macro-averaged metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"MACRO-AVERAGE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for method in ['patchcore', 'padim']:\n",
    "    print(f\"\\n>>> {method.upper()} <<<\")\n",
    "    \n",
    "    # Aggregate image-level\n",
    "    image_metrics_per_class = {\n",
    "        c: all_results[method][c]['image_level'] \n",
    "        for c in CLASSES\n",
    "    }\n",
    "    macro_image = aggregate_metrics(image_metrics_per_class)\n",
    "    \n",
    "    # Aggregate pixel-level\n",
    "    pixel_metrics_per_class = {\n",
    "        c: all_results[method][c]['pixel_level'] \n",
    "        for c in CLASSES\n",
    "    }\n",
    "    macro_pixel = aggregate_pixel_metrics(pixel_metrics_per_class)\n",
    "    \n",
    "    # Store\n",
    "    all_results[method]['macro_average'] = {\n",
    "        'image_level': macro_image,\n",
    "        'pixel_level': macro_pixel\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nImage-Level (macro-avg):\")\n",
    "    print(f\"  AUROC: {macro_image.get('auroc', 'N/A'):.4f}\")\n",
    "    print(f\"  AUPRC: {macro_image.get('auprc', 'N/A'):.4f}\")\n",
    "    print(f\"  F1: {macro_image.get('f1', 'N/A'):.4f}\")\n",
    "    print(f\"  Accuracy: {macro_image.get('accuracy', 'N/A'):.4f}\")\n",
    "    \n",
    "    print(f\"\\nPixel-Level (macro-avg):\")\n",
    "    print(f\"  Pixel AUROC: {macro_pixel.get('pixel_auroc', 'N/A'):.4f}\")\n",
    "    print(f\"  PRO: {macro_pixel.get('pro', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, method in enumerate(['patchcore', 'padim']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for class_name in CLASSES:\n",
    "        scores = np.array(all_results[method][class_name]['test_scores'])\n",
    "        labels = np.array(all_results[method][class_name]['test_labels'])\n",
    "        \n",
    "        fpr, tpr, _ = compute_roc_curve(labels, scores)\n",
    "        auroc = all_results[method][class_name]['image_level']['auroc']\n",
    "        \n",
    "        ax.plot(fpr, tpr, label=f'{class_name} (AUROC={auroc:.3f})', linewidth=2)\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random')\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title(f'{method.upper()} - ROC Curves', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, method in enumerate(['patchcore', 'padim']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for class_name in CLASSES:\n",
    "        scores = np.array(all_results[method][class_name]['test_scores'])\n",
    "        labels = np.array(all_results[method][class_name]['test_labels'])\n",
    "        \n",
    "        precision, recall, _ = compute_pr_curve(labels, scores)\n",
    "        auprc = all_results[method][class_name]['image_level']['auprc']\n",
    "        \n",
    "        ax.plot(recall, precision, label=f'{class_name} (AUPRC={auprc:.3f})', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Recall', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_title(f'{method.upper()} - Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'pr_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(len(CLASSES), 2, figsize=(10, 4*len(CLASSES)))\n",
    "\n",
    "for i, class_name in enumerate(CLASSES):\n",
    "    for j, method in enumerate(['patchcore', 'padim']):\n",
    "        ax = axes[i, j]\n",
    "        \n",
    "        scores = np.array(all_results[method][class_name]['test_scores'])\n",
    "        labels = np.array(all_results[method][class_name]['test_labels'])\n",
    "        threshold = all_results[method][class_name]['threshold']\n",
    "        \n",
    "        cm = compute_confusion_matrix(labels, scores, threshold)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                    xticklabels=['Normal', 'Anomalous'],\n",
    "                    yticklabels=['Normal', 'Anomalous'])\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "        ax.set_title(f'{method.upper()} - {class_name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Confusion Matrices (Test-clean)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Bar Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "metrics_to_plot = ['auroc', 'auprc', 'f1', 'accuracy']\n",
    "x = np.arange(len(CLASSES))\n",
    "width = 0.35\n",
    "\n",
    "for idx, metric in enumerate(['auroc', 'f1']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    pc_values = [all_results['patchcore'][c]['image_level'][metric] for c in CLASSES]\n",
    "    pd_values = [all_results['padim'][c]['image_level'][metric] for c in CLASSES]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, pc_values, width, label='PatchCore', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, pd_values, width, label='PaDiM', color='darkorange')\n",
    "    \n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel(metric.upper())\n",
    "    ax.set_title(f'{metric.upper()} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([c.capitalize() for c in CLASSES])\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.3f}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('PatchCore vs PaDiM - Clean Domain', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(VIZ_DIR / 'method_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final results\n",
    "final_results = {\n",
    "    'metadata': {\n",
    "        'phase': 'Phase 5 - Evaluation Clean Domain',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'seed': 42,\n",
    "        'classes': CLASSES\n",
    "    },\n",
    "    'patchcore': {\n",
    "        class_name: {\n",
    "            'threshold': all_results['patchcore'][class_name]['threshold'],\n",
    "            'image_level': all_results['patchcore'][class_name]['image_level'],\n",
    "            'pixel_level': all_results['patchcore'][class_name]['pixel_level']\n",
    "        }\n",
    "        for class_name in CLASSES\n",
    "    },\n",
    "    'padim': {\n",
    "        class_name: {\n",
    "            'threshold': all_results['padim'][class_name]['threshold'],\n",
    "            'image_level': all_results['padim'][class_name]['image_level'],\n",
    "            'pixel_level': all_results['padim'][class_name]['pixel_level']\n",
    "        }\n",
    "        for class_name in CLASSES\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add macro-averages\n",
    "final_results['patchcore']['macro_average'] = all_results['patchcore']['macro_average']\n",
    "final_results['padim']['macro_average'] = all_results['padim']['macro_average']\n",
    "\n",
    "# Save\n",
    "results_path = RESULTS_DIR / 'clean_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n[OK] Results saved to: {results_path}\")\n",
    "print(f\"[OK] Visualizations saved to: {VIZ_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary dataframe\n",
    "rows = []\n",
    "for method in ['patchcore', 'padim']:\n",
    "    for class_name in CLASSES + ['macro_average']:\n",
    "        if class_name == 'macro_average':\n",
    "            img = all_results[method]['macro_average']['image_level']\n",
    "            pix = all_results[method]['macro_average']['pixel_level']\n",
    "            threshold = '-'\n",
    "        else:\n",
    "            img = all_results[method][class_name]['image_level']\n",
    "            pix = all_results[method][class_name]['pixel_level']\n",
    "            threshold = all_results[method][class_name]['threshold']\n",
    "        \n",
    "        rows.append({\n",
    "            'Method': method.upper(),\n",
    "            'Class': class_name.capitalize(),\n",
    "            'Threshold': threshold if isinstance(threshold, str) else f'{threshold:.2f}',\n",
    "            'AUROC': f\"{img.get('auroc', 0):.4f}\",\n",
    "            'AUPRC': f\"{img.get('auprc', 0):.4f}\",\n",
    "            'F1': f\"{img.get('f1', 0):.4f}\",\n",
    "            'Accuracy': f\"{img.get('accuracy', 0):.4f}\",\n",
    "            'Pixel AUROC': f\"{pix.get('pixel_auroc', 0):.4f}\" if pix.get('pixel_auroc') else '-',\n",
    "            'PRO': f\"{pix.get('pro', 0):.4f}\" if pix.get('pro') else '-'\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FINAL RESULTS SUMMARY - CLEAN DOMAIN\")\n",
    "print(\"=\" * 100)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(RESULTS_DIR / 'clean_results_summary.csv', index=False)\n",
    "print(f\"\\n[OK] Summary saved to: {RESULTS_DIR / 'clean_results_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PHASE 5 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOutputs generated:\")\n",
    "print(f\"  - {RESULTS_DIR / 'clean_results.json'}\")\n",
    "print(f\"  - {RESULTS_DIR / 'clean_results_summary.csv'}\")\n",
    "print(f\"  - {THRESHOLDS_DIR / 'patchcore_clean_thresholds.json'}\")\n",
    "print(f\"  - {THRESHOLDS_DIR / 'padim_clean_thresholds.json'}\")\n",
    "print(f\"  - Visualizations in {VIZ_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
