% ============================================================================
% 5. Results and Discussion
% ============================================================================
\section{Results and Discussion}
\label{sec:results}

This section presents the experimental results across the evaluation scenarios described in Section~\ref{sec:evaluation_scenarios}. 

\subsection{Clean Domain Performance}
\label{sec:clean_results}

Table~\ref{tab:clean_results} reports the detection and localization performance of PatchCore and PaDiM on Test-clean, using thresholds calibrated on Val-clean. Both methods achieve strong baseline performance, validating the experimental setup and implementations.

\begin{table}[ht]
\centering
\small
\caption{Clean domain performance (Test-clean). Best per metric in \textbf{bold}.}
\label{tab:clean_results}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}llccccc@{}}
\toprule
\textbf{Method} & \textbf{Class} & \textbf{AUC} & \textbf{PRC} & \textbf{F1} & \textbf{pAUC} & \textbf{PRO} \\
\midrule
\multirow{4}{*}{\rotatebox{90}{PatchCore}} 
 & Hazel. & \textbf{1.00} & \textbf{1.00} & \textbf{1.00} & \textbf{.988} & \textbf{.855} \\
 & Carpet & \textbf{.973} & \textbf{.991} & \textbf{.951} & \textbf{.988} & \textbf{.836} \\
 & Zipper & \textbf{.982} & \textbf{.994} & \textbf{.949} & \textbf{.986} & \textbf{.833} \\
 & \textit{Avg} & \textit{\textbf{.985}} & \textit{\textbf{.995}} & \textit{\textbf{.966}} & \textit{\textbf{.987}} & \textit{\textbf{.841}} \\
\midrule
\multirow{4}{*}{\rotatebox{90}{PaDiM}}
 & Hazel. & .971 & .975 & .867 & .963 & .797 \\
 & Carpet & .959 & .985 & .923 & .984 & .534 \\
 & Zipper & .862 & .942 & .895 & .972 & .797 \\
 & \textit{Avg} & \textit{.930} & \textit{.967} & \textit{.895} & \textit{.973} & \textit{.709} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Detection Performance.}
PatchCore demonstrates superior image-level detection, achieving near-perfect performance on Hazelnut (AUROC = 1.000, F1 = 1.000) and strong results across all categories (macro-average AUROC = 0.985, F1 = 0.966). PaDiM, while competitive, shows a consistent performance gap, particularly on Zipper (AUROC = 0.862 vs.\ 0.982 for PatchCore). This difference in macro-average AUROC (0.985 vs.\ 0.930) confirms the advantage of PatchCore's non-parametric approach.

\paragraph{Localization Performance.}
Both methods achieve excellent pixel-level AUROC (PatchCore: 0.987, PaDiM: 0.973), indicating strong discrimination between defective and normal pixels. However, the PRO metric reveals a more significant gap: PatchCore achieves 0.841 compared to PaDiM's 0.709. The PRO metric, which penalizes predictions that only partially cover defect regions, highlights PatchCore's more precise spatial localization. Notably, PaDiM's PRO on Carpet (0.534) is substantially lower than other categories, suggesting difficulty with fine-grained texture defects.

\paragraph{Method Comparison.}
The performance gap between PatchCore and PaDiM can be attributed to their fundamentally different modeling approaches:

\begin{itemize}
    \item \textbf{Non-parametric vs.\ Parametric:} PatchCore's memory bank approach (Section~\ref{sec:patchcore}) enables exact matching, while PaDiM's Gaussian assumption (Section~\ref{sec:padim}) may not capture complex feature distributions.
    
    \item \textbf{Density Reweighting:} PatchCore applies density-based reweighting (see Section~\ref{sec:patchcore}) to anomaly scores, reducing the influence of outliers in the memory bank. PaDiM lacks this mechanism, making it more sensitive to covariance estimation errors.
    
    \item \textbf{Trade-offs:} PaDiM offers lower memory footprint and faster inference (no nearest-neighbor search required), making it suitable for resource-constrained deployments. However, for maximum detection accuracy, PatchCore's memory bank approach proves more effective.
\end{itemize}

Figure~\ref{fig:roc_clean} shows the ROC curves for both methods across all categories. PatchCore consistently achieves higher true positive rates at low false positive rates, reflecting its superior ranking ability. The confusion matrices (Figure~\ref{fig:confusion_clean}) reveal that PatchCore's errors are primarily false positives (8 total across all classes), while PaDiM exhibits a higher false negative rate (9 missed anomalies on Carpet alone). Figure~\ref{fig:patchcore_qualitative} provides qualitative evidence of PatchCore's localization quality on Hazelnut, showing accurate heatmap activations on defective regions while producing uniform low-intensity maps for normal samples.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/06_evaluation_clean/roc_curves_clean.png}
    \caption{ROC curves on Test-clean for PatchCore and PaDiM across the three categories. PatchCore achieves near-perfect AUROC on Hazelnut and consistently outperforms PaDiM.}
    \label{fig:roc_clean}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/06_evaluation_clean/confusion_matrices_clean.png}
    \caption{Confusion matrices on Test-clean at calibrated thresholds. PatchCore shows fewer false negatives, while PaDiM exhibits higher miss rates, particularly on Carpet and Hazelnut.}
    \label{fig:confusion_clean}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/patchcore_clean_hazelnut_validation.png}
    \caption{Qualitative results for PatchCore on Hazelnut (Test-clean). Top row: input images with anomaly scores. Bottom row: predicted heatmaps. Normal samples (left) receive low scores with uniform heatmaps, while anomalous samples (right) show high scores and accurate defect localization.}
    \label{fig:patchcore_qualitative}
\end{figure}

\subsection{Impact of Domain Shift}
\label{sec:shift_no_adapt}

We now evaluate the robustness of clean-trained models when tested on the shifted domain (Test-shift) \emph{without any adaptation}. This scenario uses the same models trained on Train-clean and the same thresholds calibrated on Val-clean, isolating the pure impact of distribution shift.

\paragraph{Experimental Setup.}
The domain shift transformations applied to Test-shift include geometric perturbations and photometric changes (see Section~\ref{sec:domain_shift} for details).

Table~\ref{tab:shift_no_adapt} presents the performance degradation compared to the clean domain baseline.

\begin{table}[t]
\centering
\small
\caption{Domain shift degradation (No Adaptation). Macro-average metrics comparing Clean vs.\ Shift. $\Delta$ = Shift $-$ Clean.}
\label{tab:shift_no_adapt}
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}lccc ccc@{}}
\toprule
& \multicolumn{3}{c}{\textbf{AUROC}} & \multicolumn{3}{c}{\textbf{Specificity}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Method} & Clean & Shift & $\Delta$ & Clean & Shift & $\Delta$ \\
\midrule
PatchCore & .985 & .892 & \textcolor{red}{-.093} & .905 & .223 & \textcolor{red}{-.682} \\
PaDiM     & .930 & .684 & \textcolor{red}{-.246} & .869 & .183 & \textcolor{red}{-.686} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Overall Degradation.}
Both methods suffer substantial performance drops when evaluated on shifted data with clean-domain thresholds. PatchCore's macro-average AUROC decreases from 0.985 to 0.892 ($\Delta = -0.093$), while PaDiM experiences a more severe drop from 0.930 to 0.684 ($\Delta = -0.246$). This larger degradation for PaDiM indicates that Gaussian parametric models are more sensitive to distribution shift than memory-bank approaches.

\paragraph{Threshold Invalidity and Specificity Collapse.}
The most striking observation is the \textbf{catastrophic collapse of specificity}, dropping from $\sim$0.90 to $\sim$0.20 for both methods. For Zipper, \emph{both methods achieve exactly 0\% specificity}, meaning every normal image is misclassified as anomalous. This occurs because the domain shift causes anomaly scores to increase systematically across all samples, pushing normal images above the threshold calibrated on clean data.

Figure~\ref{fig:confusion_shifted} visualizes this effect: the bottom-left quadrant (True Negatives) is nearly empty, while false positives dominate. Despite maintaining high recall (PatchCore: 1.000, PaDiM: 0.972), the models become useless for practical deployment due to excessive false alarms.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/shifted_no_adaptation/confusion_matrices_shifted.png}
    \caption{Confusion matrices on Test-shift (No Adaptation). Both methods exhibit near-zero specificity, with Zipper showing complete failure to identify normal samples. The threshold calibrated on Val-clean is no longer valid under domain shift.}
    \label{fig:confusion_shifted}
\end{figure}

\paragraph{The Rotation Sensitivity.}
A good finding is PaDiM's disproportionate sensitivity to domain shift, particularly on Hazelnut (AUROC: 0.971 $\rightarrow$ 0.605, $\Delta = -0.366$) and Zipper (AUROC: 0.862 $\rightarrow$ 0.555, $\Delta = -0.307$). We hypothesize this is due to the $\pm10째$ rotations included in our shift transformations:

\begin{itemize}
    \item \textbf{PaDiM's Position-Specific Modeling:} PaDiM estimates a separate Gaussian distribution for each spatial position $(i,j)$ in the feature map~\cite{defard2020padim}. When an image is rotated, features from position $(i,j)$ in the test image correspond to a different physical location than in training. This spatial misalignment causes features to be compared against incorrect reference distributions, artificially inflating Mahalanobis distances.
    
    \item \textbf{PatchCore's Global Memory Bank:} In contrast, PatchCore's memory bank is position-agnostic: it stores patch embeddings without spatial indexing. During inference, each test patch is matched to its globally nearest neighbor regardless of position. Small rotations merely shuffle which memory bank patches are retrieved, without fundamentally breaking the matching process.
\end{itemize}

This \emph{rotation sensitivity} explains why PaDiM, despite being competitive on clean data, degrades more severely under geometric transformations. The effect is less pronounced on Carpet, a texture category with inherent translation invariance, where PaDiM's degradation is more modest ($\Delta = -0.065$).

\paragraph{Pixel-Level Degradation.}
Pixel-level metrics also degrade under shift, though less dramatically than image-level specificity. PatchCore's Pixel AUROC drops from 0.987 to 0.922 ($\Delta = -0.065$), while PaDiM drops from 0.973 to 0.827 ($\Delta = -0.146$). The PRO metric shows larger degradation (PatchCore: 0.841 $\rightarrow$ 0.645; PaDiM: 0.709 $\rightarrow$ 0.527), indicating that localization precision suffers more than pixel-level discrimination under distribution shift.

\paragraph{Implications.}
These results underscore the critical importance of threshold recalibration and/or model adaptation when deploying anomaly detection systems in environments with even modest acquisition variations. The next sections will examine whether threshold-only adaptation (Section~\ref{sec:threshold_adapt}) or full model retraining (Section~\ref{sec:full_adapt}) can recover the lost performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/shifted_no_adaptation/roc_curves_shifted.png}
    \caption{ROC curves on Test-shift (No Adaptation). PatchCore maintains reasonable ranking ability (macro AUROC = 0.892), while PaDiM's curves approach the diagonal (random classifier) for Hazelnut and Zipper.}
    \label{fig:roc_shifted}
\end{figure}


\subsection{Adaptation Strategies}
\label{sec:adaptation}

Having established the severity of domain shift degradation in Section~\ref{sec:shift_no_adapt}, we now evaluate two adaptation strategies: (1)~\emph{threshold-only adaptation}, which recalibrates the decision threshold on Val-shift without modifying the model, and (2)~\emph{full adaptation}, which retrains the model on Train-shift. Table~\ref{tab:adaptation_comparison} summarizes the macro-averaged results across all scenarios.

\begin{table}[ht]
\centering
\small
\caption{Comparison of adaptation strategies (macro-average). AUROC and Pixel AUROC are threshold-independent; values in \textcolor{gray}{gray} indicate no change from Shift-NoAdapt. Best per-metric in \textbf{bold}.}
\label{tab:adaptation_comparison}
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}l ccc ccc@{}}
\toprule
& \multicolumn{3}{c}{\textbf{PatchCore}} & \multicolumn{3}{c}{\textbf{PaDiM}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Scenario} & AUC & pAUC & PRO & AUC & pAUC & PRO \\
\midrule
Clean (Baseline) & \textbf{.985} & \textbf{.987} & \textbf{.841} & \textbf{.930} & \textbf{.973} & \textbf{.709} \\
Shift-NoAdapt & .892 & .922 & .645 & .684 & .827 & .527 \\
Threshold-Only & \textcolor{gray}{.892} & \textcolor{gray}{.922} & \textcolor{gray}{.645} & \textcolor{gray}{.684} & \textcolor{gray}{.827} & \textcolor{gray}{.527} \\
Full Adaptation & .961 & .966 & .800 & .885 & .922 & .629 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\small
\caption{Threshold-dependent metrics (macro-average). These metrics \textbf{change} with threshold recalibration, unlike Table~\ref{tab:adaptation_comparison}.}
\label{tab:threshold_dependent}
\setlength{\tabcolsep}{2.5pt}
\begin{tabular}{@{}l ccc ccc@{}}
\toprule
& \multicolumn{3}{c}{\textbf{PatchCore}} & \multicolumn{3}{c}{\textbf{PaDiM}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Scenario} & F1 & Spec & Acc & F1 & Spec & Acc \\
\midrule
Clean (Baseline) & \textbf{.966} & \textbf{.905} & \textbf{.952} & \textbf{.895} & \textbf{.869} & \textbf{.870} \\
Shift-NoAdapt & .839 & .223 & .749 & .806 & .183 & .696 \\
Threshold-Only & .850 & .830 & .814 & .800 & .183 & .700 \\
Full Adaptation & .921 & .911 & .896 & .858 & .824 & .819 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Threshold-Only Adaptation}
\label{sec:threshold_adapt}
An important observation from Table~\ref{tab:adaptation_comparison} is that AUROC, Pixel AUROC, and PRO remain \emph{unchanged} between Shift-NoAdapt and Threshold-Only scenarios. This is expected: these metrics are \textbf{threshold-independent}, measuring the model's ranking ability by integrating over all possible thresholds. Since the underlying model and its anomaly scores are identical in both cases, only threshold-dependent metrics (F1, Specificity, Accuracy) can improve, as shown in Table~\ref{tab:threshold_dependent}.

The practical benefit of threshold recalibration is the recovery of specificity (Table~\ref{tab:threshold_dependent}). For PatchCore, specificity increases from 22.3\% to 83.0\% by adjusting the threshold from 227.5 to 441.8 (the optimal F1 point on Val-shift). This eliminates the false positive explosion observed in Section~\ref{sec:shift_no_adapt} at \emph{zero computational cost}: no retraining or feature extraction required.

However, threshold-only adaptation has fundamental limitations: it cannot improve detection accuracy beyond what the existing score distribution allows. For PaDiM, where the score distributions of normal and anomalous samples overlap significantly under shift (particularly for Zipper), recalibration yields minimal benefit (specificity remains at 18.3\%).

\subsubsection{Full Adaptation}
\label{sec:full_adapt}
Full model retraining on Train-shift provides substantial performance recovery. PatchCore's AUROC increases from 0.892 to 0.961 ($\Delta = +6.9$~pp), while PaDiM shows even larger recovery from 0.684 to 0.885 ($\Delta = +20.1$~pp). The improvement mechanism differs between methods:

\begin{itemize}
    \item \textbf{PatchCore:} The memory bank is rebuilt using patch embeddings from shifted normal images. This allows the nearest-neighbor matching to account for the new appearance variations introduced by domain shift transformations. The coreset subsampling~\ref{sec:methodology} ensures efficient coverage of the expanded feature manifold.
    
    \item \textbf{PaDiM:} New Gaussian distributions are estimated at each spatial position using shifted training data. This recalibrates both the mean vectors $\mu_{ij}$ and covariance matrices $\Sigma_{ij}$ to match the shifted domain, reducing the systematic Mahalanobis distance inflation observed without adaptation.
\end{itemize}

The effect of these adaptation strategies on score distributions is visualized in Figure~\ref{fig:score_dist_comparison}, focusing on the Zipper category where the contrast is most evident. Under threshold-only adaptation (top), the score distributions of normal (blue) and anomalous (red) samples remain heavily overlapped for both methods, with PaDiM showing near-complete overlap that explains the persistent 18.3\% specificity. In contrast, full adaptation (bottom) re-separates the distributions by learning new feature representations, restoring clearer boundaries between normal and anomalous samples.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{img/score_distributions_shift_threshold_ZIPPER.png}
    \\[0.5em]
    \includegraphics[width=\linewidth]{img/score_distributions_shift_full_adaptation_ZIPPER.png}
    \caption{Score distributions on Zipper (Val-shift): \textbf{Top:} Threshold-only adaptation: distributions remain overlapped; only the threshold (green dashed) is recalibrated. \textbf{Bottom:} Full adaptation: retraining re-separates normal and anomalous distributions, enabling effective classification.}
    \label{fig:score_dist_comparison}
\end{figure}

Figure~\ref{fig:roc_full_adapt} shows the ROC curves after full adaptation. PatchCore achieves near-perfect detection on Hazelnut (AUROC = 0.998) and strong performance across all categories. The confusion matrices (Figure~\ref{fig:confusion_full_adapt}) confirm the recovery of specificity to practical levels (PatchCore: 91.1\%, PaDiM: 82.4\%). Qualitative comparison (Figure~\ref{fig:qualitative_full_adapt}) further illustrates the difference: PatchCore produces sharper, better-localized heatmaps that closely match ground truth masks, while PaDiM's predictions tend to be more diffuse and occasionally miss subtle defects.


\paragraph{Residual Gap Analysis.}
Despite full retraining, neither method fully recovers to clean domain performance. PatchCore exhibits a residual gap of $-2.4$~pp (0.985 $\rightarrow$ 0.961), while PaDiM shows $-4.5$~pp (0.930 $\rightarrow$ 0.885). We attribute this to three factors:

\begin{enumerate}
    \item \textbf{Increased intra-class variance:} The shift transformations (rotation, color jitter, blur, illumination gradients) expand the feature distribution of normal images, making the boundary between normal and anomalous less distinct.
    
    \item \textbf{Transformation artifacts:} Image interpolation during geometric transformations and border effects introduce subtle patterns that differ from real acquisition variations, potentially confusing the models.
    
    \item \textbf{Data quality:} Synthetically augmented data, while useful for robustness, does not fully replicate the natural variations encountered in real domain shifts.
\end{enumerate}

\paragraph{Method Comparison.}
PatchCore's smaller residual gap ($-2.4$~pp vs.\ $-4.5$~pp for PaDiM) confirms the advantage of non-parametric approaches under domain shift. The memory bank can flexibly accommodate new appearance variations without making distributional assumptions. In contrast, PaDiM's position-specific Gaussian modeling~\cite{defard2020padim} remains partially misaligned even after retraining: the $\pm10째$ rotations in our shift protocol cause features at position $(i,j)$ to originate from physically different locations than in training, violating the positional correspondence assumption.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/shifted_full_adaptation/roc_curves_shift_full_adaptation.png}
    \caption{ROC curves after full adaptation on Train-shift. PatchCore achieves AUROC $\geq 0.936$ on all categories, with near-perfect performance on Hazelnut (0.998). PaDiM shows significant recovery but remains below PatchCore, particularly on Zipper (0.809).}
    \label{fig:roc_full_adapt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/shifted_full_adaptation/confusion_matrices_shift_full_adaptation.png}
    \caption{Confusion matrices after full adaptation. Both methods recover practical specificity levels (PatchCore: 91.1\%, PaDiM: 82.4\%), with PatchCore achieving perfect specificity on Hazelnut (100\%).}
    \label{fig:confusion_full_adapt}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{../outputs/visualizations/shifted_full_adaptation/sample_predictions_hazelnut_shift_full_adaptation.png}
    \caption{Qualitative comparison on Hazelnut (Test-shift) after full adaptation. Columns: original image, ground truth mask, PatchCore heatmap, PaDiM heatmap. PatchCore produces sharper localization, while PaDiM's predictions are more diffuse.}
    \label{fig:qualitative_full_adapt}
\end{figure}


% ============================================================================
% 5.4 Coreset Ratio Analysis
% ============================================================================
\subsection{Coreset Ratio Analysis}
\label{sec:coreset_analysis}

The Greedy Coreset Subsampling mechanism is a core component of PatchCore, reducing the memory bank size while preserving feature space coverage~\cite{roth2022patchcore}. In this study, we evaluate the impact of different coreset ratios (1\%, 5\%, 10\%) on detection performance when training and testing on the \emph{shifted domain}. This setting isolates the effect of memory bank size without domain shift confounding, as both Train-Shift and Test-Shift share the same distribution.

\paragraph{Experimental Results.}
Table~\ref{tab:coreset_metrics} reports the macro-averaged performance across all three categories. Counterintuitively, the smallest coreset ratio (1\%) achieves the \textbf{best detection performance} (AUROC = 0.9625, F1 = 0.9294), while the largest ratio (10\%) yields the \emph{worst} results (AUROC = 0.9560, F1 = 0.9205). This finding contradicts the naive expectation that larger memory banks should provide better coverage of the nominal feature space.

\begin{table}[ht]
\centering
\small
\caption{Coreset ratio ablation on shifted domain (macro-averaged). Best results in \textbf{bold}.}
\label{tab:coreset_metrics}
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Coreset} & \textbf{AUROC} & \textbf{AUPRC} & \textbf{F1} & \textbf{Time (s)} & \textbf{Mem Bank} \\
\midrule
\textbf{1\%} & \textbf{.9625} & \textbf{.9857} & \textbf{.9294} & \textbf{368} & 1,902 \\
5\% & .9605 & .9850 & .9210 & 1,350 & 9,512 \\
10\% & .9560 & .9834 & .9205 & 2,254 & 19,024 \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:coreset_delta} quantifies the relative change compared to the 1\% baseline. Increasing the coreset ratio to 10\% results in a $-0.67$\% AUROC degradation while incurring a $+513$\% increase in training time. The 5\% ratio, used as the default in our full adaptation experiments (Section~\ref{sec:adaptation}), represents a middle ground with marginal performance loss ($-0.20$\% AUROC) but substantially higher computational cost ($+267$\% time).

\begin{table}[ht]
\centering
\small
\caption{Relative performance change vs.\ 1\% coreset baseline.}
\label{tab:coreset_delta}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Coreset} & \textbf{$\Delta$ AUROC} & \textbf{$\Delta$ F1} & \textbf{$\Delta$ Time} \\
\midrule
1\% (baseline) & 0.00\% & 0.00\% & 0.00\% \\
5\% & \textcolor{red}{$-$0.20\%} & \textcolor{red}{$-$0.90\%} & +267\% \\
10\% & \textcolor{red}{$-$0.67\%} & \textcolor{red}{$-$0.96\%} & +513\% \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Per-Class Breakdown.}
The effect of coreset ratio varies across categories. Hazelnut, a rigid object category, shows a slight improvement with larger coresets ($+0.10$\% AUROC from 1\% to 10\%), likely benefiting from increased coverage of pose variations. In contrast, texture categories exhibit performance degradation: Carpet drops by $-0.66$\% and Zipper by $-1.51$\%. These categories, characterized by high intra-class variance in the shifted domain, are more susceptible to the negative effects of larger memory banks.

\paragraph{Interpretation: Noise Filtering Hypothesis.}
We attribute this counterintuitive result to an implicit \emph{noise filtering} effect of aggressive coreset subsampling on domain-shifted data:

\begin{enumerate}
    \item \textbf{Noisy Training Features.} The shifted domain includes images with geometric perturbations (rotation $\pm10째$, scale variations), photometric distortions (color jitter, blur, Gaussian noise), and non-uniform illumination gradients. These transformations expand the feature distribution of normal images, introducing ``peripheral'' patch embeddings that represent unusual but nominally valid appearances.
    
    \item \textbf{Coreset Overfitting.} At 10\% coreset, the Greedy Coreset algorithm~\cite{roth2022patchcore} retains not only core structural features but also these peripheral/noisy patches. The resulting memory bank becomes overly tolerant of variance, causing anomalies with similar distortion patterns to receive lower scores (closer neighbors in the polluted feature space).
    
    \item \textbf{Filtering Effect.} At 1\% coreset, the minimax facility location objective selects only the most \emph{representative} prototypes, discarding less central patches. This acts as an implicit noise filter: transformation artifacts are pruned, leaving a cleaner representation of structural normality with sharper decision boundaries.
\end{enumerate}

This interpretation aligns with observations in the original PatchCore paper, where Roth~\etal~\cite{roth2022patchcore} note that ``for certain subsampling intervals (between around 50\% and 10\%), we even find joint performance over anomaly detection and localization to partly \emph{increase} as compared to non-subsampled PatchCore.''

\paragraph{Comparison with Original Paper Results.}
The original PatchCore paper~\cite{roth2022patchcore} evaluates coreset ratios exclusively on the \emph{clean} MVTec AD benchmark, reporting nearly equivalent performance: PatchCore-25\% achieves 99.1\% AUROC, PatchCore-10\% achieves 99.0\%, and PatchCore-1\% achieves 99.0\%. The authors advocate for aggressive 1\% subsampling, demonstrating that it reduces inference time to $<$200ms while maintaining state-of-the-art detection. Notably, the original evaluation uses no data augmentation~\cite{roth2022patchcore}, meaning all models are trained and tested on well-aligned, controlled imagery.

Our experiments extend this analysis to a \emph{synthetically shifted} domain, applying controlled transformations (rotation $\pm10째$, color jitter, blur, illumination gradients) to simulate acquisition variability. Under these conditions, the 1\% coreset \emph{outperforms} larger ratios, contrasting with the clean-domain equivalence. This suggests that the optimal coreset ratio is \textbf{domain-dependent}: on clean, compact feature distributions, all ratios perform similarly; under domain shift, aggressive subsampling acts as an implicit noise filter.

We note that our synthetic shifts represent \emph{mild, controlled} perturbations compared to real-world distribution shifts. Recent work on MVTec AD 2~\cite{hecklerkram2025mvtecad2} introduces significantly more challenging scenarios including transparent objects, dark-field illumination, and explicit lighting-change test sets, where PatchCore achieves only 53.8\% AU-PRO versus $\sim$93\% on the original benchmark. Our ablation study provides a preliminary investigation into coreset sensitivity under distribution shift, but the behavior on truly complex, real-world domain shifts remains an open question for future work.

\paragraph{Justification for 5\% Default.}
Throughout our main experiments (Sections~\ref{sec:clean_results}--\ref{sec:adaptation}), we adopted a 5\% coreset ratio as a \emph{conservative middle ground}. This choice balances three considerations:

\begin{enumerate}
    \item \textbf{Literature alignment:} Many implementations (e.g., Anomalib) default to 10\% for robustness; 5\% represents a more aggressive but still conservative setting.
    
    \item \textbf{Uncertainty at design time:} Before conducting the ablation study, we lacked evidence that 1\% would outperform larger ratios under domain shift. The 5\% choice hedged against potential underfitting.
    
    \item \textbf{Reproducibility:} A single fixed ratio across all experiments ensures fair comparison between clean and shifted domain scenarios.
\end{enumerate}

In retrospect, our ablation validates that 1\% would have been optimal for the shifted domain, while confirming that the paper's original claim that 1\% is sufficient for clean domains extends robustly to our setup.

\paragraph{Efficiency Trade-off.}
Figure~\ref{fig:coreset_tradeoff} illustrates the performance-efficiency trade-off. The 1\% coreset achieves both the highest AUROC and the fastest training time, dominating the Pareto frontier. Training time increases dramatically with coreset size: Hazelnut requires 563s at 1\% vs.\ 3,338s at 10\% ($+493$\%), while Zipper shows the largest relative increase ($+664$\%). Memory bank size scales linearly (1,902 $\rightarrow$ 19,024 patches), directly impacting inference-time nearest-neighbor search complexity.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{../outputs/visualizations/coreset/performance_efficiency_tradeoff.png}
    \caption{Performance vs.\ efficiency trade-off for different coreset ratios on the shifted domain. The 1\% coreset (leftmost point) dominates the Pareto frontier, achieving highest AUROC with lowest training time.}
    \label{fig:coreset_tradeoff}
\end{figure}

\paragraph{Practical Implications.}
These findings have direct implications for deploying PatchCore in non-ideal environments:

\begin{itemize}
    \item \textbf{Aggressive subsampling is beneficial under domain shift.} When training data exhibits high variance (common in real-world industrial settings with lighting/positioning variations), smaller coreset ratios prevent ``pollution'' of the nominal feature space.
    
    \item \textbf{1\% coreset is recommended for shifted domains.} It provides the best detection performance, fastest training, and lowest memory footprint. The 5\% ratio offers negligible benefit at $3.7\times$ the computational cost.
    
    \item \textbf{Larger coresets may help for rigid objects.} Categories with low intra-class variance (e.g., Hazelnut) can benefit from increased coverage, but the effect is marginal ($+0.10$\% AUROC).
\end{itemize}


% ============================================================================
% 5.5 Unified Model Analysis
% ============================================================================
\subsection{Model Unified Analysis}
\label{sec:unified_model}

The preceding sections evaluate \emph{per-class models}, where each category has its own dedicated model trained exclusively on that class's normal data. We now investigate the \textbf{Model-Unified} setting (see Section~\ref{sec:global_model} and Section~\ref{par:model_unified}), where a single model is trained on pooled normal data from all three categories (Hazelnut, Carpet, Zipper). At inference time, class identity is assumed known and \emph{per-class thresholds} are applied. This setting is valuable for industrial deployments where maintaining separate models for each product variant is impractical.


\paragraph{Quantitative Comparison.}
Table~\ref{tab:global_results} reports the macro-averaged performance of the unified models across all three categories. PatchCore maintains strong detection (AUROC = 0.984) and localization (Pixel-AUROC = 0.981, PRO = 0.801) capabilities even when trained on heterogeneous data. PaDiM shows competitive detection performance (AUROC = 0.934), but its localization remains weak: the PRO of 0.672 is consistent with its per-class performance (0.709 in Table~\ref{tab:clean_results}), confirming that PaDiM's difficulty with precise defect localization is intrinsic to its Gaussian modeling approach rather than a consequence of the unified setting.

\begin{table}[H]
\centering
\small
\caption{Unified model performance (macro-averaged across all classes).}
\label{tab:global_results}
\setlength{\tabcolsep}{8pt}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Method} & \textbf{AUROC} & \textbf{Pixel-AUROC} & \textbf{PRO} \\
\midrule
PatchCore & .984 & .981 & .801 \\
PaDiM & .934 & .951 & .672 \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:gap_analysis} quantifies the AUROC gap between per-class and unified models. PatchCore exhibits negligible degradation (max gap = +0.5\% on Carpet), confirming that its memory bank effectively handles multi-class training data. PaDiM shows larger variance: a 1.5\% drop on Carpet but a 2.1\% \emph{improvement} on Zipper, likely due to the larger training set providing better feature coverage for this challenging category.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{../outputs/visualizations/global/global_vs_perclass_gap.png}
    \caption{AUROC gap between per-class and unified models. Positive = per-class better, negative = unified better. PatchCore shows negligible gaps ($<$0.5\%), while PaDiM exhibits larger variance.}
    \label{fig:gap_analysis}
\end{figure}

\paragraph{The ``Identical Shortcut'' Problem.}
While per-class thresholds mitigate most issues in the Model-Unified setting, a critical pathology emerges when we analyze \emph{cross-class} behavior. As discussed in Section~\ref{par:identical_shortcut}, You~\etal~\cite{you2022uniad} identified the ``identical shortcut'' problem: in unified settings, models may learn to discriminate between classes rather than between normal and anomalous samples within each class. To test this, we apply each class's calibrated threshold to normal images from \emph{other} classes and measure the false positive rate (FPR).

Figure~\ref{fig:shortcut_confusion} shows the cross-class confusion matrices. PatchCore achieves \textbf{0\% FPR across all class pairs}, demonstrating that its memory bank effectively separates the feature manifolds of different categories. In contrast, PaDiM exhibits a catastrophic failure: when the Zipper threshold ($\tau = 18.05$) is applied to Hazelnut normal images, \textbf{55.1\% are misclassified as anomalous} (172/312 images). This occurs because Zipper's lower threshold, optimized for its own score distribution, incorrectly flags Hazelnut's higher natural scores.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.49\linewidth]{../outputs/visualizations/global/identical_shortcut_confusion.png}
    \includegraphics[width=0.49\linewidth]{../outputs/visualizations/global/identical_shortcut_confusion_padim.png}
    \caption{Cross-class confusion matrices showing FPR (\%) when applying one class's threshold to another class's normal images. \textbf{Left:} PatchCore achieves 0\% FPR everywhere. \textbf{Right:} PaDiM shows severe failure when Zipper's threshold is applied to Hazelnut (55.1\%) and Carpet (5.4\%).}
    \label{fig:shortcut_confusion}
\end{figure}

\paragraph{Why PatchCore is More Robust.}

\begin{itemize}
    \item \textbf{PatchCore's Memory Bank:} Stores actual patch embeddings from each class, which naturally occupy distinct regions in feature space. The nearest-neighbor matching ensures that test patches are compared only against visually similar prototypes. When Hazelnut patches query the global memory bank, they find Hazelnut-like neighbors regardless of which threshold is applied: the anomaly score reflects true visual deviation.
    
    \item \textbf{PaDiM's Shared Gaussian:} Estimates a single Gaussian distribution per spatial position over \emph{all} classes. This creates overlapping distributions where Hazelnut's feature vectors, which tend to have higher Mahalanobis distances due to their distinct appearance (nuts vs.\ fabric), naturally produce higher baseline scores. Zipper's threshold, calibrated for lower scores, incorrectly triggers.
\end{itemize}

\paragraph{Practical Implications.}
These findings have direct implications for industrial deployment:

\begin{enumerate}
    \item \textbf{Per-class thresholds are essential:} Even with a unified model, applying a single global threshold is not advisable. Our Model-Unified results confirm that per-class threshold calibration effectively compensates for score distribution differences.
    
    \item \textbf{PatchCore is preferred for multi-class scenarios:} Its memory bank approach inherently preserves class boundaries, avoiding the identical shortcut problem. PaDiM's Gaussian modeling, while computationally efficient, requires careful attention to prevent cross-class interference.
    
    \item \textbf{Scalability consideration:} A single unified PatchCore model with per-class thresholds achieves comparable performance to per-class models (AUROC gap $<$0.5\%) while reducing deployment complexity from 3 models to 1.
\end{enumerate}

