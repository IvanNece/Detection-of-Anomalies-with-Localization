{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4755fd27",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/PatchCore/notebooks/04_patchcore_clean.ipynb)\n",
    "\n",
    "# PatchCore Clean Domain Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cae058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository on PatchCore branch\n",
    "print(\"Cloning repository (branch: PatchCore)...\")\n",
    "repo_dir = '/content/Detection-of-Anomalies-with-Localization'\n",
    "\n",
    "# Remove if exists\n",
    "if os.path.exists(repo_dir):\n",
    "    print(\"Removing existing repository...\")\n",
    "    !rm -rf {repo_dir}\n",
    "\n",
    "# Clone with specific branch\n",
    "!git clone -b PatchCore https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git {repo_dir}\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(repo_dir)\n",
    "\n",
    "# Dataset location (direct from Drive, no duplication)\n",
    "DATASET_PATH = Path('/content/drive/MyDrive/mvtec_ad')\n",
    "\n",
    "# Output directories\n",
    "MODELS_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'outputs' / 'results'\n",
    "VIZ_DIR = PROJECT_ROOT / 'outputs' / 'visualizations'\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify dataset exists\n",
    "if not DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {DATASET_PATH}\\n\"\n",
    "        f\"Please ensure mvtec_ad folder is in your Google Drive root.\"\n",
    "    )\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Project:  {PROJECT_ROOT}\")\n",
    "print(f\"Dataset:  {DATASET_PATH}\")\n",
    "print(f\"Branch:   PatchCore\")\n",
    "print(f\"Models:   {MODELS_DIR}\")\n",
    "print(f\"Results:  {RESULTS_DIR}\")\n",
    "print(f\"Viz:      {VIZ_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe51d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSTALL FAISS - MUST BE DONE BEFORE IMPORTS!\n",
    "# ============================================================\n",
    "# FAISS speeds up coreset sampling by 10-100x\n",
    "\n",
    "!pip install faiss-cpu --quiet\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import faiss\n",
    "    print(\"✓ FAISS installed successfully!\")\n",
    "    print(f\" FAISS version: {faiss.__version__}\")\n",
    "    print(\"\\n  IMPORTANT: Now you can proceed with imports\")\n",
    "except ImportError:\n",
    "    print(\" FAISS installation failed, will use numpy fallback (VERY SLOW)\")\n",
    "    print(\"   Try running: !pip install faiss-cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c1277",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import load_config\n",
    "from src.utils.paths import ProjectPaths\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.data.transforms import get_clean_transforms\n",
    "from src.data.splitter import load_splits\n",
    "from src.models.patchcore import PatchCore\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(PROJECT_ROOT / 'configs' / 'experiment_config.yaml')\n",
    "paths = ProjectPaths(PROJECT_ROOT)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18717f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATCH: Fix dataset __getitem__ to handle errors properly\n",
    "# This is needed because the GitHub code doesn't have error handling\n",
    "from PIL import Image\n",
    "\n",
    "def patched_getitem(self, idx):\n",
    "    \"\"\"Patched version with error handling.\"\"\"\n",
    "    image_path = self.images[idx]\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load image {image_path}: {e}\")\n",
    "    \n",
    "    mask_path = self.masks[idx]\n",
    "    mask = None\n",
    "    \n",
    "    if mask_path is not None:\n",
    "        try:\n",
    "            mask = Image.open(mask_path).convert('L')\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load mask {mask_path}: {e}\")\n",
    "    \n",
    "    if self.transform is not None:\n",
    "        try:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Transform failed for {image_path}: {e}\")\n",
    "    \n",
    "    if image is None:\n",
    "        raise RuntimeError(f\"Transform returned None for image {image_path}\")\n",
    "    \n",
    "    label = self.labels[idx]\n",
    "    return image, mask, label, image_path\n",
    "\n",
    "# Apply patch\n",
    "MVTecDataset.__getitem__ = patched_getitem\n",
    "print(\"✓ Dataset patched with error handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c672f03",
   "metadata": {},
   "source": [
    "## Load Clean Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df532722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean splits\n",
    "splits = load_splits(paths.get_split_path('clean'))\n",
    "\n",
    "print(\"Dataset splits loaded:\")\n",
    "print(\"=\" * 70)\n",
    "for class_name in config.dataset.classes:\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_data = splits[class_name][split_name]\n",
    "        n_normal = sum(1 for l in split_data['labels'] if l == 0)\n",
    "        n_anomalous = sum(1 for l in split_data['labels'] if l == 1)\n",
    "        print(f\"  {split_name:5s}: {len(split_data['labels']):4d} images \"\n",
    "              f\"({n_normal:3d} normal, {n_anomalous:3d} anomalous)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3f8a6",
   "metadata": {},
   "source": [
    "## Train PatchCore Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bad463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transform = get_clean_transforms(image_size=config.dataset.image_size)\n",
    "\n",
    "# Custom collate function per debug\n",
    "def debug_collate_fn(batch):\n",
    "    \"\"\"Custom collate con debug per trovare None.\"\"\"\n",
    "    for i, item in enumerate(batch):\n",
    "        if item is None:\n",
    "            raise RuntimeError(f\"Batch item {i} is None!\")\n",
    "        if not isinstance(item, tuple) or len(item) != 4:\n",
    "            raise RuntimeError(f\"Batch item {i} has wrong format: {type(item)}\")\n",
    "        img, mask, label, path = item\n",
    "        if img is None:\n",
    "            raise RuntimeError(f\"Image is None for path: {path}\")\n",
    "    \n",
    "    # Default collate\n",
    "    import torch\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    masks = [item[1] for item in batch]\n",
    "    labels = torch.tensor([item[2] for item in batch])\n",
    "    paths = [item[3] for item in batch]\n",
    "    return images, masks, labels, paths\n",
    "\n",
    "# Hyperparameters\n",
    "CORESET_RATIO = config.patchcore.coreset_sampling_ratio\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 0  # Set to 0 per Colab\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Coreset ratio: {CORESET_RATIO*100:.1f}%\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Num workers: {NUM_WORKERS}\")\n",
    "print(f\"Image size: {config.dataset.image_size}\")\n",
    "print(f\"Backbone layers: {config.patchcore.layers}\")\n",
    "print(f\"Patch size: {config.patchcore.patch_size}\")\n",
    "print(f\"N neighbors: {config.patchcore.n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each class\n",
    "trained_models = {}\n",
    "training_stats = {}\n",
    "\n",
    "for class_name in config.dataset.classes:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Training PatchCore for: {class_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create train dataset (only normal images)\n",
    "    train_split = splits[class_name]['train']\n",
    "    train_dataset = MVTecDataset.from_split(\n",
    "        train_split,\n",
    "        transform=transform,\n",
    "        phase='train'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,  # Disabilitato per evitare warning\n",
    "        collate_fn=debug_collate_fn  # Custom collate per debug\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain dataset: {len(train_dataset)} images\")\n",
    "    \n",
    "    # Initialize PatchCore\n",
    "    model = PatchCore(\n",
    "        backbone_layers=config.patchcore.layers,\n",
    "        patch_size=config.patchcore.patch_size,\n",
    "        coreset_ratio=CORESET_RATIO,\n",
    "        n_neighbors=config.patchcore.n_neighbors,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    start_time = time.time()\n",
    "    model.fit(train_loader, apply_coreset=True)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save model\n",
    "    model.save(paths.MODELS, class_name, domain='clean')\n",
    "    \n",
    "    # Store statistics\n",
    "    training_stats[class_name] = {\n",
    "        'n_train_images': len(train_dataset),\n",
    "        'memory_bank_size': len(model.memory_bank),\n",
    "        'training_time': training_time,\n",
    "        'spatial_dims': model.spatial_dims\n",
    "    }\n",
    "    \n",
    "    trained_models[class_name] = model\n",
    "    \n",
    "    print(f\"\\nCompleted {class_name.upper()}:\")\n",
    "    print(f\"  Memory bank size: {len(model.memory_bank)}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"  Spatial dims: {model.spatial_dims}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All PatchCore models trained successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e590c4",
   "metadata": {},
   "source": [
    "## Training Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10205061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "stats_df = pd.DataFrame(training_stats).T\n",
    "stats_df['training_time'] = stats_df['training_time'].apply(lambda x: f\"{x:.2f}s\")\n",
    "\n",
    "print(\"\\nTraining Statistics Summary:\")\n",
    "print(stats_df.to_string())\n",
    "\n",
    "# Save statistics\n",
    "stats_output_path = paths.RESULTS / 'patchcore_clean_training_stats.csv'\n",
    "stats_df.to_csv(stats_output_path)\n",
    "print(f\"\\nStatistics saved to: {stats_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160adb0",
   "metadata": {},
   "source": [
    "## Quick Validation Test\n",
    "\n",
    "Test predictions on a few validation images to verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(img_tensor, mean, std):\n",
    "    \"\"\"Denormalize image for visualization.\"\"\"\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    img = img * np.array(std) + np.array(mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# Test on validation set\n",
    "class_name = 'hazelnut'  # Change to test different classes\n",
    "model = trained_models[class_name]\n",
    "\n",
    "# Load validation data\n",
    "val_split = splits[class_name]['val']\n",
    "val_dataset = MVTecDataset.from_split(\n",
    "    val_split,\n",
    "    transform=transform,\n",
    "    phase='val'\n",
    ")\n",
    "\n",
    "# Select images: 4 normal + 4 anomalous\n",
    "normal_idx = [i for i, l in enumerate(val_dataset.labels) if l == 0][:4]\n",
    "anomalous_idx = [i for i, l in enumerate(val_dataset.labels) if l == 1][:4]\n",
    "test_indices = normal_idx + anomalous_idx\n",
    "\n",
    "# Get predictions\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for idx in test_indices:\n",
    "    img, mask, label, _ = val_dataset[idx]\n",
    "    test_images.append(img)\n",
    "    test_labels.append(label)\n",
    "\n",
    "test_batch = torch.stack(test_images)\n",
    "scores, heatmaps = model.predict(test_batch, return_heatmaps=True)\n",
    "\n",
    "print(f\"\\nPrediction scores for {class_name}:\")\n",
    "print(f\"Normal images (0-3): {scores[:4]}\")\n",
    "print(f\"Anomalous images (4-7): {scores[4:]}\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(f\"  Normal - mean: {scores[:4].mean():.3f}, std: {scores[:4].std():.3f}\")\n",
    "print(f\"  Anomalous - mean: {scores[4:].mean():.3f}, std: {scores[4:].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6f8f8",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b890448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "mean = config.dataset.normalize.mean\n",
    "std = config.dataset.normalize.std\n",
    "\n",
    "# Calculate global vmax for consistent heatmap scaling\n",
    "vmax_global = np.max([h.max() for h in heatmaps])\n",
    "print(f\"Global heatmap vmax: {vmax_global:.3f}\")\n",
    "\n",
    "for i, (img, score, heatmap, label) in enumerate(\n",
    "    zip(test_images, scores, heatmaps, test_labels)\n",
    "):\n",
    "    # Denormalize image\n",
    "    img_np = denormalize_image(img, mean, std)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, i].imshow(img_np)\n",
    "    axes[0, i].set_title(\n",
    "        f\"{'Normal' if label == 0 else 'Anomalous'}\\nScore: {score:.3f}\",\n",
    "        fontsize=10\n",
    "    )\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Heatmap overlay with global normalization\n",
    "    axes[1, i].imshow(img_np)\n",
    "    axes[1, i].imshow(heatmap, alpha=0.5, cmap='jet', vmin=0, vmax=vmax_global)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Heatmap', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.suptitle(f'PatchCore Predictions - {class_name.upper()}', fontsize=16, y=1.02)plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a0e16",
   "metadata": {},
   "source": [
    "## Save Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee79a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save visualization for each class\n",
    "for class_name in config.dataset.classes:\n",
    "    model = trained_models[class_name]\n",
    "    val_split = splits[class_name]['val']\n",
    "    val_dataset = MVTecDataset.from_split(\n",
    "        val_split, transform=transform, phase='val'\n",
    "    )\n",
    "    \n",
    "    # Select images\n",
    "    normal_idx = [i for i, l in enumerate(val_dataset.labels) if l == 0][:4]\n",
    "    anomalous_idx = [i for i, l in enumerate(val_dataset.labels) if l == 1][:4]\n",
    "    test_indices = normal_idx + anomalous_idx\n",
    "    \n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for idx in test_indices:\n",
    "        img, _, label, _ = val_dataset[idx]\n",
    "        test_images.append(img)\n",
    "        test_labels.append(label)\n",
    "    \n",
    "    test_batch = torch.stack(test_images)\n",
    "    scores, heatmaps = model.predict(test_batch, return_heatmaps=True)\n",
    "    \n",
    "    # Calculate global vmax for consistent heatmap scaling\n",
    "    vmax_global = np.max([h.max() for h in heatmaps])\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "    \n",
    "    for i, (img, score, heatmap, label) in enumerate(\n",
    "        zip(test_images, scores, heatmaps, test_labels)\n",
    "    ):\n",
    "        img_np = denormalize_image(img, mean, std)\n",
    "        \n",
    "        axes[0, i].imshow(img_np)\n",
    "        axes[0, i].set_title(\n",
    "            f\"{'Normal' if label == 0 else 'Anomalous'}\\nScore: {score:.3f}\",\n",
    "            fontsize=10\n",
    "        )\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(img_np)\n",
    "        axes[1, i].imshow(heatmap, alpha=0.5, cmap='jet', vmin=0, vmax=vmax_global)\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Heatmap', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f'PatchCore Predictions - {class_name.upper()}', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = paths.VISUALIZATIONS / f'patchcore_clean_{class_name}_validation.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    print(f\"Saved visualization: {save_path}\")print(\"\\nAll visualizations saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fcc5a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Phase 3 completed successfully:\n",
    "- Trained PatchCore models for all classes\n",
    "- Memory banks saved in `outputs/models/`\n",
    "- Validation predictions show clear separation between normal and anomalous\n",
    "- Ready for Phase 5: Threshold calibration and full evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b2027",
   "metadata": {},
   "source": [
    "## Save Complete Training Metadata\n",
    "\n",
    "Save comprehensive metadata for reproducibility and downstream evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b487bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Collect validation scores for threshold calibration\n",
    "validation_predictions = {}\n",
    "\n",
    "for class_name in config.dataset.classes:\n",
    "    model = trained_models[class_name]\n",
    "    \n",
    "    # Load validation set\n",
    "    val_split = splits[class_name]['val']\n",
    "    val_dataset = MVTecDataset.from_split(\n",
    "        val_split, transform=transform, phase='val'\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=False,\n",
    "        collate_fn=debug_collate_fn  # Use custom collate to handle None masks\n",
    "    )\n",
    "    \n",
    "    # Get predictions for all validation images\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    \n",
    "    for images, masks, labels, image_paths in val_loader:\n",
    "        scores, _ = model.predict(images, return_heatmaps=False)\n",
    "        \n",
    "        all_scores.extend(scores.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_paths.extend(image_paths)\n",
    "    \n",
    "    validation_predictions[class_name] = {\n",
    "        'scores': all_scores,\n",
    "        'labels': all_labels,\n",
    "        'image_paths': all_paths,\n",
    "        'n_normal': sum(1 for l in all_labels if l == 0),\n",
    "        'n_anomalous': sum(1 for l in all_labels if l == 1),\n",
    "        'score_stats': {\n",
    "            'normal_mean': float(np.mean([s for s, l in zip(all_scores, all_labels) if l == 0])),\n",
    "            'normal_std': float(np.std([s for s, l in zip(all_scores, all_labels) if l == 0])),\n",
    "            'anomalous_mean': float(np.mean([s for s, l in zip(all_scores, all_labels) if l == 1])),\n",
    "            'anomalous_std': float(np.std([s for s, l in zip(all_scores, all_labels) if l == 1]))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"{class_name}: Normal scores: {validation_predictions[class_name]['score_stats']['normal_mean']:.3f} +/- \"\n",
    "          f\"{validation_predictions[class_name]['score_stats']['normal_std']:.3f}, \"\n",
    "          f\"Anomalous: {validation_predictions[class_name]['score_stats']['anomalous_mean']:.3f} +/- \"\n",
    "          f\"{validation_predictions[class_name]['score_stats']['anomalous_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f110aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure paths object is not corrupted (safety check)\n",
    "if not hasattr(paths, 'MODELS'):\n",
    "    paths = ProjectPaths(PROJECT_ROOT)\n",
    "\n",
    "# Create comprehensive metadata\n",
    "metadata = {\n",
    "    'notebook': '04_patchcore_clean.ipynb',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'phase': 'Phase 3 - PatchCore Clean Domain',\n",
    "    'seed': 42,\n",
    "    'device': device,\n",
    "    \n",
    "    'configuration': {\n",
    "        'backbone': config.patchcore.backbone,\n",
    "        'layers': config.patchcore.layers,\n",
    "        'patch_size': config.patchcore.patch_size,\n",
    "        'coreset_ratio': CORESET_RATIO,\n",
    "        'n_neighbors': config.patchcore.n_neighbors,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'image_size': config.dataset.image_size,\n",
    "        'normalize_mean': config.dataset.normalize.mean,\n",
    "        'normalize_std': config.dataset.normalize.std\n",
    "    },\n",
    "    \n",
    "    'training_statistics': {},\n",
    "    'validation_predictions': validation_predictions,\n",
    "    \n",
    "    'models_saved': {},\n",
    "    'splits_used': 'clean_splits.json'\n",
    "}\n",
    "\n",
    "# Add training statistics per class\n",
    "for class_name in config.dataset.classes:\n",
    "    stats = training_stats[class_name]\n",
    "    metadata['training_statistics'][class_name] = {\n",
    "        'n_train_images': stats['n_train_images'],\n",
    "        'memory_bank_size': stats['memory_bank_size'],\n",
    "        'training_time_seconds': stats['training_time'],\n",
    "        'spatial_dims': stats['spatial_dims'],\n",
    "        'compression_ratio': stats['memory_bank_size'] / (stats['n_train_images'] * stats['spatial_dims'][0] * stats['spatial_dims'][1]) if stats['n_train_images'] > 0 else 0\n",
    "    }\n",
    "    \n",
    "    metadata['models_saved'][class_name] = {\n",
    "        'memory_bank': f\"patchcore_{class_name}_clean.npy\",\n",
    "        'config': f\"patchcore_{class_name}_clean_config.pth\",\n",
    "        'location': str(paths.MODELS)\n",
    "    }\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = paths.RESULTS / 'patchcore_clean_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetadata saved to: {metadata_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Classes trained: {len(config.dataset.classes)}\")\n",
    "print(f\"  Total memory bank samples: {sum(stats['memory_bank_size'] for stats in training_stats.values())}\")\n",
    "print(f\"  Average training time: {np.mean([stats['training_time'] for stats in training_stats.values()]):.2f}s\")\n",
    "print(f\"  Models saved in: {paths.MODELS}\")\n",
    "print(f\"  Results saved in: {paths.RESULTS}\")\n",
    "print(f\"  Visualizations saved in: {paths.VISUALIZATIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c54f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3 COMPLETED - PATCHCORE CLEAN DOMAIN TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "for class_name in config.dataset.classes:\n",
    "    stats = training_stats[class_name]\n",
    "    val_stats = validation_predictions[class_name]['score_stats']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Class': class_name.upper(),\n",
    "        'Train Images': stats['n_train_images'],\n",
    "        'Memory Bank': stats['memory_bank_size'],\n",
    "        'Train Time (s)': f\"{stats['training_time']:.2f}\",\n",
    "        'Normal Score': f\"{val_stats['normal_mean']:.3f}±{val_stats['normal_std']:.3f}\",\n",
    "        'Anomalous Score': f\"{val_stats['anomalous_mean']:.3f}±{val_stats['anomalous_std']:.3f}\",\n",
    "        'Separation': f\"{val_stats['anomalous_mean'] - val_stats['normal_mean']:.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Next Steps:\")\n",
    "print(\"  1. Phase 4: Train PaDiM baseline (notebook 05)\")\n",
    "print(\"  2. Phase 5: Threshold calibration and evaluation (notebook 06)\")\n",
    "print(\"  3. Use validation scores for optimal threshold selection\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07fd2a",
   "metadata": {},
   "source": [
    "## Files Generated\n",
    "\n",
    "This notebook generated the following files:\n",
    "\n",
    "**Models** (`outputs/models/`):\n",
    "- `patchcore_hazelnut_clean.npy` - Memory bank\n",
    "- `patchcore_hazelnut_clean_config.pth` - Model config\n",
    "- `patchcore_carpet_clean.npy` - Memory bank\n",
    "- `patchcore_carpet_clean_config.pth` - Model config\n",
    "- `patchcore_zipper_clean.npy` - Memory bank\n",
    "- `patchcore_zipper_clean_config.pth` - Model config\n",
    "\n",
    "**Results** (`outputs/results/`):\n",
    "- `patchcore_clean_training_stats.csv` - Training statistics\n",
    "- `patchcore_clean_metadata.json` - Complete metadata with validation scores\n",
    "\n",
    "**Visualizations** (`outputs/visualizations/`):\n",
    "- `patchcore_clean_hazelnut_validation.png` - Validation predictions\n",
    "- `patchcore_clean_carpet_validation.png` - Validation predictions\n",
    "- `patchcore_clean_zipper_validation.png` - Validation predictions\n",
    "\n",
    "All files are ready for Phase 5 (evaluation) and Phase 6-7 (domain shift experiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984e0aa",
   "metadata": {},
   "source": [
    "## Save Outputs to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da53f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory on Drive\n",
    "DRIVE_OUTPUT = Path('/content/drive/MyDrive/anomaly_detection_project/04_patchcore_clean_outputs')\n",
    "DRIVE_OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Saving outputs to Google Drive...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Copy model files (memory banks + configs)\n",
    "models_saved = []\n",
    "for class_name in config.dataset.classes:\n",
    "    memory_bank = paths.MODELS / f\"patchcore_{class_name}_clean.npy\"\n",
    "    model_config = paths.MODELS / f\"patchcore_{class_name}_clean_config.pth\"\n",
    "    \n",
    "    if memory_bank.exists():\n",
    "        shutil.copy2(memory_bank, DRIVE_OUTPUT / memory_bank.name)\n",
    "        models_saved.append(memory_bank.name)\n",
    "        print(f\"✓ Saved: {memory_bank.name}\")\n",
    "    \n",
    "    if model_config.exists():\n",
    "        shutil.copy2(model_config, DRIVE_OUTPUT / model_config.name)\n",
    "        models_saved.append(model_config.name)\n",
    "        print(f\"✓ Saved: {model_config.name}\")\n",
    "\n",
    "# 2. Copy results files\n",
    "results_files = [\n",
    "    (paths.RESULTS / 'patchcore_clean_training_stats.csv', 'patchcore_clean_training_stats.csv'),\n",
    "    (paths.RESULTS / 'patchcore_clean_metadata.json', 'patchcore_clean_metadata.json')\n",
    "]\n",
    "\n",
    "for src, dst_name in results_files:\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, DRIVE_OUTPUT / dst_name)\n",
    "        print(f\"✓ Saved: {dst_name}\")\n",
    "\n",
    "# 3. Copy visualization files\n",
    "viz_files = []\n",
    "for class_name in config.dataset.classes:\n",
    "    viz_file = paths.VISUALIZATIONS / f\"patchcore_clean_{class_name}_validation.png\"\n",
    "    if viz_file.exists():\n",
    "        shutil.copy2(viz_file, DRIVE_OUTPUT / viz_file.name)\n",
    "        viz_files.append(viz_file.name)\n",
    "        print(f\"✓ Saved: {viz_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OUTPUTS SAVED TO GOOGLE DRIVE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Location: {DRIVE_OUTPUT}\")\n",
    "print(f\"Total files: {len(list(DRIVE_OUTPUT.iterdir()))}\")\n",
    "print(f\"Total size: {sum(f.stat().st_size for f in DRIVE_OUTPUT.iterdir() if f.is_file()) / (1024*1024):.2f} MB\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f76b71",
   "metadata": {},
   "source": [
    "## Create ZIP Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create zip archive with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "zip_name = f'04_patchcore_clean_outputs_{timestamp}'\n",
    "zip_path = Path('/content/drive/MyDrive/anomaly_detection_project') / zip_name\n",
    "\n",
    "print(f\"Creating zip archive...\")\n",
    "print(f\"Source: {DRIVE_OUTPUT}\")\n",
    "print(f\"Archive: {zip_path}.zip\")\n",
    "print(\"\\nThis may take a minute...\")\n",
    "\n",
    "# Create zip\n",
    "shutil.make_archive(str(zip_path), 'zip', DRIVE_OUTPUT)\n",
    "\n",
    "zip_size = (Path(str(zip_path) + '.zip')).stat().st_size / (1024*1024)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ZIP ARCHIVE CREATED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Location: {zip_path}.zip\")\n",
    "print(f\"Size: {zip_size:.2f} MB\")\n",
    "print(f\"\\nContents:\")\n",
    "print(f\"  - {len(models_saved)} model files (.npy + .pth)\")\n",
    "print(f\"  - {len(results_files)} result files (.csv, .json)\")\n",
    "print(f\"  - {len(viz_files)} visualizations (.png)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d34107",
   "metadata": {},
   "source": [
    "## Download ZIP Locally (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download zip to local machine\n",
    "from google.colab import files\n",
    "\n",
    "# Create a local copy of the zip\n",
    "local_zip = f'/content/{zip_name}.zip'\n",
    "shutil.copy2(str(zip_path) + '.zip', local_zip)\n",
    "\n",
    "print(\"Downloading ZIP to your local machine...\")\n",
    "print(f\"File: {local_zip}\")\n",
    "print(f\"Size: {zip_size:.2f} MB\")\n",
    "print(\"\\nDownload will start automatically...\")\n",
    "\n",
    "files.download(local_zip)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DOWNLOAD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1. Extract ZIP file locally\")\n",
    "print(\"  2. Copy files to your local project's outputs/ folder\")\n",
    "print(\"  3. Review results and visualizations\")\n",
    "print(\"  4. Proceed with Phase 4: PaDiM baseline (notebook 05)\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
