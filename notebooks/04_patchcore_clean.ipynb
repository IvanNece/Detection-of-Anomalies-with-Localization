{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4755fd27",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/IvanRepo/Detection-of-Anomalies-with-Localization/blob/PatchCore/notebooks/04_patchcore_clean.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# PatchCore Clean Domain Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c1277",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.config import load_config\n",
    "from src.utils.paths import ProjectPaths\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.data.transforms import get_clean_transforms\n",
    "from src.data.splitter import load_splits\n",
    "from src.models.patchcore import PatchCore\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('../configs/experiment_config.yaml')\n",
    "paths = ProjectPaths()\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c672f03",
   "metadata": {},
   "source": [
    "## Load Clean Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df532722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean splits\n",
    "splits = load_splits(paths.get_split_path('clean'))\n",
    "\n",
    "print(\"Dataset splits loaded:\")\n",
    "print(\"=\" * 70)\n",
    "for class_name in config.dataset.classes:\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        split_data = splits[class_name][split_name]\n",
    "        n_normal = sum(1 for l in split_data['labels'] if l == 0)\n",
    "        n_anomalous = sum(1 for l in split_data['labels'] if l == 1)\n",
    "        print(f\"  {split_name:5s}: {len(split_data['labels']):4d} images \"\n",
    "              f\"({n_normal:3d} normal, {n_anomalous:3d} anomalous)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3f8a6",
   "metadata": {},
   "source": [
    "## Train PatchCore Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bad463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "transform = get_clean_transforms(image_size=config.dataset.image_size)\n",
    "\n",
    "# Hyperparameters\n",
    "CORESET_RATIO = config.patchcore.coreset_sampling_ratio\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = config.num_workers\n",
    "\n",
    "print(\"\\nTraining Configuration:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Coreset ratio: {CORESET_RATIO*100:.1f}%\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Num workers: {NUM_WORKERS}\")\n",
    "print(f\"Image size: {config.dataset.image_size}\")\n",
    "print(f\"Backbone layers: {config.patchcore.layers}\")\n",
    "print(f\"Patch size: {config.patchcore.patch_size}\")\n",
    "print(f\"N neighbors: {config.patchcore.n_neighbors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each class\n",
    "trained_models = {}\n",
    "training_stats = {}\n",
    "\n",
    "for class_name in config.dataset.classes:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Training PatchCore for: {class_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create train dataset (only normal images)\n",
    "    train_split = splits[class_name]['train']\n",
    "    train_dataset = MVTecDataset.from_split(\n",
    "        train_split,\n",
    "        transform=transform,\n",
    "        phase='train'\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain dataset: {len(train_dataset)} images\")\n",
    "    \n",
    "    # Initialize PatchCore\n",
    "    model = PatchCore(\n",
    "        backbone_layers=config.patchcore.layers,\n",
    "        patch_size=config.patchcore.patch_size,\n",
    "        coreset_ratio=CORESET_RATIO,\n",
    "        n_neighbors=config.patchcore.n_neighbors,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    start_time = time.time()\n",
    "    model.fit(train_loader, apply_coreset=True)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save model\n",
    "    model.save(paths.MODELS, class_name, domain='clean')\n",
    "    \n",
    "    # Store statistics\n",
    "    training_stats[class_name] = {\n",
    "        'n_train_images': len(train_dataset),\n",
    "        'memory_bank_size': len(model.memory_bank),\n",
    "        'training_time': training_time,\n",
    "        'spatial_dims': model.spatial_dims\n",
    "    }\n",
    "    \n",
    "    trained_models[class_name] = model\n",
    "    \n",
    "    print(f\"\\nCompleted {class_name.upper()}:\")\n",
    "    print(f\"  Memory bank size: {len(model.memory_bank)}\")\n",
    "    print(f\"  Training time: {training_time:.2f}s\")\n",
    "    print(f\"  Spatial dims: {model.spatial_dims}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"All PatchCore models trained successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e590c4",
   "metadata": {},
   "source": [
    "## Training Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10205061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "stats_df = pd.DataFrame(training_stats).T\n",
    "stats_df['training_time'] = stats_df['training_time'].apply(lambda x: f\"{x:.2f}s\")\n",
    "\n",
    "print(\"\\nTraining Statistics Summary:\")\n",
    "print(stats_df.to_string())\n",
    "\n",
    "# Save statistics\n",
    "stats_output_path = paths.RESULTS / 'patchcore_clean_training_stats.csv'\n",
    "stats_df.to_csv(stats_output_path)\n",
    "print(f\"\\nStatistics saved to: {stats_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160adb0",
   "metadata": {},
   "source": [
    "## Quick Validation Test\n",
    "\n",
    "Test predictions on a few validation images to verify the model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(img_tensor, mean, std):\n",
    "    \"\"\"Denormalize image for visualization.\"\"\"\n",
    "    img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    img = img * np.array(std) + np.array(mean)\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# Test on validation set\n",
    "class_name = 'hazelnut'  # Change to test different classes\n",
    "model = trained_models[class_name]\n",
    "\n",
    "# Load validation data\n",
    "val_split = splits[class_name]['val']\n",
    "val_dataset = MVTecDataset.from_split(\n",
    "    val_split,\n",
    "    transform=transform,\n",
    "    phase='val'\n",
    ")\n",
    "\n",
    "# Select images: 4 normal + 4 anomalous\n",
    "normal_idx = [i for i, l in enumerate(val_dataset.labels) if l == 0][:4]\n",
    "anomalous_idx = [i for i, l in enumerate(val_dataset.labels) if l == 1][:4]\n",
    "test_indices = normal_idx + anomalous_idx\n",
    "\n",
    "# Get predictions\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for idx in test_indices:\n",
    "    img, mask, label, _ = val_dataset[idx]\n",
    "    test_images.append(img)\n",
    "    test_labels.append(label)\n",
    "\n",
    "test_batch = torch.stack(test_images)\n",
    "scores, heatmaps = model.predict(test_batch, return_heatmaps=True)\n",
    "\n",
    "print(f\"\\nPrediction scores for {class_name}:\")\n",
    "print(f\"Normal images (0-3): {scores[:4]}\")\n",
    "print(f\"Anomalous images (4-7): {scores[4:]}\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(f\"  Normal - mean: {scores[:4].mean():.3f}, std: {scores[:4].std():.3f}\")\n",
    "print(f\"  Anomalous - mean: {scores[4:].mean():.3f}, std: {scores[4:].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a6f8f8",
   "metadata": {},
   "source": [
    "## Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b890448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "mean = config.dataset.normalize.mean\n",
    "std = config.dataset.normalize.std\n",
    "\n",
    "for i, (img, score, heatmap, label) in enumerate(\n",
    "    zip(test_images, scores, heatmaps, test_labels)\n",
    "):\n",
    "    # Denormalize image\n",
    "    img_np = denormalize_image(img, mean, std)\n",
    "    \n",
    "    # Original image\n",
    "    axes[0, i].imshow(img_np)\n",
    "    axes[0, i].set_title(\n",
    "        f\"{'Normal' if label == 0 else 'Anomalous'}\\nScore: {score:.3f}\",\n",
    "        fontsize=10\n",
    "    )\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Heatmap overlay\n",
    "    axes[1, i].imshow(img_np)\n",
    "    axes[1, i].imshow(heatmap, alpha=0.5, cmap='jet', vmin=0, vmax=heatmap.max())\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Heatmap', fontsize=12)\n",
    "\n",
    "plt.suptitle(f'PatchCore Predictions - {class_name.upper()}', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4a0e16",
   "metadata": {},
   "source": [
    "## Save Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee79a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save visualization for each class\n",
    "for class_name in config.dataset.classes:\n",
    "    model = trained_models[class_name]\n",
    "    val_split = splits[class_name]['val']\n",
    "    val_dataset = MVTecDataset.from_split(\n",
    "        val_split, transform=transform, phase='val'\n",
    "    )\n",
    "    \n",
    "    # Select images\n",
    "    normal_idx = [i for i, l in enumerate(val_dataset.labels) if l == 0][:4]\n",
    "    anomalous_idx = [i for i, l in enumerate(val_dataset.labels) if l == 1][:4]\n",
    "    test_indices = normal_idx + anomalous_idx\n",
    "    \n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for idx in test_indices:\n",
    "        img, _, label, _ = val_dataset[idx]\n",
    "        test_images.append(img)\n",
    "        test_labels.append(label)\n",
    "    \n",
    "    test_batch = torch.stack(test_images)\n",
    "    scores, heatmaps = model.predict(test_batch, return_heatmaps=True)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "    \n",
    "    for i, (img, score, heatmap, label) in enumerate(\n",
    "        zip(test_images, scores, heatmaps, test_labels)\n",
    "    ):\n",
    "        img_np = denormalize_image(img, mean, std)\n",
    "        \n",
    "        axes[0, i].imshow(img_np)\n",
    "        axes[0, i].set_title(\n",
    "            f\"{'Normal' if label == 0 else 'Anomalous'}\\nScore: {score:.3f}\",\n",
    "            fontsize=10\n",
    "        )\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(img_np)\n",
    "        axes[1, i].imshow(heatmap, alpha=0.5, cmap='jet', vmin=0, vmax=heatmap.max())\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Heatmap', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f'PatchCore Predictions - {class_name.upper()}', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = paths.VISUALIZATIONS / f'patchcore_clean_{class_name}_validation.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved visualization: {save_path}\")\n",
    "\n",
    "print(\"\\nAll visualizations saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759fcc5a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Phase 3 completed successfully:\n",
    "- Trained PatchCore models for all classes\n",
    "- Memory banks saved in `outputs/models/`\n",
    "- Validation predictions show clear separation between normal and anomalous\n",
    "- Ready for Phase 5: Threshold calibration and full evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b2027",
   "metadata": {},
   "source": [
    "## Save Complete Training Metadata\n",
    "\n",
    "Save comprehensive metadata for reproducibility and downstream evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b487bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Collect validation scores for threshold calibration\n",
    "validation_predictions = {}\n",
    "\n",
    "for class_name in config.dataset.classes:\n",
    "    model = trained_models[class_name]\n",
    "    \n",
    "    # Load validation set\n",
    "    val_split = splits[class_name]['val']\n",
    "    val_dataset = MVTecDataset.from_split(\n",
    "        val_split, transform=transform, phase='val'\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Get predictions for all validation images\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "    \n",
    "    for images, masks, labels, paths in val_loader:\n",
    "        scores, _ = model.predict(images, return_heatmaps=False)\n",
    "        \n",
    "        all_scores.extend(scores.tolist())\n",
    "        all_labels.extend(labels.tolist())\n",
    "        all_paths.extend(paths)\n",
    "    \n",
    "    validation_predictions[class_name] = {\n",
    "        'scores': all_scores,\n",
    "        'labels': all_labels,\n",
    "        'image_paths': all_paths,\n",
    "        'n_normal': sum(1 for l in all_labels if l == 0),\n",
    "        'n_anomalous': sum(1 for l in all_labels if l == 1),\n",
    "        'score_stats': {\n",
    "            'normal_mean': float(np.mean([s for s, l in zip(all_scores, all_labels) if l == 0])),\n",
    "            'normal_std': float(np.std([s for s, l in zip(all_scores, all_labels) if l == 0])),\n",
    "            'anomalous_mean': float(np.mean([s for s, l in zip(all_scores, all_labels) if l == 1])),\n",
    "            'anomalous_std': float(np.std([s for s, l in zip(all_scores, all_labels) if l == 1]))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"{class_name}: Normal scores: {validation_predictions[class_name]['score_stats']['normal_mean']:.3f} +/- \"\n",
    "          f\"{validation_predictions[class_name]['score_stats']['normal_std']:.3f}, \"\n",
    "          f\"Anomalous: {validation_predictions[class_name]['score_stats']['anomalous_mean']:.3f} +/- \"\n",
    "          f\"{validation_predictions[class_name]['score_stats']['anomalous_std']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f110aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive metadata\n",
    "metadata = {\n",
    "    'notebook': '04_patchcore_clean.ipynb',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'phase': 'Phase 3 - PatchCore Clean Domain',\n",
    "    'seed': 42,\n",
    "    'device': device,\n",
    "    \n",
    "    'configuration': {\n",
    "        'backbone': config.patchcore.backbone,\n",
    "        'layers': config.patchcore.layers,\n",
    "        'patch_size': config.patchcore.patch_size,\n",
    "        'coreset_ratio': CORESET_RATIO,\n",
    "        'n_neighbors': config.patchcore.n_neighbors,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'image_size': config.dataset.image_size,\n",
    "        'normalize_mean': config.dataset.normalize.mean,\n",
    "        'normalize_std': config.dataset.normalize.std\n",
    "    },\n",
    "    \n",
    "    'training_statistics': {},\n",
    "    'validation_predictions': validation_predictions,\n",
    "    \n",
    "    'models_saved': {},\n",
    "    'splits_used': 'clean_splits.json'\n",
    "}\n",
    "\n",
    "# Add training statistics per class\n",
    "for class_name in config.dataset.classes:\n",
    "    stats = training_stats[class_name]\n",
    "    metadata['training_statistics'][class_name] = {\n",
    "        'n_train_images': stats['n_train_images'],\n",
    "        'memory_bank_size': stats['memory_bank_size'],\n",
    "        'training_time_seconds': stats['training_time'],\n",
    "        'spatial_dims': stats['spatial_dims'],\n",
    "        'compression_ratio': stats['memory_bank_size'] / (stats['n_train_images'] * stats['spatial_dims'][0] * stats['spatial_dims'][1]) if stats['n_train_images'] > 0 else 0\n",
    "    }\n",
    "    \n",
    "    metadata['models_saved'][class_name] = {\n",
    "        'memory_bank': f\"patchcore_{class_name}_clean.npy\",\n",
    "        'config': f\"patchcore_{class_name}_clean_config.pth\",\n",
    "        'location': str(paths.MODELS)\n",
    "    }\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = paths.RESULTS / 'patchcore_clean_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nMetadata saved to: {metadata_path}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Classes trained: {len(config.dataset.classes)}\")\n",
    "print(f\"  Total memory bank samples: {sum(stats['memory_bank_size'] for stats in training_stats.values())}\")\n",
    "print(f\"  Average training time: {np.mean([stats['training_time'] for stats in training_stats.values()]):.2f}s\")\n",
    "print(f\"  Models saved in: {paths.MODELS}\")\n",
    "print(f\"  Results saved in: {paths.RESULTS}\")\n",
    "print(f\"  Visualizations saved in: {paths.VISUALIZATIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c54f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3 COMPLETED - PATCHCORE CLEAN DOMAIN TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_data = []\n",
    "for class_name in config.dataset.classes:\n",
    "    stats = training_stats[class_name]\n",
    "    val_stats = validation_predictions[class_name]['score_stats']\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Class': class_name.upper(),\n",
    "        'Train Images': stats['n_train_images'],\n",
    "        'Memory Bank': stats['memory_bank_size'],\n",
    "        'Train Time (s)': f\"{stats['training_time']:.2f}\",\n",
    "        'Normal Score': f\"{val_stats['normal_mean']:.3f}±{val_stats['normal_std']:.3f}\",\n",
    "        'Anomalous Score': f\"{val_stats['anomalous_mean']:.3f}±{val_stats['anomalous_std']:.3f}\",\n",
    "        'Separation': f\"{val_stats['anomalous_mean'] - val_stats['normal_mean']:.3f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Next Steps:\")\n",
    "print(\"  1. Phase 4: Train PaDiM baseline (notebook 05)\")\n",
    "print(\"  2. Phase 5: Threshold calibration and evaluation (notebook 06)\")\n",
    "print(\"  3. Use validation scores for optimal threshold selection\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f07fd2a",
   "metadata": {},
   "source": [
    "## Files Generated\n",
    "\n",
    "This notebook generated the following files:\n",
    "\n",
    "**Models** (`outputs/models/`):\n",
    "- `patchcore_hazelnut_clean.npy` - Memory bank\n",
    "- `patchcore_hazelnut_clean_config.pth` - Model config\n",
    "- `patchcore_carpet_clean.npy` - Memory bank\n",
    "- `patchcore_carpet_clean_config.pth` - Model config\n",
    "- `patchcore_zipper_clean.npy` - Memory bank\n",
    "- `patchcore_zipper_clean_config.pth` - Model config\n",
    "\n",
    "**Results** (`outputs/results/`):\n",
    "- `patchcore_clean_training_stats.csv` - Training statistics\n",
    "- `patchcore_clean_metadata.json` - Complete metadata with validation scores\n",
    "\n",
    "**Visualizations** (`outputs/visualizations/`):\n",
    "- `patchcore_clean_hazelnut_validation.png` - Validation predictions\n",
    "- `patchcore_clean_carpet_validation.png` - Validation predictions\n",
    "- `patchcore_clean_zipper_validation.png` - Validation predictions\n",
    "\n",
    "All files are ready for Phase 5 (evaluation) and Phase 6-7 (domain shift experiments)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
