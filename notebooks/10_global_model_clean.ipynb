{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c039d719",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IvanNece/Detection-of-Anomalies-with-Localization/blob/main/notebooks/10_global_model_clean.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67760d",
   "metadata": {},
   "source": [
    "# PHASE 8: Global Model - Unified Training\n",
    "\n",
    "**Objective**: Train a **single** anomaly detection model on **all 3 classes** simultaneously.\n",
    "\n",
    "## Key Differences from Per-Class Models:\n",
    "1. **Single Model**: Train ONE PatchCore and ONE PaDiM on merged training data\n",
    "2. **Per-Class Thresholds**: Calibrate separate thresholds for each class on validation\n",
    "3. **Identical Shortcut Problem**: Can normals from one class be confused with anomalies from another?\n",
    "4. **Performance Gap Analysis**: Quantify degradation vs per-class models\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outcome:\n",
    "Global models should perform **worse** than per-class models due to:\n",
    "- **Distribution heterogeneity**: Mixing different textures/objects\n",
    "- **Identical shortcut**: Normal patterns of Class A may appear anomalous for Class B\n",
    "- **Feature space contamination**: Shared representation struggles with diverse nominal distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a7b8b",
   "metadata": {},
   "source": [
    "## Terminological Note (Model-Unified vs Absolute-Unified)\n",
    "\n",
    "Following the taxonomy from recent literature [CADA, Guo et al. 2024; HierCore, Heo & Kang 2025]:\n",
    "\n",
    "| Setting | Training | Inference | Threshold | Our Experiment |\n",
    "|---------|----------|-----------|-----------|----------------|\n",
    "| **Per-Class** | Separate model per class | Class known | Per-class | ❌ |\n",
    "| **Model-Unified** | Single model for all classes | Class known | Per-class | ✅ **This notebook** |\n",
    "| **Absolute-Unified** | Single model for all classes | Class UNKNOWN | Single global | ❌ |\n",
    "\n",
    "**Our setting**: We train ONE global model but calibrate **per-class thresholds** at inference \n",
    "(class is known). This is the \"model-unified\" setting, NOT \"absolute-unified\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d69dc0",
   "metadata": {},
   "source": [
    "## 0. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfc2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP - Mount Google Drive & Clone Repository\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Mount Google Drive\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Clone repository on main branch\n",
    "print(\"Cloning repository (branch: main)...\")\n",
    "repo_dir = '/content/Detection-of-Anomalies-with-Localization'\n",
    "\n",
    "# Remove if exists\n",
    "if os.path.exists(repo_dir):\n",
    "    print(\"Removing existing repository...\")\n",
    "    !rm -rf {repo_dir}\n",
    "\n",
    "# Clone from main branch\n",
    "!git clone https://github.com/IvanNece/Detection-of-Anomalies-with-Localization.git {repo_dir}\n",
    "print(\"Done!\\n\")\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path(repo_dir)\n",
    "\n",
    "# Dataset location (only clean for this notebook)\n",
    "CLEAN_DATASET_PATH = Path('/content/drive/MyDrive/mvtec_ad')\n",
    "\n",
    "# Output directories\n",
    "MODELS_DIR = PROJECT_ROOT / 'outputs' / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'outputs' / 'results'\n",
    "THRESHOLDS_DIR = PROJECT_ROOT / 'outputs' / 'thresholds'\n",
    "VIZ_DIR = PROJECT_ROOT / 'outputs' / 'visualizations' / 'global_model'\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "THRESHOLDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIZ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Verify dataset exists\n",
    "if not CLEAN_DATASET_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Clean dataset not found at {CLEAN_DATASET_PATH}\\n\"\n",
    "        f\"Please ensure mvtec_ad folder is in your Google Drive.\"\n",
    "    )\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SETUP COMPLETE - PHASE 8: GLOBAL MODEL (MODEL-UNIFIED)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Project:    {PROJECT_ROOT}\")\n",
    "print(f\"Dataset:    {CLEAN_DATASET_PATH}\")\n",
    "print(f\"Branch:     main\")\n",
    "print(f\"Models:     {MODELS_DIR}\")\n",
    "print(f\"Results:    {RESULTS_DIR}\")\n",
    "print(f\"Thresholds: {THRESHOLDS_DIR}\")\n",
    "print(f\"Viz:        {VIZ_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94538d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu --quiet\n",
    "!pip install anomalib --quiet\n",
    "!pip install umap-learn --quiet\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c9ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Project imports\n",
    "from src.data.dataset import MVTecDataset\n",
    "from src.data.transforms import get_clean_transforms\n",
    "from src.models.patchcore import PatchCore\n",
    "from src.models.padim_wrapper import PadimWrapper\n",
    "from src.metrics.threshold_selection import calibrate_threshold\n",
    "from src.metrics.image_metrics import compute_auroc, compute_auprc, compute_f1_at_threshold, compute_classification_metrics\n",
    "from src.metrics.pixel_metrics import compute_pixel_auroc, compute_pro\n",
    "from src.utils.reproducibility import set_seed\n",
    "from src.utils.paths import ProjectPaths\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c933060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Configuration\n",
    "CLASSES = ['hazelnut', 'carpet', 'zipper']\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "paths = ProjectPaths()\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "print(f\"Seed: 42 (FIXED for reproducibility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18792528",
   "metadata": {},
   "source": [
    "## 1. Data Preparation: Merge Training Sets from All Classes\n",
    "\n",
    "**CRITICAL**: We create a **SINGLE** global training set by merging Train-clean from all 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean splits\n",
    "splits_path = paths.data_processed / 'clean_splits.json'\n",
    "with open(splits_path, 'r') as f:\n",
    "    splits = json.load(f)\n",
    "\n",
    "print(f\"Loaded splits for classes: {list(splits.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f62c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all train_clean images into a single global dataset\n",
    "global_train_images = []\n",
    "global_train_masks = []\n",
    "global_train_labels = []\n",
    "global_train_class_ids = []  # Track which class each image belongs to\n",
    "\n",
    "for class_idx, class_name in enumerate(CLASSES):\n",
    "    class_splits = splits[class_name]['train']\n",
    "    \n",
    "    # Train only contains normal images\n",
    "    n_samples = len(class_splits['images'])\n",
    "    \n",
    "    global_train_images.extend(class_splits['images'])\n",
    "    global_train_masks.extend([None] * n_samples)  # Only normals\n",
    "    global_train_labels.extend([0] * n_samples)  # Label 0 = normal\n",
    "    global_train_class_ids.extend([class_idx] * n_samples)\n",
    "    \n",
    "    print(f\"  {class_name:10s}: {n_samples:4d} normal images\")\n",
    "\n",
    "print(f\"\\nGlobal Train Set: {len(global_train_images)} total images\")\n",
    "print(f\"   Distribution: Hazelnut={global_train_class_ids.count(0)}, \"\n",
    "      f\"Carpet={global_train_class_ids.count(1)}, Zipper={global_train_class_ids.count(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa729e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global training dataset\n",
    "transform_clean = get_clean_transforms()\n",
    "\n",
    "global_train_dataset = MVTecDataset(\n",
    "    images=global_train_images,\n",
    "    masks=global_train_masks,\n",
    "    labels=global_train_labels,\n",
    "    transform=transform_clean,\n",
    "    phase='train'\n",
    ")\n",
    "\n",
    "global_train_loader = DataLoader(\n",
    "    global_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,  # Important for reproducibility\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Global Train DataLoader: {len(global_train_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f1966",
   "metadata": {},
   "source": [
    "## 2. Train Single PatchCore Global Model\n",
    "\n",
    "**KEY POINT**: Training ONE model on the merged dataset (not 3 separate models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c0b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING GLOBAL PATCHCORE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize PatchCore with same hyperparameters as per-class models\n",
    "patchcore_global = PatchCore(\n",
    "    backbone_layers=['layer2', 'layer3'],\n",
    "    patch_size=3,\n",
    "    coreset_ratio=0.05,  # 5% as per PHASE 3.5\n",
    "    n_neighbors=9,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train on global dataset\n",
    "start_time = time.time()\n",
    "patchcore_global.fit(global_train_loader, apply_coreset=True)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nPatchCore Global trained in {training_time:.2f}s\")\n",
    "print(f\"   Memory bank size: {patchcore_global.memory_bank.features.shape[0]:,} patches\")\n",
    "\n",
    "# Save model\n",
    "save_path = paths.models / 'patchcore_global_clean.npy'\n",
    "patchcore_global.save(paths.models, class_name='global', domain='clean')\n",
    "print(f\"   Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ee2bf",
   "metadata": {},
   "source": [
    "## 3. Train Single PaDiM Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TRAINING GLOBAL PADIM MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize PaDiM\n",
    "padim_global = PadimWrapper(\n",
    "    backbone='wide_resnet50_2',\n",
    "    layers=['layer1', 'layer2', 'layer3'],\n",
    "    n_features=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train on global dataset\n",
    "start_time = time.time()\n",
    "padim_global.fit(global_train_loader, verbose=True)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nPaDiM Global trained in {training_time:.2f}s\")\n",
    "\n",
    "# Save model\n",
    "save_path = paths.models / 'padim_global_clean.pt'\n",
    "padim_global.save(save_path)\n",
    "print(f\"   Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571c097",
   "metadata": {},
   "source": [
    "## 4. Per-Class Threshold Calibration (Using Global Models)\n",
    "\n",
    "**CRITICAL**: Although we have ONE model, we calibrate **SEPARATE thresholds** for each class on their validation sets.\n",
    "\n",
    "This allows fair comparison: each class gets an optimal threshold despite using a shared model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31091db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PER-CLASS THRESHOLD CALIBRATION (GLOBAL MODELS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "thresholds_global = {\n",
    "    'patchcore': {},\n",
    "    'padim': {}\n",
    "}\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\nCalibrating thresholds for {class_name}...\")\n",
    "    \n",
    "    # Load validation split for this class\n",
    "    val_split = splits[class_name]['val']\n",
    "    \n",
    "    val_dataset = MVTecDataset(\n",
    "        images=val_split['images'],\n",
    "        masks=val_split['masks'],\n",
    "        labels=val_split['labels'],\n",
    "        transform=transform_clean,\n",
    "        phase='val'\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Get predictions from GLOBAL models\n",
    "    patchcore_scores, _ = patchcore_global.predict(\n",
    "        torch.cat([batch[0] for batch in val_loader]).to(device),\n",
    "        return_heatmaps=False\n",
    "    )\n",
    "    \n",
    "    padim_scores, _ = padim_global.predict(\n",
    "        torch.cat([batch[0] for batch in val_loader]).to(device),\n",
    "        return_heatmaps=False\n",
    "    )\n",
    "    \n",
    "    val_labels = np.array(val_split['labels'])\n",
    "    \n",
    "    # Calibrate thresholds to maximize F1\n",
    "    threshold_pc = calibrate_threshold(patchcore_scores, val_labels)\n",
    "    threshold_pd = calibrate_threshold(padim_scores, val_labels)\n",
    "    \n",
    "    thresholds_global['patchcore'][class_name] = float(threshold_pc)\n",
    "    thresholds_global['padim'][class_name] = float(threshold_pd)\n",
    "    \n",
    "    # Compute F1 at calibrated thresholds\n",
    "    f1_pc = compute_f1_at_threshold(val_labels, patchcore_scores, threshold_pc)\n",
    "    f1_pd = compute_f1_at_threshold(val_labels, padim_scores, threshold_pd)\n",
    "    \n",
    "    print(f\"   PatchCore: threshold={threshold_pc:.4f}, val_F1={f1_pc:.4f}\")\n",
    "    print(f\"   PaDiM:     threshold={threshold_pd:.4f}, val_F1={f1_pd:.4f}\")\n",
    "\n",
    "# Save thresholds\n",
    "thresholds_path = paths.thresholds / 'global_thresholds.json'\n",
    "with open(thresholds_path, 'w') as f:\n",
    "    json.dump(thresholds_global, f, indent=2)\n",
    "\n",
    "print(f\"\\nThresholds saved to: {thresholds_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7358f2",
   "metadata": {},
   "source": [
    "## 5. Evaluate Global Models on Test-Clean (Per-Class)\n",
    "\n",
    "Test the global models on each class separately using per-class thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EVALUATING GLOBAL MODELS ON TEST-CLEAN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_global = {\n",
    "    'patchcore': {},\n",
    "    'padim': {}\n",
    "}\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    print(f\"\\nEvaluating {class_name}...\")\n",
    "    \n",
    "    # Load test split\n",
    "    test_split = splits[class_name]['test']\n",
    "    \n",
    "    test_dataset = MVTecDataset(\n",
    "        images=test_split['images'],\n",
    "        masks=test_split['masks'],\n",
    "        labels=test_split['labels'],\n",
    "        transform=transform_clean,\n",
    "        phase='test'\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Get all test images and labels\n",
    "    test_images = torch.cat([batch[0] for batch in test_loader]).to(device)\n",
    "    test_masks_list = [batch[1] for batch in test_loader]\n",
    "    test_labels = np.array(test_split['labels'])\n",
    "    \n",
    "    # PatchCore predictions\n",
    "    pc_scores, pc_heatmaps = patchcore_global.predict(test_images, return_heatmaps=True)\n",
    "    pc_predictions = (pc_scores >= thresholds_global['patchcore'][class_name]).astype(int)\n",
    "    \n",
    "    # PaDiM predictions\n",
    "    pd_scores, pd_heatmaps = padim_global.predict(test_images, return_heatmaps=True)\n",
    "    pd_predictions = (pd_scores >= thresholds_global['padim'][class_name]).astype(int)\n",
    "    \n",
    "    # Compute image-level metrics\n",
    "    pc_auroc = compute_auroc(test_labels, pc_scores)\n",
    "    pc_auprc = compute_auprc(test_labels, pc_scores)\n",
    "    pc_metrics = compute_classification_metrics(test_labels, pc_predictions)\n",
    "    \n",
    "    pd_auroc = compute_auroc(test_labels, pd_scores)\n",
    "    pd_auprc = compute_auprc(test_labels, pd_scores)\n",
    "    pd_metrics = compute_classification_metrics(test_labels, pd_predictions)\n",
    "    \n",
    "    # Compute pixel-level metrics (for anomalous images with masks)\n",
    "    anomaly_indices = np.where(test_labels == 1)[0]\n",
    "    if len(anomaly_indices) > 0:\n",
    "        test_masks_anomalies = [test_masks_list[i] for i in anomaly_indices if test_masks_list[i] is not None]\n",
    "        pc_heatmaps_anomalies = pc_heatmaps[anomaly_indices]\n",
    "        pd_heatmaps_anomalies = pd_heatmaps[anomaly_indices]\n",
    "        \n",
    "        if len(test_masks_anomalies) > 0:\n",
    "            pc_pixel_auroc = compute_pixel_auroc(test_masks_anomalies, pc_heatmaps_anomalies)\n",
    "            pc_pro = compute_pro(test_masks_anomalies, pc_heatmaps_anomalies)\n",
    "            \n",
    "            pd_pixel_auroc = compute_pixel_auroc(test_masks_anomalies, pd_heatmaps_anomalies)\n",
    "            pd_pro = compute_pro(test_masks_anomalies, pd_heatmaps_anomalies)\n",
    "        else:\n",
    "            pc_pixel_auroc = pc_pro = pd_pixel_auroc = pd_pro = None\n",
    "    else:\n",
    "        pc_pixel_auroc = pc_pro = pd_pixel_auroc = pd_pro = None\n",
    "    \n",
    "    # Store results\n",
    "    results_global['patchcore'][class_name] = {\n",
    "        'auroc': pc_auroc,\n",
    "        'auprc': pc_auprc,\n",
    "        'f1': pc_metrics['f1'],\n",
    "        'accuracy': pc_metrics['accuracy'],\n",
    "        'precision': pc_metrics['precision'],\n",
    "        'recall': pc_metrics['recall'],\n",
    "        'pixel_auroc': pc_pixel_auroc,\n",
    "        'pro': pc_pro\n",
    "    }\n",
    "    \n",
    "    results_global['padim'][class_name] = {\n",
    "        'auroc': pd_auroc,\n",
    "        'auprc': pd_auprc,\n",
    "        'f1': pd_metrics['f1'],\n",
    "        'accuracy': pd_metrics['accuracy'],\n",
    "        'precision': pd_metrics['precision'],\n",
    "        'recall': pd_metrics['recall'],\n",
    "        'pixel_auroc': pd_pixel_auroc,\n",
    "        'pro': pd_pro\n",
    "    }\n",
    "    \n",
    "    print(f\"   PatchCore: AUROC={pc_auroc:.4f}, F1={pc_metrics['f1']:.4f}, Pixel AUROC={pc_pixel_auroc:.4f if pc_pixel_auroc else 'N/A'}\")\n",
    "    print(f\"   PaDiM:     AUROC={pd_auroc:.4f}, F1={pd_metrics['f1']:.4f}, Pixel AUROC={pd_pixel_auroc:.4f if pd_pixel_auroc else 'N/A'}\")\n",
    "\n",
    "print(\"\\nGlobal model evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f569c90",
   "metadata": {},
   "source": [
    "## 6. Load Per-Class Models for Comparison\n",
    "\n",
    "Load the per-class trained models to compute the performance gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING PER-CLASS MODELS FOR COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load per-class results from PHASE 5 (clean domain evaluation)\n",
    "clean_results_path = paths.results / 'clean_results.json'\n",
    "with open(clean_results_path, 'r') as f:\n",
    "    clean_results = json.load(f)\n",
    "\n",
    "# Extract per-class model results\n",
    "# clean_results structure: {\"metadata\": {...}, \"patchcore\": {...}, \"padim\": {...}}\n",
    "results_per_class = {\n",
    "    'patchcore': clean_results['patchcore'],\n",
    "    'padim': clean_results['padim']\n",
    "}\n",
    "\n",
    "print(\"Per-class model results loaded\")\n",
    "print(f\"   Available classes: {list(results_per_class['patchcore'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda86126",
   "metadata": {},
   "source": [
    "## 7. Performance Gap Analysis: Global vs Per-Class\n",
    "\n",
    "**Key Question**: How much performance do we lose by using a single global model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6221209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PERFORMANCE GAP ANALYSIS: Per-Class vs Global Model\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gaps = {\n",
    "    'patchcore': {},\n",
    "    'padim': {}\n",
    "}\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "for method in ['patchcore', 'padim']:\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    print(f\"{'Class':<12} {'Per-Class AUROC':<18} {'Global AUROC':<15} {'Gap':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for class_name in CLASSES:\n",
    "        auroc_per_class = results_per_class[method][class_name]['image_level']['auroc']\n",
    "        auroc_global = results_global[method][class_name]['auroc']\n",
    "        gap = auroc_per_class - auroc_global\n",
    "        \n",
    "        gaps[method][class_name] = gap\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Method': method.upper(),\n",
    "            'Class': class_name,\n",
    "            'Per-Class AUROC': auroc_per_class,\n",
    "            'Global AUROC': auroc_global,\n",
    "            'Gap': gap\n",
    "        })\n",
    "        \n",
    "        print(f\"{class_name:<12} {auroc_per_class:>16.4f}   {auroc_global:>13.4f}   {gap:>+8.4f}\")\n",
    "    \n",
    "    # Macro average\n",
    "    avg_per_class = np.mean([results_per_class[method][c]['image_level']['auroc'] for c in CLASSES])\n",
    "    avg_global = np.mean([results_global[method][c]['auroc'] for c in CLASSES])\n",
    "    avg_gap = avg_per_class - avg_global\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'MACRO AVG':<12} {avg_per_class:>16.4f}   {avg_global:>13.4f}   {avg_gap:>+8.4f}\")\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Method': method.upper(),\n",
    "        'Class': 'MACRO_AVG',\n",
    "        'Per-Class AUROC': avg_per_class,\n",
    "        'Global AUROC': avg_global,\n",
    "        'Gap': avg_gap\n",
    "    })\n",
    "\n",
    "# Create DataFrame for export\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "comparison_csv_path = paths.results / 'global_vs_per_class_comparison.csv'\n",
    "df_comparison.to_csv(comparison_csv_path, index=False)\n",
    "print(f\"\\nComparison saved to: {comparison_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728feb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Bar chart of performance gaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, method in enumerate(['patchcore', 'padim']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    gap_values = [gaps[method][c] for c in CLASSES]\n",
    "    colors = ['red' if g < 0 else 'green' for g in gap_values]\n",
    "    \n",
    "    bars = ax.bar(CLASSES, gap_values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "    ax.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, val in zip(bars, gap_values):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{val:+.3f}',\n",
    "                ha='center', va='bottom' if height > 0 else 'top',\n",
    "                fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_title(f'{method.upper()}: Performance Gap\\n(Per-Class - Global)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('AUROC Gap', fontsize=11)\n",
    "    ax.set_xlabel('Class', fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle=':')\n",
    "    ax.set_ylim([min(gap_values) - 0.05, max(gap_values) + 0.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "gap_plot_path = paths.visualizations / 'global_model_performance_gap.png'\n",
    "plt.savefig(gap_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Plot saved to: {gap_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391ec3f",
   "metadata": {},
   "source": [
    "## 8. Identical Shortcut Problem Analysis\n",
    "\n",
    "**Context**: In model-unified anomaly detection [CADA, Guo et al. 2024], a single model \n",
    "is trained on multiple classes but evaluated with per-class thresholds. This differs from \n",
    "absolute-unified settings where class information is unavailable at inference.\n",
    "\n",
    "**The \"Identical Shortcut\" Problem** [UniAD, You et al. 2022]: When training a unified model \n",
    "on heterogeneous classes, normal patterns from one class may appear anomalous for another.\n",
    "\n",
    "**Hypothesis**: Normal images from Class A may exceed the anomaly threshold calibrated for Class B, \n",
    "even with per-class thresholds, due to shared feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db705e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"IDENTICAL SHORTCUT PROBLEM ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"Testing: Do normals from Class A trigger anomaly threshold for Class B?\\n\")\n",
    "\n",
    "# Focus on PatchCore for this analysis (same applies to PaDiM)\n",
    "confusion_matrix_cross = np.zeros((len(CLASSES), len(CLASSES)))\n",
    "\n",
    "for target_idx, target_class in enumerate(CLASSES):\n",
    "    threshold = thresholds_global['patchcore'][target_class]\n",
    "    print(f\"\\nTarget Class: {target_class} (threshold={threshold:.4f})\")\n",
    "    \n",
    "    for source_idx, source_class in enumerate(CLASSES):\n",
    "        # Load NORMAL test images from source class\n",
    "        source_test = splits[source_class]['test']\n",
    "        normal_indices = [i for i, lbl in enumerate(source_test['labels']) if lbl == 0]\n",
    "        \n",
    "        if len(normal_indices) == 0:\n",
    "            confusion_matrix_cross[target_idx, source_idx] = 0.0\n",
    "            continue\n",
    "        \n",
    "        normal_images = [source_test['images'][i] for i in normal_indices]\n",
    "        \n",
    "        # Create dataset\n",
    "        normal_dataset = MVTecDataset(\n",
    "            images=normal_images,\n",
    "            masks=[None] * len(normal_images),\n",
    "            labels=[0] * len(normal_images),\n",
    "            transform=transform_clean,\n",
    "            phase='test'\n",
    "        )\n",
    "        \n",
    "        normal_loader = DataLoader(normal_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "        normal_images_tensor = torch.cat([batch[0] for batch in normal_loader]).to(device)\n",
    "        \n",
    "        # Predict with global model\n",
    "        scores, _ = patchcore_global.predict(normal_images_tensor, return_heatmaps=False)\n",
    "        \n",
    "        # Confusion rate: % of normals from source that exceed target threshold\n",
    "        false_positive_rate = (scores > threshold).mean()\n",
    "        confusion_matrix_cross[target_idx, source_idx] = false_positive_rate\n",
    "        \n",
    "        if source_class == target_class:\n",
    "            print(f\"   {source_class:10s} (same class): {false_positive_rate:.2%} FP rate\")\n",
    "        else:\n",
    "            print(f\"   {source_class:10s} → confusion: {false_positive_rate:.2%}\")\n",
    "\n",
    "print(\"\\nCross-class confusion analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43443568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Confusion heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "    confusion_matrix_cross * 100,  # Convert to percentage\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='RdYlGn_r',  # Red = high confusion, Green = low confusion\n",
    "    xticklabels=CLASSES,\n",
    "    yticklabels=CLASSES,\n",
    "    cbar_kws={'label': 'False Positive Rate (%)'},\n",
    "    vmin=0,\n",
    "    vmax=100,\n",
    "    linewidths=1,\n",
    "    linecolor='gray',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title('Identical Shortcut Problem\\n'\n",
    "             'Cross-Class Confusion Matrix (PatchCore Global Model)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Source Class (Normal Images)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Target Class (Threshold)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add explanation text\n",
    "fig.text(0.5, 0.02,\n",
    "         'Higher values (red) indicate normals from Source Class are confused as anomalies for Target Class',\n",
    "         ha='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "confusion_plot_path = paths.visualizations / 'identical_shortcut_confusion.png'\n",
    "plt.savefig(confusion_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confusion heatmap saved to: {confusion_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c95bf8",
   "metadata": {},
   "source": [
    "## 9. Feature Space Visualization: T-SNE\n",
    "\n",
    "Visualize how the global model represents normal and anomalous samples from all classes in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3619c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"FEATURE SPACE VISUALIZATION (T-SNE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect features from all classes\n",
    "all_features = []\n",
    "all_labels = []\n",
    "all_class_names = []\n",
    "\n",
    "n_samples_per_class = 30  # Limit for visualization\n",
    "\n",
    "for class_name in CLASSES:\n",
    "    test_split = splits[class_name]['test']\n",
    "    \n",
    "    # Sample normal and anomalous images\n",
    "    normal_indices = [i for i, lbl in enumerate(test_split['labels']) if lbl == 0][:n_samples_per_class]\n",
    "    anomaly_indices = [i for i, lbl in enumerate(test_split['labels']) if lbl == 1][:n_samples_per_class]\n",
    "    \n",
    "    selected_indices = normal_indices + anomaly_indices\n",
    "    selected_images = [test_split['images'][i] for i in selected_indices]\n",
    "    selected_labels = [test_split['labels'][i] for i in selected_indices]\n",
    "    \n",
    "    # Create dataset\n",
    "    sample_dataset = MVTecDataset(\n",
    "        images=selected_images,\n",
    "        masks=[None] * len(selected_images),\n",
    "        labels=selected_labels,\n",
    "        transform=transform_clean,\n",
    "        phase='test'\n",
    "    )\n",
    "    \n",
    "    sample_loader = DataLoader(sample_dataset, batch_size=len(selected_images), shuffle=False)\n",
    "    sample_images_tensor = next(iter(sample_loader))[0].to(device)\n",
    "    \n",
    "    # Extract features using PatchCore backbone\n",
    "    with torch.no_grad():\n",
    "        features = patchcore_global.backbone(sample_images_tensor)  # (B, C, H, W)\n",
    "        # Global average pooling to get image-level features\n",
    "        features_pooled = features.mean(dim=[2, 3])  # (B, C)\n",
    "        features_numpy = features_pooled.cpu().numpy()\n",
    "    \n",
    "    all_features.append(features_numpy)\n",
    "    all_labels.extend(selected_labels)\n",
    "    all_class_names.extend([class_name] * len(selected_images))\n",
    "    \n",
    "    print(f\"   {class_name}: {len(normal_indices)} normals, {len(anomaly_indices)} anomalies\")\n",
    "\n",
    "# Concatenate all features\n",
    "all_features = np.vstack(all_features)\n",
    "print(f\"\\nCollected {all_features.shape[0]} samples with {all_features.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d2d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run T-SNE\n",
    "print(\"Running T-SNE (this may take a minute)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "features_2d = tsne.fit_transform(all_features)\n",
    "print(\"T-SNE complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20343bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Define colors and markers\n",
    "class_colors = {'hazelnut': 'blue', 'carpet': 'green', 'zipper': 'purple'}\n",
    "label_markers = {0: 'o', 1: 'X'}  # o = normal, X = anomaly\n",
    "label_sizes = {0: 50, 1: 100}\n",
    "\n",
    "# Plot each combination\n",
    "for class_name in CLASSES:\n",
    "    for label_type in [0, 1]:\n",
    "        mask = [(c == class_name and l == label_type) \n",
    "                for c, l in zip(all_class_names, all_labels)]\n",
    "        \n",
    "        if not any(mask):\n",
    "            continue\n",
    "        \n",
    "        label_str = 'Normal' if label_type == 0 else 'Anomaly'\n",
    "        \n",
    "        ax.scatter(\n",
    "            features_2d[mask, 0],\n",
    "            features_2d[mask, 1],\n",
    "            c=class_colors[class_name],\n",
    "            marker=label_markers[label_type],\n",
    "            s=label_sizes[label_type],\n",
    "            alpha=0.7,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5,\n",
    "            label=f'{class_name} - {label_str}'\n",
    "        )\n",
    "\n",
    "ax.set_title('T-SNE Visualization of Feature Space\\n'\n",
    "             'Global PatchCore Model (All Classes)',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('T-SNE Dimension 1', fontsize=12)\n",
    "ax.set_ylabel('T-SNE Dimension 2', fontsize=12)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10, frameon=True, shadow=True)\n",
    "ax.grid(alpha=0.3, linestyle=':')\n",
    "\n",
    "plt.tight_layout()\n",
    "tsne_plot_path = paths.visualizations / 'tsne_global_model.png'\n",
    "plt.savefig(tsne_plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"T-SNE plot saved to: {tsne_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558eabf",
   "metadata": {},
   "source": [
    "## 10. Summary Table and Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45135eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "for method in ['patchcore', 'padim']:\n",
    "    for class_name in CLASSES:\n",
    "        # Per-class model\n",
    "        per_class_res = results_per_class[method][class_name]['image_level']\n",
    "        \n",
    "        # Global model\n",
    "        global_res = results_global[method][class_name]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Method': method.upper(),\n",
    "            'Class': class_name,\n",
    "            'Model Type': 'Per-Class',\n",
    "            'AUROC': per_class_res['auroc'],\n",
    "            'AUPRC': per_class_res['auprc'],\n",
    "            'F1': per_class_res['f1'],\n",
    "            'Accuracy': per_class_res['accuracy']\n",
    "        })\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Method': method.upper(),\n",
    "            'Class': class_name,\n",
    "            'Model Type': 'Global',\n",
    "            'AUROC': global_res['auroc'],\n",
    "            'AUPRC': global_res['auprc'],\n",
    "            'F1': global_res['f1'],\n",
    "            'Accuracy': global_res['accuracy']\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "# Display\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY: Per-Class vs Global Model\")\n",
    "print(\"=\"*70)\n",
    "print(df_summary.to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "summary_path = paths.results / 'global_model_summary.csv'\n",
    "df_summary.to_csv(summary_path, index=False)\n",
    "print(f\"\\nSummary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete results dictionary\n",
    "final_results = {\n",
    "    'global_model_results': results_global,\n",
    "    'per_class_model_results': results_per_class,\n",
    "    'performance_gaps': gaps,\n",
    "    'cross_class_confusion_matrix': confusion_matrix_cross.tolist(),\n",
    "    'thresholds_global': thresholds_global,\n",
    "    'metadata': {\n",
    "        'classes': CLASSES,\n",
    "        'global_train_size': len(global_train_images),\n",
    "        'patchcore_coreset_ratio': 0.05,\n",
    "        'seed': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "results_json_path = paths.results / 'global_model_analysis.json'\n",
    "with open(results_json_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2)\n",
    "\n",
    "print(f\"Complete results saved to: {results_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a635234a",
   "metadata": {},
   "source": [
    "Save results (google drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e586331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COPY ALL RESULTS TO GOOGLE DRIVE FOR PERSISTENCE\n",
    "# ============================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Create destination folder in Drive\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/anomaly_detection_project')\n",
    "PHASE8_OUTPUTS = DRIVE_ROOT / '10_global_model_outputs'\n",
    "PHASE8_OUTPUTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COPYING FILES TO GOOGLE DRIVE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nDestination: {PHASE8_OUTPUTS}\")\n",
    "\n",
    "# List of all generated files\n",
    "generated_files = []\n",
    "\n",
    "# Models (Global models)\n",
    "print(\"\\nCopying models...\")\n",
    "model_files = [\n",
    "    MODELS_DIR / 'patchcore_global_clean.npy',\n",
    "    MODELS_DIR / 'patchcore_global_clean_config.pth',\n",
    "    MODELS_DIR / 'padim_global_clean.pt',\n",
    "    MODELS_DIR / 'padim_global_clean.json'\n",
    "]\n",
    "generated_files.extend(model_files)\n",
    "\n",
    "# Results\n",
    "print(\"Copying results...\")\n",
    "result_files = [\n",
    "    RESULTS_DIR / 'global_model_analysis.json',\n",
    "    RESULTS_DIR / 'global_model_summary.csv',\n",
    "    RESULTS_DIR / 'global_vs_per_class_comparison.csv'\n",
    "]\n",
    "generated_files.extend(result_files)\n",
    "\n",
    "# Thresholds\n",
    "print(\"Copying thresholds...\")\n",
    "threshold_files = [\n",
    "    THRESHOLDS_DIR / 'global_thresholds.json'\n",
    "]\n",
    "generated_files.extend(threshold_files)\n",
    "\n",
    "# Visualizations\n",
    "print(\"Copying visualizations...\")\n",
    "viz_files = [\n",
    "    VIZ_DIR / 'global_model_performance_gap.png',\n",
    "    VIZ_DIR / 'identical_shortcut_confusion.png',\n",
    "    VIZ_DIR / 'tsne_global_model.png'\n",
    "]\n",
    "generated_files.extend(viz_files)\n",
    "\n",
    "# Copy all files\n",
    "copied_count = 0\n",
    "missing_count = 0\n",
    "\n",
    "for src_path in generated_files:\n",
    "    if src_path.exists():\n",
    "        # Preserve directory structure\n",
    "        if 'models' in str(src_path):\n",
    "            dst_dir = PHASE8_OUTPUTS / 'models'\n",
    "        elif 'results' in str(src_path):\n",
    "            dst_dir = PHASE8_OUTPUTS / 'results'\n",
    "        elif 'thresholds' in str(src_path):\n",
    "            dst_dir = PHASE8_OUTPUTS / 'thresholds'\n",
    "        elif 'visualizations' in str(src_path):\n",
    "            dst_dir = PHASE8_OUTPUTS / 'visualizations'\n",
    "        else:\n",
    "            dst_dir = PHASE8_OUTPUTS\n",
    "\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "        dst_path = dst_dir / src_path.name\n",
    "\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        print(f\"  ✓ {src_path.name}\")\n",
    "        copied_count += 1\n",
    "    else:\n",
    "        print(f\"  ✗ MISSING: {src_path.name}\")\n",
    "        missing_count += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"✓ Copy complete: {copied_count} files copied, {missing_count} missing\")\n",
    "print(f\"✓ All results saved to: {PHASE8_OUTPUTS}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d01dc51",
   "metadata": {},
   "source": [
    "// TO CHECK\n",
    "## 11. Key Findings and Interpretation\n",
    "\n",
    "### Setting Clarification:\n",
    "This experiment uses the **Model-Unified** setting [CADA, 2024; HierCore, 2025]:\n",
    "- Single model trained on all classes\n",
    "- **Per-class thresholds** at inference (class is known)\n",
    "- This is NOT \"absolute-unified\" (which would require class-agnostic thresholds)\n",
    "\n",
    "### Observations:\n",
    "1. **Performance Gap**: Model-unified approach shows degraded AUROC vs per-class models\n",
    "   - This validates CADA's observation that shared representations struggle with heterogeneous distributions\n",
    "   \n",
    "2. **Cross-Class Confusion**: The confusion matrix shows non-zero false positive rates across classes\n",
    "   - Normal textures from one class can trigger another class's threshold\n",
    "   - This is the \"identical shortcut\" phenomenon [UniAD, You et al. 2022]\n",
    "\n",
    "### Implications:\n",
    "- For industrial deployment with **single-category** quality control: **per-class models remain optimal**\n",
    "- Model-unified approaches are useful when:\n",
    "  - Storage/training efficiency is critical\n",
    "  - Class categories are related (e.g., similar textures)\n",
    "- Absolute-unified remains an open research challenge (see CADA, HierCore for solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ecd0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Outputs Generated:\n",
    "1. **Models**: `patchcore_global_clean.npy`, `padim_global_clean.pt`\n",
    "2. **Thresholds**: `global_thresholds.json` (per-class thresholds for global models)\n",
    "3. **Results**: `global_model_analysis.json`, `global_model_summary.csv`\n",
    "4. **Visualizations**:\n",
    "   - `global_model_performance_gap.png` (bar chart)\n",
    "   - `identical_shortcut_confusion.png` (heatmap)\n",
    "   - `tsne_global_model.png` (feature space)\n",
    "\n",
    "- Compare with [You et al., 2022] findings on unified anomaly detection\n",
    "- Discuss implications for industrial deployment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
